# DP: ZeRO-3 (数据并行与显存优化)

## 核心思想

数据并行 (Data Parallelism, DP) 是最常见、最简单的并行策略。其核心思想是：

1.  **模型复制**：在每个计算设备（例如 GPU 或 CPU 进程）上都保留一份完整的模型副本。
2.  **数据切分**：将一个大的训练批次 (Global Batch) 切分成多个小的子批次 (Micro-Batch)，每个设备分配一个子批次。
3.  **独立计算**：每个设备独立地在其子批次上执行模型的前向传播和反向传播，计算出本地梯度。
4.  **梯度同步**：使用 `All-Reduce` 通信操作，将所有设备上的本地梯度进行聚合（通常是求和或求平均）。同步后，每个设备上都拥有了全局梯度。
5.  **同步更新**：每个设备使用相同的全局梯度，独立地更新其模型副本的权重。

这样，所有设备上的模型始终保持一致。

## ZeRO 的动机与演进

传统的 DP 策略虽然简单，但存在巨大的显存冗余。对于一个拥有 `N` 个设备（`N`卡）的集群，总显存消耗是单卡的 `N` 倍。这是因为每个设备都存储了三份主要的模型相关数据：

1.  **模型参数 (Parameters)**: `Ψ`
2.  **梯度 (Gradients)**: `ΔΨ`
3.  **优化器状态 (Optimizer States)**: 例如 Adam 优化器中的一阶动量（momentum）和二阶动量（variance），通常是参数量的 2 倍。

**ZeRO (Zero Redundancy Optimizer)** 的目标就是消除这种冗余，将模型相关的数据在不同设备间进行 **分区 (Partition)**，从而在 `N` 个设备上实现近似于单卡的显存占用。

ZeRO 分为三个阶段：

### ZeRO-Stage 1 (ZeRO-1): 优化器状态分区

-   **分区对象**：优化器状态。
-   **实现**：每个设备只存储和更新 `1/N` 的优化器状态。在参数更新步骤（`optimizer.step()`）中，每个设备需要从其他设备那里 `All-Gather` 到完整的参数，用以计算自己负责的那部分优化器状态和更新后的参数。更新完成后，每个设备只保留自己那 `1/N` 的新参数，并丢弃其他部分的参数。

### ZeRO-Stage 2 (ZeRO-2): 梯度和优化器状态分区

-   **分区对象**：梯度 + 优化器状态。
-   **实现**：在反向传播计算出梯度后，每个设备立即通过 `Reduce-Scatter` 操作，只保留自己负责的那 `1/N` 的梯度。这样，每个设备上的梯度内存从 `ΔΨ` 降低到 `ΔΨ/N`。在参数更新时，由于每个设备已经拥有了所需的分区梯度，它只需更新自己负责的那部分优化器状态和参数即可。更新后，使用 `All-Gather` 将所有分区更新后的参数广播给所有设备，确保前向传播时每个设备都有完整的模型。

### ZeRO-Stage 3 (ZeRO-3): 参数、梯度和优化器状态全部分区

-   **分区对象**：模型参数 + 梯度 + 优化器状态。
-   **这是最彻底的优化，也是最具挑战性的。**
-   **实现**:
    1.  **初始化**：在模型初始化时，每个设备只创建和存储 `1/N` 的模型参数。
    2.  **前向传播**：
        -   对于模型中的每一层（例如一个 `nn.Linear`），在执行该层的前向计算 **之前**，所有设备需要通过 `All-Gather` 操作，从其他设备收集该层所需的全部分区参数，以重建该层的完整参数。
        -   执行该层的前向计算。
        -   计算完成后，**立即丢弃** 刚刚收集来的不属于自己负责的参数，以释放显存。
    3.  **反向传播**：
        -   与前向传播类似，在计算某一层的梯度 **之前**，需要先 `All-Gather` 该层的完整参数。
        -   计算该层的梯度。
        -   计算完成后，**立即丢弃** 刚刚收集来的参数。同时，对计算出的梯度进行 `Reduce-Scatter`，每个设备只保留 `1/N` 的梯度。
    4.  **参数更新**：
        -   每个设备使用自己持有的 `1/N` 梯度，来更新自己负责的 `1/N` 优化器状态和 `1/N` 模型参数。这个步骤不需要任何通信。

## 本项目实现要点 (main.py)

我们将从零开始，用 Pytorch 的 `torch.distributed` 和 GLOO 后端模拟实现 ZeRO-3 的核心逻辑。

1.  **分布式环境设置**：初始化 `torch.distributed`，获取 `rank` 和 `world_size`。
2.  **参数分区**：创建一个 `PartitionedParameter` 类或类似的机制，让每个 `rank` 只在本地 `torch.nn.Parameter` 中存储完整参数的一部分。
3.  **模型封装**：创建一个 `Zero3DDP` 模块，它会遍历模型的所有参数，并将它们转换为 `PartitionedParameter`。
4.  **Hook 机制**：
    -   使用 `torch.nn.Module.register_forward_pre_hook` 在每个模块（特别是 `nn.Linear` 等）计算 **前** 注入一个 `pre-forward-hook`。这个 hook 的作用是执行 `All-Gather` 操作，临时组装出完整的参数，并替换掉模块中的分区参数。
    -   使用 `torch.nn.Module.register_forward_hook` 在模块计算 **后** 注入一个 `post-forward-hook`。这个 hook 的作用是释放掉刚刚组装的完整参数，恢复成分区状态。
    -   类似地，使用 `register_full_backward_pre_hook` 和 `register_full_backward_hook` (或 `tensor.register_hook`) 来处理反向传播中的参数收集、梯度 `Reduce-Scatter` 和参数释放。
5.  **自定义优化器**：创建一个 `Zero3Optimizer`，它的 `step()` 方法知道参数是分区的，因此只更新当前 `rank` 负责的那部分参数和优化器状态。

这个实现将不依赖 DeepSpeed 库，让我们能清晰地看到 ZeRO-3 内部的通信和内存管理细节。

## 代码实现 (Code Implementation)

本项目提供了一个基于 PyTorch 的 ZeRO-3 核心逻辑的简化实现。

### 文件结构

-   `zero3.py`: 实现了 `Zero3Linear` 层和 `Zero3AllGather` 自动求导函数。这是 ZeRO-3 的核心，负责参数的分片存储、前向传播时的 All-Gather 重建以及反向传播时的 Reduce-Scatter 梯度聚合。
-   `train.py`: 一个简单的训练脚本，演示了如何使用 `Zero3Linear` 构建模型并进行分布式训练。

### 运行方法

确保你的环境安装了 PyTorch (带 CUDA 支持)。

```bash
python train.py
```

注意：此脚本需要至少 2 个 GPU 才能运行演示。

### 实现细节

1.  **参数分片 (Parameter Sharding)**:
    在 `Zero3Linear` 初始化时，完整的权重矩阵被切分为多个分片 (Shards)，每个 GPU 只存储其中一个分片。这大大降低了显存占用。

2.  **前向传播 (Forward)**:
    -   在执行线性变换前，通过 `Zero3AllGather` (封装了 `dist.all_gather`) 从所有 GPU 收集完整的权重参数。
    -   计算完成后，完整的权重参数不被保存（除了用于反向传播的临时图），从而节省显存。

3.  **反向传播 (Backward)**:
    -   `Zero3AllGather` 的 `backward` 方法实现了梯度的 `Reduce-Scatter`。
    -   每个 GPU 计算出完整权重的梯度后，通过 `dist.reduce_scatter` 将梯度聚合到对应的分片持有者上。
    -   最终，每个 GPU 只获得它所负责的那部分参数的梯度。

4.  **初始化一致性**:
    为了保证所有 GPU 上的模型初始化一致，我们在 Rank 0 上生成初始权重，然后广播 (Broadcast) 到所有其他 Rank，然后再进行分片。

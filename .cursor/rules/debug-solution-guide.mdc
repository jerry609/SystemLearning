---
description: 
globs: 
alwaysApply: false
---
# 调试解决方案指南

## 调试解决方案归档系统

本项目建立了完善的调试解决方案归档系统，位于 [debugging-solutions/](mdc:debugging-solutions) 目录下。在遇到任何技术问题时，应优先参考此系统。

## 快速错误定位流程

### 1. 错误分类识别
根据错误类型，查看相应目录：
- **CUDA相关错误** → [debugging-solutions/cuda-errors/](mdc:debugging-solutions/cuda-errors)
- **训练过程问题** → [debugging-solutions/training-issues/](mdc:debugging-solutions/training-issues)
- **模型相关问题** → [debugging-solutions/model-issues/](mdc:debugging-solutions/model-issues)
- **通用修复方案** → [debugging-solutions/general-fixes/](mdc:debugging-solutions/general-fixes)

### 2. 快速查找资源
- 📖 **快速查找指南**: [debugging-solutions/quick-reference.md](mdc:debugging-solutions/quick-reference.md)
- 📊 **问题索引**: [debugging-solutions/README.md](mdc:debugging-solutions/README.md)
- 📝 **标准模板**: [debugging-solutions/template.md](mdc:debugging-solutions/template.md)

## 已解决的典型问题

### CUDA错误
- **CUDA-001**: `torch.multinomial` CUDA断言失败
  - 文档: [debugging-solutions/cuda-errors/multinomial-assertion-failure.md](mdc:debugging-solutions/cuda-errors/multinomial-assertion-failure.md)
  - 根因: 数值稳定性问题，logits包含NaN/inf值
  - 解决方案: 多层数值稳定性保护机制

### 训练问题
- **TRAIN-001**: GRPO训练梯度爆炸
  - 文档: [debugging-solutions/training-issues/grpo-gradient-explosion.md](mdc:debugging-solutions/training-issues/grpo-gradient-explosion.md)
  - 根因: 学习率过高、缺乏梯度裁剪
  - 解决方案: 渐进式数值稳定策略

## 错误处理最佳实践

### 遇到新问题时的处理步骤

1. **首先查看快速查找指南**: [debugging-solutions/quick-reference.md](mdc:debugging-solutions/quick-reference.md)
2. **按症状搜索**: 使用错误关键词在已有文档中搜索
3. **确认错误分类**: 根据错误类型确定应归属的目录
4. **创建新文档**: 如果是新问题，使用 [debugging-solutions/template.md](mdc:debugging-solutions/template.md) 创建文档

### 调试文档标准格式

每个调试文档应包含：
- **基本信息**: 错误ID、分类、严重程度
- **问题描述**: 详细的错误症状和环境信息
- **根因分析**: 深入的技术分析和错误链条
- **解决方案**: 具体的修复步骤和代码示例
- **验证方法**: 确认问题解决的测试方法
- **预防措施**: 避免再次发生的建议

## 强化学习相关调试

由于项目当前重点在强化学习模块 [reinforcement-learning/](mdc:reinforcement-learning)，特别关注：

### 数值稳定性问题
- 参考 [reinforcement-learning/lab03-优化PPO-实现GRPO/grpo_stable.py](mdc:reinforcement-learning/lab03-优化PPO-实现GRPO/grpo_stable.py)
- 使用多层数值保护：logits限制、梯度裁剪、参数健康监控

### 错误分析工具
- 使用 [reinforcement-learning/lab03-优化PPO-实现GRPO/error_analysis.py](mdc:reinforcement-learning/lab03-优化PPO-实现GRPO/error_analysis.py) 进行深度错误分析
- 实时监控梯度范数、参数健康度、损失值异常

## 🚨 必须遵守的底线原则

### ⚠️ 核心底线 - 绝不妥协的调试态度

1. **🔍 根因优先，拒绝降级**: 
   - **必须**: 深入分析问题的根本原因，找到真正的解决方案
   - **禁止**: 简单地降低精度、减少功能或采用临时workaround逃避问题
   - **标准**: 每个问题都要追踪到技术本质，不接受"能跑就行"的心态

2. **💪 技术卓越，拒绝放弃**:
   - **必须**: 穷尽所有技术手段，查阅文档、源码、学术论文
   - **禁止**: 遇到困难就轻易放弃或选择简化版本
   - **标准**: 如果一个技术方案在理论上可行，就要坚持实现到底

3. **🧠 深度理解，拒绝表面修复**:
   - **必须**: 理解每一行修复代码的技术原理和作用机制
   - **禁止**: 盲目复制网上的解决方案或试错式修复
   - **标准**: 能够解释为什么这个修复方案有效，以及它解决了什么根本问题

4. **📊 数据驱动，拒绝猜测**:
   - **必须**: 用具体的数值、日志、profiling结果支撑分析
   - **禁止**: 基于主观猜测或经验主义进行修复
   - **标准**: 每个诊断结论都要有量化的证据支持

5. **🔄 系统思维，拒绝孤立修复**:
   - **必须**: 考虑修复方案对整个系统的影响和长期可维护性
   - **禁止**: 只关注当前问题而忽视系统整体设计
   - **标准**: 修复方案要与项目的技术架构和质量标准保持一致

### 🎯 实践要求

- **遇到错误时**: 优先查找 [debugging-solutions/](mdc:debugging-solutions) 中的根因分析
- **分析问题时**: 使用 [reinforcement-learning/lab03-优化PPO-实现GRPO/error_analysis.py](mdc:reinforcement-learning/lab03-优化PPO-实现GRPO/error_analysis.py) 进行深度数据分析
- **设计方案时**: 参考 [reinforcement-learning/lab03-优化PPO-实现GRPO/grpo_stable.py](mdc:reinforcement-learning/lab03-优化PPO-实现GRPO/grpo_stable.py) 的多层保护机制思路
- **记录解决方案时**: 必须包含完整的根因分析链条和技术原理解释

## 调试原则

1. **优先使用已有解决方案**: 避免重复造轮子
2. **系统化记录**: 所有新问题都应该标准化归档
3. **知识积累**: 将调试经验转化为可复用的知识库
4. **预防为主**: 通过代码review和最佳实践预防问题发生

## 与其他模块的集成

调试解决方案应该与其他学习模块协同工作：
- **Kubernetes**: 容器和集群相关的调试问题
- **Docker**: 容器化部署的调试场景
- **强化学习**: AI模型训练和推理的调试需求

遇到跨模块的问题时，应该在相应的调试分类下创建综合性解决方案。




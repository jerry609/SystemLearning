# å®žéªŒ10ï¼šç”Ÿäº§çŽ¯å¢ƒæœ€ä½³å®žè·µ

## ðŸŽ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬å®žéªŒï¼Œä½ å°†ï¼š
- æŽŒæ¡Kubernetesç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²çš„å®Œæ•´æµç¨‹
- å­¦ä¹ CI/CDç®¡é“ä¸ŽGitOpsæœ€ä½³å®žè·µ
- å®žè·µé«˜å¯ç”¨é›†ç¾¤æž¶æž„è®¾è®¡
- æŽŒæ¡æ€§èƒ½è°ƒä¼˜å’Œèµ„æºä¼˜åŒ–ç­–ç•¥
- äº†è§£å®‰å…¨åŠ å›ºå’Œåˆè§„æ€§è¦æ±‚
- å­¦ä¹ æ•…éšœæ¢å¤å’Œç¾éš¾å¤‡ä»½æ–¹æ¡ˆ

## ðŸ“š ç†è®ºçŸ¥è¯†å­¦ä¹ 

### ç”Ÿäº§çŽ¯å¢ƒæž¶æž„è®¾è®¡

```
â”Œâ”€â”€â”€ ç”Ÿäº§çŽ¯å¢ƒæž¶æž„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                       â”‚
â”‚ â”Œâ”€ è´Ÿè½½å‡è¡¡å±‚ â”€â”  â”Œâ”€ ç½‘å…³å±‚ â”€â”  â”Œâ”€ åº”ç”¨å±‚ â”€â”         â”‚
â”‚ â”‚  CloudFlare  â”‚â†’â”‚ Ingress  â”‚â†’â”‚ Services  â”‚         â”‚
â”‚ â”‚     /        â”‚  â”‚ Controllerâ”‚  â”‚    &     â”‚         â”‚
â”‚ â”‚   ALB/NLB    â”‚  â”‚    +     â”‚  â”‚   Pods   â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   WAF    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                       â”‚
â”‚ â”Œâ”€ æ•°æ®å±‚ â”€â”    â”Œâ”€ ç¼“å­˜å±‚ â”€â”    â”Œâ”€ æ¶ˆæ¯é˜Ÿåˆ— â”€â”       â”‚
â”‚ â”‚ Database â”‚    â”‚  Redis   â”‚    â”‚   Kafka    â”‚       â”‚
â”‚ â”‚ Cluster  â”‚    â”‚ Cluster  â”‚    â”‚  /RabbitMQ â”‚       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                       â”‚
â”‚ â”Œâ”€ ç›‘æŽ§å±‚ â”€â”    â”Œâ”€ æ—¥å¿—å±‚ â”€â”    â”Œâ”€ å®‰å…¨å±‚ â”€â”         â”‚
â”‚ â”‚Prometheusâ”‚    â”‚   ELK    â”‚    â”‚   RBAC   â”‚         â”‚
â”‚ â”‚ Grafana  â”‚    â”‚  Stack   â”‚    â”‚ Network  â”‚         â”‚
â”‚ â”‚ AlertMgr â”‚    â”‚          â”‚    â”‚ Policies â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç”Ÿäº§çŽ¯å¢ƒæ¸…å•

| ç±»åˆ« | ç»„ä»¶ | ç›®çš„ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **é«˜å¯ç”¨** | å¤šMasterèŠ‚ç‚¹ | æŽ§åˆ¶å¹³é¢é«˜å¯ç”¨ | ðŸ”´ å¿…é¡» |
| **ç½‘ç»œ** | CNIæ’ä»¶ | ç½‘ç»œé€šä¿¡ | ðŸ”´ å¿…é¡» |
| **å­˜å‚¨** | æŒä¹…åŒ–å­˜å‚¨ | æ•°æ®æŒä¹…åŒ– | ðŸ”´ å¿…é¡» |
| **ç›‘æŽ§** | Prometheus+Grafana | ç³»ç»Ÿç›‘æŽ§ | ðŸŸ¡ é‡è¦ |
| **æ—¥å¿—** | ELK/EFK Stack | æ—¥å¿—èšåˆ | ðŸŸ¡ é‡è¦ |
| **å®‰å…¨** | RBAC+NetworkPolicy | è®¿é—®æŽ§åˆ¶ | ðŸ”´ å¿…é¡» |
| **å¤‡ä»½** | Velero | ç¾éš¾æ¢å¤ | ðŸŸ¡ é‡è¦ |

## ðŸš€ CI/CDä¸ŽGitOpså®žè·µ

### å‡†å¤‡å·¥ä½œ

```bash
# åˆ›å»ºå®žéªŒç›®å½•
mkdir -p ~/k8s-labs/lab10/{ci-cd,gitops,ha,monitoring,security,backup}
cd ~/k8s-labs/lab10

# åˆ›å»ºç”Ÿäº§çŽ¯å¢ƒå‘½åç©ºé—´
kubectl create namespace production
kubectl create namespace staging
kubectl create namespace monitoring
```

### 1. GitOpså·¥ä½œæµè®¾è®¡

```bash
cat > gitops/workflow-overview.md << 'EOF'
# GitOpså·¥ä½œæµç¨‹

## 1. ä»£ç æäº¤æµç¨‹
```
å¼€å‘è€…æäº¤ä»£ç  â†’ Gitä»“åº“ â†’ è§¦å‘CIç®¡é“ â†’ æž„å»ºDockeré•œåƒ â†’ æŽ¨é€åˆ°é•œåƒä»“åº“
                                  â†“
æ›´æ–°éƒ¨ç½²æ¸…å• â†’ GitOpsä»“åº“ â†’ ArgoCDæ£€æµ‹å˜æ›´ â†’ è‡ªåŠ¨éƒ¨ç½²åˆ°é›†ç¾¤
```

## 2. çŽ¯å¢ƒç®¡ç†ç­–ç•¥
- **å¼€å‘çŽ¯å¢ƒ**: è‡ªåŠ¨éƒ¨ç½²æ¯æ¬¡æäº¤
- **æµ‹è¯•çŽ¯å¢ƒ**: è‡ªåŠ¨éƒ¨ç½²ç¨³å®šåˆ†æ”¯
- **é¢„ç”Ÿäº§çŽ¯å¢ƒ**: æ‰‹åŠ¨æ‰¹å‡†éƒ¨ç½²
- **ç”Ÿäº§çŽ¯å¢ƒ**: è“ç»¿éƒ¨ç½²æˆ–é‡‘ä¸é›€éƒ¨ç½²

## 3. å›žæ»šç­–ç•¥
- Gitå›žæ»š: æ¢å¤åˆ°ä¹‹å‰çš„éƒ¨ç½²æ¸…å•
- Kuberneteså›žæ»š: ä½¿ç”¨rolloutå‘½ä»¤
- é•œåƒå›žæ»š: åˆ‡æ¢åˆ°ç¨³å®šç‰ˆæœ¬é•œåƒ
EOF
```

### 2. GitHub Actions CI/CDç®¡é“

```bash
cat > ci-cd/github-actions.yaml << 'EOF'
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run linting
      run: npm run lint
    
    - name: Run unit tests
      run: npm test
    
    - name: Run security scan
      run: npm audit

  build:
    name: Build and Push Image
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
      with:
        repository: your-org/k8s-manifests
        token: ${{ secrets.GITOPS_TOKEN }}
    
    - name: Update staging manifest
      run: |
        NEW_TAG=$(echo $GITHUB_SHA | cut -c1-7)
        sed -i "s|image: .*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:develop-${NEW_TAG}|" staging/deployment.yaml
        
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add staging/deployment.yaml
        git commit -m "Update staging image to ${NEW_TAG}"
        git push

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v3
      with:
        repository: your-org/k8s-manifests
        token: ${{ secrets.GITOPS_TOKEN }}
    
    - name: Update production manifest
      run: |
        NEW_TAG=$(echo $GITHUB_SHA | cut -c1-7)
        sed -i "s|image: .*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main-${NEW_TAG}|" production/deployment.yaml
        
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add production/deployment.yaml
        git commit -m "Update production image to ${NEW_TAG}"
        git push
EOF
```

### 3. ArgoCD GitOpséƒ¨ç½²

```bash
cat > gitops/argocd-install.yaml << 'EOF'
# ArgoCD å®‰è£…é…ç½®
apiVersion: v1
kind: Namespace
metadata:
  name: argocd
---
# ä½¿ç”¨å®˜æ–¹å®‰è£…æ¸…å•
# kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ArgoCD åº”ç”¨é…ç½®
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: production-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: HEAD
    path: production
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 3
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

# å®‰è£…ArgoCD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ç­‰å¾…ArgoCDå°±ç»ª
kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd

# èŽ·å–åˆå§‹å¯†ç 
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

## ðŸ—ï¸ é«˜å¯ç”¨æž¶æž„å®žè·µ

### 1. å¤šMasterèŠ‚ç‚¹é…ç½®

```bash
cat > ha/ha-cluster-setup.md << 'EOF'
# é«˜å¯ç”¨é›†ç¾¤æ­å»º

## 1. è´Ÿè½½å‡è¡¡å™¨é…ç½®
ä½¿ç”¨äº‘è´Ÿè½½å‡è¡¡å™¨æˆ–HAProxyä¸ºAPI Serveræä¾›é«˜å¯ç”¨ï¼š

```yaml
# HAProxyé…ç½®ç¤ºä¾‹
global
    daemon

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend k8s-api
    bind *:6443
    default_backend k8s-masters

backend k8s-masters
    balance roundrobin
    server master1 10.0.1.10:6443 check
    server master2 10.0.1.11:6443 check
    server master3 10.0.1.12:6443 check
```

## 2. etcdé›†ç¾¤é…ç½®
ç¡®ä¿etcdè¿è¡Œåœ¨å¥‡æ•°èŠ‚ç‚¹ä¸Š(3æˆ–5ä¸ªèŠ‚ç‚¹)
```

cat > ha/etcd-backup.sh << 'EOF'
#!/bin/bash
# etcdå¤‡ä»½è„šæœ¬

ETCDCTL_API=3
BACKUP_DIR="/backup/etcd/$(date +%Y%m%d-%H%M%S)"
ENDPOINT="https://127.0.0.1:2379"

mkdir -p $BACKUP_DIR

# åˆ›å»ºetcdå¿«ç…§
etcdctl snapshot save $BACKUP_DIR/etcd-snapshot.db \
  --endpoints=$ENDPOINT \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# éªŒè¯å¿«ç…§
etcdctl snapshot status $BACKUP_DIR/etcd-snapshot.db --write-out=table

echo "Backup completed: $BACKUP_DIR/etcd-snapshot.db"

# æ¸…ç†7å¤©å‰çš„å¤‡ä»½
find /backup/etcd -type d -mtime +7 -exec rm -rf {} \;
EOF

chmod +x ha/etcd-backup.sh
```

### 2. åº”ç”¨å±‚é«˜å¯ç”¨è®¾è®¡

```bash
cat > ha/app-ha-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: high-availability-app
  namespace: production
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # ä¿è¯å¯ç”¨æ€§
  selector:
    matchLabels:
      app: ha-app
  template:
    metadata:
      labels:
        app: ha-app
    spec:
      # åäº²å’Œæ€§ç¡®ä¿Podåˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ha-app
            topologyKey: kubernetes.io/hostname
      
      containers:
      - name: app
        image: nginx:1.21-alpine
        ports:
        - containerPort: 80
        
        # å¥åº·æ£€æŸ¥é…ç½®
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # èµ„æºé™åˆ¶
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        
        # ä¼˜é›…å…³é—­
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 15
      
      # å®¹å¿åº¦é…ç½®
      tolerations:
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 30
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: ha-app-service
  namespace: production
spec:
  selector:
    app: ha-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
# æœåŠ¡ä¸­æ–­é¢„ç®—
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ha-app-pdb
  namespace: production
spec:
  minAvailable: 2  # è‡³å°‘ä¿æŒ2ä¸ªPodè¿è¡Œ
  selector:
    matchLabels:
      app: ha-app
EOF
```

## âš¡ æ€§èƒ½è°ƒä¼˜å®žè·µ

### 1. èµ„æºé…é¢å’Œé™åˆ¶

```bash
cat > monitoring/resource-quota.yaml << 'EOF'
# å‘½åç©ºé—´èµ„æºé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    services: "20"
    secrets: "20"
    configmaps: "20"
---
# CPUé™åˆ¶èŒƒå›´
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    type: Container
EOF

kubectl apply -f monitoring/resource-quota.yaml
```

### 2. èŠ‚ç‚¹æ€§èƒ½ä¼˜åŒ–

```bash
cat > monitoring/node-performance.yaml << 'EOF'
# èŠ‚ç‚¹æ€§èƒ½ç›‘æŽ§DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-performance-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-monitor
  template:
    metadata:
      labels:
        app: node-monitor
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:latest
        args:
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--path.rootfs=/host/root'
        - '--collector.filesystem.ignored-mount-points'
        - '^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
        ports:
        - containerPort: 9100
          hostPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host/root
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
      tolerations:
      - effect: NoSchedule
        operator: Exists
EOF

kubectl apply -f monitoring/node-performance.yaml
```

### 3. åº”ç”¨æ€§èƒ½ä¼˜åŒ–é…ç½®

```bash
cat > monitoring/performance-tuning.yaml << 'EOF'
# æ€§èƒ½ä¼˜åŒ–çš„åº”ç”¨éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-app
  namespace: production
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  selector:
    matchLabels:
      app: optimized-app
  template:
    metadata:
      labels:
        app: optimized-app
      annotations:
        # æ€§èƒ½ç›¸å…³æ³¨è§£
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:
      # æ€§èƒ½ä¼˜åŒ–é…ç½®
      priority: 1000  # é«˜ä¼˜å…ˆçº§
      priorityClassName: high-priority
      
      # è°ƒåº¦ä¼˜åŒ–
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-type
                operator: In
                values:
                - high-performance
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - optimized-app
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: app
        image: your-app:optimized
        ports:
        - containerPort: 8080
        
        # JVMä¼˜åŒ–å‚æ•°
        env:
        - name: JAVA_OPTS
          value: "-Xms512m -Xmx1g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"
        - name: SPRING_PROFILES_ACTIVE
          value: "production"
        
        # èµ„æºç²¾ç¡®é…ç½®
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        
        # ä¼˜åŒ–çš„å¥åº·æ£€æŸ¥
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 20
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
---
# é«˜ä¼˜å…ˆçº§ç±»
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "High priority class for critical applications"
EOF
```

## ðŸ”’ å®‰å…¨åŠ å›ºé…ç½®

### 1. Podå®‰å…¨æ ‡å‡†

```bash
cat > security/pod-security.yaml << 'EOF'
# Podå®‰å…¨ç­–ç•¥
apiVersion: v1
kind: Namespace
metadata:
  name: secure-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
# å®‰å…¨çš„åº”ç”¨éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: secure-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
    spec:
      serviceAccountName: secure-app-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      
      containers:
      - name: app
        image: nginx:alpine
        ports:
        - containerPort: 8080
        
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: var-cache
          mountPath: /var/cache/nginx
        - name: var-run
          mountPath: /var/run
      
      volumes:
      - name: tmp
        emptyDir: {}
      - name: var-cache
        emptyDir: {}
      - name: var-run
        emptyDir: {}
---
# æœåŠ¡è´¦æˆ·
apiVersion: v1
kind: ServiceAccount
metadata:
  name: secure-app-sa
  namespace: secure-namespace
automountServiceAccountToken: false
EOF
```

### 2. ç½‘ç»œå®‰å…¨ç­–ç•¥

```bash
cat > security/network-policies.yaml << 'EOF'
# é»˜è®¤æ‹’ç»ç­–ç•¥
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
# å…è®¸åº”ç”¨é—´é€šä¿¡
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-communication
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: gateway
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080
  - to: []  # å…è®¸DNSè§£æž
    ports:
    - protocol: UDP
      port: 53
---
# æ•°æ®åº“è®¿é—®ç­–ç•¥
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432
EOF
```

## ðŸ’¾ å¤‡ä»½ä¸Žç¾éš¾æ¢å¤

### 1. Veleroå¤‡ä»½ç³»ç»Ÿ

```bash
cat > backup/velero-setup.sh << 'EOF'
#!/bin/bash

# å®‰è£…Velero
curl -fsSL -o velero-v1.12.0-linux-amd64.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xzf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# é…ç½®AWS S3å­˜å‚¨ï¼ˆç¤ºä¾‹ï¼‰
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket velero-backups \
  --secret-file ./credentials-velero \
  --backup-location-config region=us-west-2 \
  --snapshot-location-config region=us-west-2

# åˆ›å»ºå¤‡ä»½
velero backup create backup-$(date +%Y%m%d) --include-namespaces production

# å®šæœŸå¤‡ä»½
velero schedule create daily-backup --schedule="0 1 * * *" --include-namespaces production --ttl 720h
EOF

cat > backup/backup-strategy.yaml << 'EOF'
# å¤‡ä»½ç­–ç•¥é…ç½®
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: production-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  template:
    includedNamespaces:
    - production
    - monitoring
    excludedResources:
    - events
    - replicationcontrollers
    - endpoints
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h0m0s  # ä¿ç•™30å¤©
EOF
```

### 2. ç¾éš¾æ¢å¤æ¼”ç»ƒ

```bash
cat > backup/disaster-recovery.sh << 'EOF'
#!/bin/bash

echo "=== ç¾éš¾æ¢å¤æ¼”ç»ƒ ==="

# 1. æ¨¡æ‹Ÿæ•…éšœ
echo "1. æ¨¡æ‹Ÿåº”ç”¨æ•…éšœ..."
kubectl delete namespace production

# 2. æ£€æŸ¥å¤‡ä»½
echo "2. æ£€æŸ¥å¯ç”¨å¤‡ä»½..."
velero backup get

# 3. æ‰§è¡Œæ¢å¤
echo "3. æ‰§è¡Œæ¢å¤æ“ä½œ..."
LATEST_BACKUP=$(velero backup get --output json | jq -r '.items[0].metadata.name')
velero restore create restore-$(date +%Y%m%d-%H%M%S) --from-backup $LATEST_BACKUP

# 4. éªŒè¯æ¢å¤
echo "4. éªŒè¯æ¢å¤çŠ¶æ€..."
kubectl get pods -n production
kubectl get services -n production

echo "ç¾éš¾æ¢å¤æ¼”ç»ƒå®Œæˆ"
EOF

chmod +x backup/disaster-recovery.sh
```

## ðŸ§ª ç”Ÿäº§çŽ¯å¢ƒå®žæˆ˜æ¼”ç»ƒ

### æ¼”ç»ƒ1ï¼šé›¶åœæœºéƒ¨ç½²

```bash
cat > ci-cd/zero-downtime-deployment.yaml << 'EOF'
# é›¶åœæœºéƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zero-downtime-app
  namespace: production
spec:
  replicas: 6
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 50%      # æœ€å¤šå¢žåŠ 50%çš„Pod
      maxUnavailable: 0   # ä¸å…è®¸ä¸å¯ç”¨çš„Pod
  selector:
    matchLabels:
      app: zero-downtime-app
  template:
    metadata:
      labels:
        app: zero-downtime-app
    spec:
      containers:
      - name: app
        image: nginx:1.21-alpine
        ports:
        - containerPort: 80
        
        # å…³é”®ï¼šç²¾ç¡®çš„å¥åº·æ£€æŸ¥
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
        
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # ä¼˜é›…å…³é—­
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 15  # ç­‰å¾…è¿žæŽ¥å¤„ç†å®Œæˆ
        
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "200m"
---
# æœåŠ¡é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: zero-downtime-service
  namespace: production
spec:
  selector:
    app: zero-downtime-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
# ä¸­æ–­é¢„ç®—
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zero-downtime-pdb
  namespace: production
spec:
  minAvailable: 3  # è‡³å°‘ä¿æŒ3ä¸ªPodè¿è¡Œ
  selector:
    matchLabels:
      app: zero-downtime-app
EOF
```

### æ¼”ç»ƒ2ï¼šé‡‘ä¸é›€éƒ¨ç½²

```bash
cat > ci-cd/canary-deployment.yaml << 'EOF'
# é‡‘ä¸é›€éƒ¨ç½² - ç¨³å®šç‰ˆæœ¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-stable
  namespace: production
  labels:
    version: stable
spec:
  replicas: 9  # 90%æµé‡
  selector:
    matchLabels:
      app: canary-app
      version: stable
  template:
    metadata:
      labels:
        app: canary-app
        version: stable
    spec:
      containers:
      - name: app
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "stable"
---
# é‡‘ä¸é›€éƒ¨ç½² - æ–°ç‰ˆæœ¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-canary
  namespace: production
  labels:
    version: canary
spec:
  replicas: 1  # 10%æµé‡
  selector:
    matchLabels:
      app: canary-app
      version: canary
  template:
    metadata:
      labels:
        app: canary-app
        version: canary
    spec:
      containers:
      - name: app
        image: nginx:1.21-alpine
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "canary"
---
# ç»Ÿä¸€æœåŠ¡
apiVersion: v1
kind: Service
metadata:
  name: canary-service
  namespace: production
spec:
  selector:
    app: canary-app
  ports:
  - port: 80
    targetPort: 80
EOF
```

## ðŸ“Š ç”Ÿäº§çŽ¯å¢ƒç›‘æŽ§ä»ªè¡¨æ¿

```bash
cat > monitoring/production-dashboard.json << 'EOF'
{
  "dashboard": {
    "title": "Production Environment Overview",
    "panels": [
      {
        "title": "Cluster Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"kubernetes-nodes\"}",
            "legendFormat": "Node Status"
          }
        ]
      },
      {
        "title": "Application Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "Request Rate"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th Percentile Latency"
          }
        ]
      },
      {
        "title": "Resource Utilization",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total[5m])",
            "legendFormat": "CPU Usage"
          },
          {
            "expr": "container_memory_usage_bytes",
            "legendFormat": "Memory Usage"
          }
        ]
      }
    ]
  }
}
EOF
```

## ðŸŽ¯ ç”Ÿäº§çŽ¯å¢ƒæ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

```bash
cat > production-checklist.md << 'EOF'
# ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•

## ðŸ” å®‰å…¨æ£€æŸ¥
- [ ] RBACæƒé™é…ç½®æ­£ç¡®
- [ ] NetworkPolicyç½‘ç»œéš”ç¦»
- [ ] Podå®‰å…¨ä¸Šä¸‹æ–‡é…ç½®
- [ ] SecretåŠ å¯†å­˜å‚¨
- [ ] é•œåƒå®‰å…¨æ‰«æé€šè¿‡
- [ ] å‡†å…¥æŽ§åˆ¶å™¨é…ç½®

## ðŸ—ï¸ é«˜å¯ç”¨æ£€æŸ¥
- [ ] å¤šå‰¯æœ¬éƒ¨ç½²(æœ€å°‘3ä¸ª)
- [ ] Podåäº²å’Œæ€§é…ç½®
- [ ] å¥åº·æ£€æŸ¥é…ç½®å®Œæ•´
- [ ] æœåŠ¡ä¸­æ–­é¢„ç®—è®¾ç½®
- [ ] è´Ÿè½½å‡è¡¡é…ç½®æ­£ç¡®
- [ ] æ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆ

## âš¡ æ€§èƒ½æ£€æŸ¥
- [ ] èµ„æºè¯·æ±‚å’Œé™åˆ¶è®¾ç½®
- [ ] HPAè‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®
- [ ] é•œåƒä¼˜åŒ–(å¤šé˜¶æ®µæž„å»º)
- [ ] JVMå‚æ•°è°ƒä¼˜
- [ ] æ•°æ®åº“è¿žæŽ¥æ± ä¼˜åŒ–
- [ ] ç¼“å­˜ç­–ç•¥é…ç½®

## ðŸ“Š ç›‘æŽ§æ£€æŸ¥
- [ ] Prometheusç›‘æŽ§é…ç½®
- [ ] Grafanaä»ªè¡¨æ¿åˆ›å»º
- [ ] å‘Šè­¦è§„åˆ™è®¾ç½®
- [ ] æ—¥å¿—æ”¶é›†é…ç½®
- [ ] é“¾è·¯è¿½è¸ªé›†æˆ
- [ ] æ€§èƒ½æŒ‡æ ‡æš´éœ²

## ðŸ’¾ å¤‡ä»½æ£€æŸ¥
- [ ] Veleroå¤‡ä»½ç­–ç•¥
- [ ] æ•°æ®å¤‡ä»½è‡ªåŠ¨åŒ–
- [ ] é…ç½®æ–‡ä»¶ç‰ˆæœ¬æŽ§åˆ¶
- [ ] ç¾éš¾æ¢å¤æ–¹æ¡ˆæµ‹è¯•
- [ ] RTO/RPOç›®æ ‡å®šä¹‰
- [ ] å¤‡ä»½éªŒè¯æµç¨‹

## ðŸš€ éƒ¨ç½²æ£€æŸ¥
- [ ] CI/CDç®¡é“æµ‹è¯•
- [ ] é‡‘ä¸é›€/è“ç»¿éƒ¨ç½²ç­–ç•¥
- [ ] å›žæ»šæ–¹æ¡ˆå‡†å¤‡
- [ ] é›¶åœæœºéƒ¨ç½²éªŒè¯
- [ ] çŽ¯å¢ƒé…ç½®åŒæ­¥
- [ ] ä¾èµ–æœåŠ¡æ£€æŸ¥
EOF
```

## ðŸŽ“ å­¦ä¹ æ£€æŸ¥

å®Œæˆæœ¬å®žéªŒåŽï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

1. **ç”Ÿäº§çŽ¯å¢ƒæž¶æž„è®¾è®¡**ï¼š
   - è®¾è®¡é«˜å¯ç”¨çš„Kubernetesé›†ç¾¤
   - è§„åˆ’åº”ç”¨çš„éƒ¨ç½²æž¶æž„
   - åˆ¶å®šå®¹ç¾å¤‡ä»½ç­–ç•¥

2. **CI/CDä¸ŽGitOps**ï¼š
   - å®žçŽ°å®Œæ•´çš„CI/CDç®¡é“
   - é…ç½®GitOpså·¥ä½œæµ
   - å®žçŽ°è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå›žæ»š

3. **æ€§èƒ½ä¸Žå®‰å…¨ä¼˜åŒ–**ï¼š
   - è¿›è¡Œèµ„æºè°ƒä¼˜å’Œæ€§èƒ½ç›‘æŽ§
   - å®žæ–½å®‰å…¨ç­–ç•¥å’Œåˆè§„è¦æ±‚
   - é…ç½®ç”Ÿäº§çº§ç›‘æŽ§å‘Šè­¦

4. **è¿ç»´æœ€ä½³å®žè·µ**ï¼š
   - æ‰§è¡Œé›¶åœæœºéƒ¨ç½²
   - å¤„ç†ç”Ÿäº§çŽ¯å¢ƒæ•…éšœ
   - è¿›è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ

## ðŸŽ‰ æ­å–œå®ŒæˆKuberneteså­¦ä¹ ä½“ç³»

ä½ å·²ç»å®Œæˆäº†ä»Žå…¥é—¨åˆ°ç”Ÿäº§çŽ¯å¢ƒçš„å®Œæ•´Kuberneteså­¦ä¹ è·¯å¾„ï¼çŽ°åœ¨ä½ å…·å¤‡äº†ï¼š

- **åŸºç¡€èƒ½åŠ›**ï¼šPodã€Serviceã€Deploymentç­‰æ ¸å¿ƒæ¦‚å¿µ
- **è¿›é˜¶æŠ€èƒ½**ï¼šå­˜å‚¨ã€ç½‘ç»œã€å®‰å…¨é…ç½®
- **é«˜çº§å®žè·µ**ï¼šç›‘æŽ§ã€æ‰©ç¼©å®¹ã€ç”Ÿäº§çŽ¯å¢ƒæœ€ä½³å®žè·µ

ç»§ç»­ä¿æŒå­¦ä¹ ï¼Œå…³æ³¨Kubernetesç”Ÿæ€çš„æœ€æ–°å‘å±•ï¼

---

**ðŸŽ¯ ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
- æ·±å…¥å­¦ä¹ IstioæœåŠ¡ç½‘æ ¼
- æŽ¢ç´¢Kubernetes Operatorå¼€å‘
- å‚ä¸Žå¼€æºç¤¾åŒºè´¡çŒ®
- å‡†å¤‡CKA/CKSè®¤è¯è€ƒè¯• 
# 实验十一：攻防演化分析与评估

## 🎯 实验目标
1. 学会如何科学地度量和评估一个动态对抗系统的性能。
2. 设计并实现用于监控攻击者和检测者能力的关键指标。
3. 对实验十中训练的系统进行全面的性能分析和可视化。
4. 撰写完整的项目总结报告，展示对整个技术栈的理解和实践成果。

## 📖 理论背景
训练一个复杂的系统只是第一步，更重要的是如何客观地评估它的效果。我们需要设计一套超越简单损失值的评估体系，来真正理解智能体之间的协同进化动态。这不仅是检验项目成功与否的标准，也是从实验中获得深刻见解（Insight）的关键。

## 🛠️ 实践内容
1. **设计评估指标**:
    - **攻击者指标**:
        - **攻击成功率**: 攻击者生成的样本成功骗过（当前版本）检测者的比率。
        - **策略多样性**: 衡量攻击者生成的攻击样本的语义多样性（例如，使用聚类或文本嵌入向量的方差）。
    - **检测者指标**:
        - **检测准确率**: 在一个固定的、包含多种已知攻击类型的测试集上的表现。这可以衡量检测者的泛化能力。
        - **鲁棒性**: 面对全新类型（OOD, Out-of-Distribution）攻击时的表现。
    - **系统指标**:
        - **攻防能力比**: `攻击成功率` vs. `检测准确率` 的曲线，观察是否存在"军备竞赛"的螺旋上升。

2. **实现评估流程**:
   - 编写一个独立的评估脚本，该脚本可以在训练过程中的任意检查点（Checkpoint）载入攻击者和检测者模型，并计算上述所有指标。
3. **数据可视化**:
   - 从训练日志和评估脚本中收集数据。
   - 使用`matplotlib`或`seaborn`等库，绘制以下关键图表：
     - 攻击者与检测者能力随训练步数变化的曲线图。
     - 攻击策略多样性随时间变化的图。
     - 最终检测者在不同攻击类型上的混淆矩阵。
4. **项目总结**:
   - 综合所有实验的观察和数据，撰写一份完整的总结报告。
   - 报告应包括：对核心算法（PPO, GRPO, DAPO, VeRL, APO）的理解、系统实现细节、性能分析、遇到的挑战与解决方案，以及对该技术未来发展的思考。 
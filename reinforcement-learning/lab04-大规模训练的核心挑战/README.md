# 实验四：大规模训练的核心挑战

## 🎯 实验目标
1. 深入理解"熵坍塌"（Entropy Collapse）问题及其对RL训练的危害。
2. 深入理解"奖励噪声"（Reward Noise）和"梯度消失"问题。
3. 通过实验，复现这两种训练不稳定性问题。
4. 认识到在复杂的动态博弈环境中，解决这些问题的重要性。

## 📖 理论背景
- **熵坍塌**: 在RL训练中，策略（Policy）的熵代表其随机性和探索性。熵坍塌指模型过快地收敛到少数几个高奖励的动作上，失去了探索其他可能更优解的能力。这在需要持续创新的攻防博弈中是致命的。
- **奖励噪声与梯度消失**: 当一个训练批次（Batch）中的所有样本奖励信号都一样时（例如，全是"好"的或全是"坏"的），计算出的优势（Advantage）会非常接近于零，导致有效的梯度信号消失，模型无法学习。这在对抗环境中很常见，因为攻击者可能生成一批同样有效或同样无效的攻击。

## 🛠️ 实践内容
1. **复现熵坍塌**:
   - 设计一个具有多个局部最优解的环境。
   - 使用前序实验中的PPO/GRPO进行训练，并调高学习率或减小熵奖励系数。
   - 观察并记录模型策略的熵值变化，以及它如何陷入某个局部最优解而无法探索全局最优解。
2. **复现梯度消失**:
   - 人为构造一个训练批次，使其奖励信号高度同质化（例如，所有样本的奖励都设置为1或0）。
   - 在训练循环中打印出计算的梯度范数（Gradient Norm）。
   - 观察当输入同质化奖励的批次时，梯度范数如何骤降至接近于零，导致训练停滞。
3. **分析与总结**: 撰写报告，分析这两种问题在"攻击-检测"博弈场景中会如何具体表现，并为下一节学习DAPO做好铺垫。 
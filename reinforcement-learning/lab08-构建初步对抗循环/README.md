# 实验八：构建初步对抗循环

## 🎯 实验目标
1. 将APO框架应用到LLM场景。
2. 构建第一个版本的"攻击-检测"对抗系统。
3. **攻击者**: 使用实验五中实现的DAPO算法进行训练。
4. **检测者**: 使用一个标准的、基于学习的奖励/分类模型。
5. 运行对抗循环，观察系统动态，并识别潜在的不稳定性。

## 📖 理论背景
这是将前序所有理论和实践进行初步集成的关键一步。我们将构建一个完整的训练循环，其中包含两个相互作用的智能体。然而，由于检测者是一个标准的"学习"模型，它本身的学习目标（最小化分类损失）可能会被更强大的攻击者"操控"，导致训练过程不稳定或崩溃。

## 🛠️ 实践内容
1. **组件准备**:
    - **攻击者**: 载入一个预训练的LLM，并配置好DAPO训练器。其奖励信号直接来自于检测者的输出。
    - **检测者**: 一个带有分类头的LLM，用于区分"真实良性数据"和"攻击者生成的数据"。
    - **数据集**: 一批真实的良性数据样本。
2. **实现对抗循环**:
   ```python
   for epoch in range(num_epochs):
       # --- 攻击者训练阶段 ---
       # 1. 攻击者生成一批"攻击"样本
       attacks = attacker.generate(prompts)
       # 2. 检测者为攻击样本打分 (作为奖励)
       rewards = detector.predict(attacks)
       # 3. 使用(attacks, rewards)通过DAPO训练攻击者
       attacker.train_step()

       # --- 检测者训练阶段 ---
       # 1. 准备训练数据：真实的良性样本(label=0) + 攻击者生成的样本(label=1)
       train_data = real_data + attacks
       # 2. 在该数据上训练检测者 (标准的监督学习/分类任务)
       detector.train_step()
   ```
3. **运行与观察**: 启动训练循环，并密切监控：
    - 攻击者的奖励变化。
    - 检测者的分类准确率和损失。
    - 观察是否出现"模式崩溃"(Mode Collapse)，即攻击者只生成一种类型的攻击，或者检测者完全失效。
4. **分析**: 总结这个初步系统的优点和(预期的)缺点。为什么一个基于学习的检测者很难在强大的攻击者面前保持稳定？ 
#!/usr/bin/env python3
"""
Lab05: DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization) ÂÆûÁé∞

Êú¨ÂÆûÈ™åÂÆûÁé∞DAPOÁÆóÊ≥ïÁöÑ‰∏§Â§ßÊ†∏ÂøÉÁªÑ‰ª∂Ôºö
1. Clip-Higher: ÊîæÂÆΩÂØπ‰ΩéÊ¶ÇÁéáË°å‰∏∫ÁöÑÊÉ©ÁΩöÔºåÈºìÂä±Êé¢Á¥¢
2. Âä®ÊÄÅÈááÊ†∑: ËøáÊª§ÊéâÂ•ñÂä±‰ø°Âè∑Ëøá‰∫éÂçï‰∏ÄÁöÑÊ†∑Êú¨ÁªÑ

ÁõÆÊ†áÔºöËß£ÂÜ≥Lab04‰∏≠ÂèëÁé∞ÁöÑÁÜµÂùçÂ°åÂíåÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò

‰ΩúËÄÖ: SystemLearning Project
Êó•Êúü: 2024-12-19
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from typing import List, Tuple, Dict, Optional
import logging
from dataclasses import dataclass
import time
from collections import defaultdict
import warnings

# ÈÖçÁΩÆmatplotlibÂ≠ó‰Ωì
def setup_matplotlib():
    """ÈÖçÁΩÆmatplotlibÊòæÁ§∫"""
    try:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        print("‚úÖ Matplotlib configuration successful")
    except Exception as e:
        print(f"‚ö†Ô∏è Matplotlib configuration failed: {e}")

setup_matplotlib()

@dataclass
class TrainingMetrics:
    """ËÆ≠ÁªÉËøáÁ®ãÁöÑÂÖ≥ÈîÆÊåáÊ†á"""
    step: int
    entropy: float
    gradient_norm: float
    loss: float
    action_distribution: List[float]
    reward_mean: float
    reward_std: float
    clip_ratio: float = 0.0
    filtered_ratio: float = 0.0  # Âä®ÊÄÅÈááÊ†∑ËøáÊª§ÊØî‰æã

class ExplainerSystem:
    """Â∞èÁôΩÂèãÂ•ΩÁöÑËß£ÈáäÁ≥ªÁªü"""
    
    @staticmethod
    def explain_concept(concept: str) -> str:
        """Ëß£ÈáäÂÖ≥ÈîÆÊ¶ÇÂøµ"""
        explanations = {
            "dapo": """
üöÄ DAPO Algorithm:
‚Ä¢ DAPO = Advanced version of GRPO for training stability
‚Ä¢ Two core technologies:
  1. Clip-Higher: Encourage exploration of high-reward behaviors
  2. Dynamic Sampling: Filter out low-quality training data
‚Ä¢ Like giving AI a "smart regulator" for stable learning
""",
            "clip_higher": """
üîß Clip-Higher Technology:
‚Ä¢ Traditional PPO/GRPO: Strictly limit policy changes
‚Ä¢ Clip-Higher: Relax limits for promising new behaviors
‚Ä¢ Key idea: Encourage rather than punish high-reward actions
‚Ä¢ Effect: Prevent entropy collapse, maintain exploration
""",
            "dynamic_sampling": """
üìä Dynamic Sampling Technology:
‚Ä¢ Problem: Monotonous training data leads to poor learning
‚Ä¢ Solution: Smart filter ensures data diversity in each batch
‚Ä¢ Like selecting study materials: need both good and bad examples
‚Ä¢ Effect: Prevent gradient vanishing, ensure effective learning
"""
        }
        return explanations.get(concept, f"Explanation for '{concept}' not implemented")

class UnstableEnvironment:
    """Unstable training environment from Lab04"""
    
    def __init__(self):
        self.action_rewards = [0.3, 1.0, 0.5]  # Action 0, 1, 2 rewards
        
    def get_state(self) -> torch.Tensor:
        """Get state (simplified as fixed state)"""
        return torch.zeros(4)
    
    def step(self, action: int) -> Tuple[torch.Tensor, float]:
        """Execute action and return reward"""
        reward = self.action_rewards[action]
        # Add noise to simulate real environment
        noise = np.random.normal(0, 0.1)
        reward = max(0, reward + noise)
        return self.get_state(), reward

class PolicyNetwork(nn.Module):
    """Policy network"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)
    
    def get_action_and_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor, torch.Tensor]:
        """Get action and log probability"""
        logits = self.forward(state)
        probs = F.softmax(logits, dim=-1)
        probs = torch.clamp(probs, min=1e-8, max=1.0)
        
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob, probs

class DAPOTrainer:
    """DAPO Trainer - implements Clip-Higher and Dynamic Sampling"""
    
    def __init__(self, 
                 learning_rate: float = 1e-3,
                 entropy_coef: float = 0.01,
                 batch_size: int = 32,
                 clip_ratio: float = 0.2,
                 use_clip_higher: bool = True,
                 use_dynamic_sampling: bool = True,
                 reward_variance_threshold: float = 0.01,
                 verbose: bool = True):
        
        self.env = UnstableEnvironment()
        self.policy = PolicyNetwork(4, 3)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        
        # Training parameters
        self.learning_rate = learning_rate
        self.entropy_coef = entropy_coef
        self.batch_size = batch_size
        self.clip_ratio = clip_ratio
        self.verbose = verbose
        
        # DAPO specific parameters
        self.use_clip_higher = use_clip_higher
        self.use_dynamic_sampling = use_dynamic_sampling
        self.reward_variance_threshold = reward_variance_threshold
        
        # Records
        self.metrics_history = []
        self.explainer = ExplainerSystem()
        
        print(f"üöÄ DAPO Trainer initialized")
        print(f"üìã Config: LR={learning_rate}, Entropy={entropy_coef}, Batch={batch_size}")
        print(f"üîß DAPO Features: Clip-Higher={use_clip_higher}, Dynamic-Sampling={use_dynamic_sampling}")

    def dynamic_sampling_filter(self, batch_data) -> Tuple[List, float]:
        """Dynamic sampling filter - DAPO core component 1"""
        if not self.use_dynamic_sampling:
            return batch_data, 0.0
        
        # Calculate reward statistics
        rewards = [item[2] for item in batch_data]
        reward_std = np.std(rewards)
        
        # Filter if reward variance is too small
        if reward_std < self.reward_variance_threshold:
            if self.verbose and len(self.metrics_history) % 20 == 0:
                print(f"‚ö†Ô∏è Dynamic sampling triggered: reward std {reward_std:.4f} < threshold {self.reward_variance_threshold}")
            
            # Keep samples with higher variance (simplified strategy)
            filtered_data = batch_data[::2]  # Simple filtering strategy
            filter_ratio = 1.0 - len(filtered_data) / len(batch_data)
            return filtered_data, filter_ratio
        
        return batch_data, 0.0

    def clip_higher_loss(self, log_probs, old_log_probs, advantages, rewards) -> torch.Tensor:
        """Clip-Higher loss calculation - DAPO core component 2"""
        ratio = torch.exp(log_probs - old_log_probs)
        
        if not self.use_clip_higher:
            # Standard PPO clipping
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages
            return -torch.min(surr1, surr2).mean()
        
        # Clip-Higher logic: relax upper bound for high-reward samples
        reward_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        high_reward_mask = reward_normalized > 0.5  # High reward samples
        
        surr1 = ratio * advantages
        
        # Use more relaxed upper bound for high-reward samples
        upper_bound = torch.where(high_reward_mask, 
                                 1 + self.clip_ratio * 2,  # Relax by 2x
                                 1 + self.clip_ratio)      # Standard limit
        
        # Ensure all clamp parameters are tensors for PyTorch compatibility
        lower_bound = torch.full_like(ratio, 1 - self.clip_ratio)
        
        surr2 = torch.clamp(ratio, min=lower_bound, max=upper_bound) * advantages
        
        return -torch.min(surr1, surr2).mean()

    def collect_batch(self):
        """Collect training batch data"""
        batch_data = []
        
        for _ in range(self.batch_size):
            state = self.env.get_state()
            action, log_prob, probs = self.policy.get_action_and_log_prob(state)
            next_state, reward = self.env.step(action)
            
            batch_data.append((state, action, reward, log_prob))
        
        # Apply dynamic sampling filter
        filtered_batch, filter_ratio = self.dynamic_sampling_filter(batch_data)
        
        return filtered_batch, filter_ratio

    def compute_advantages(self, rewards: List[float]) -> torch.Tensor:
        """Compute advantage function (simplified version)"""
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        # Simple normalization
        if rewards_tensor.std() > 1e-8:
            advantages = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)
        else:
            advantages = torch.zeros_like(rewards_tensor)
        return advantages

    def train_step(self) -> TrainingMetrics:
        """Execute one DAPO training step"""
        # Collect data
        batch_data, filter_ratio = self.collect_batch()
        
        if len(batch_data) == 0:
            print("‚ö†Ô∏è Dynamic sampling filtered all data, skipping update")
            return None
        
        # Extract data
        states = torch.stack([item[0] for item in batch_data])
        actions = torch.tensor([item[1] for item in batch_data])
        rewards = [item[2] for item in batch_data]
        old_log_probs = torch.stack([item[3] for item in batch_data])
        
        # Compute advantages
        advantages = self.compute_advantages(rewards)
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        
        # Forward pass
        logits = self.policy(states)
        probs = F.softmax(logits, dim=-1)
        probs = torch.clamp(probs, min=1e-8, max=1.0)
        
        log_probs = torch.log(probs.gather(1, actions.unsqueeze(1)).squeeze())
        
        # Calculate loss using Clip-Higher
        policy_loss = self.clip_higher_loss(log_probs, old_log_probs, advantages, rewards_tensor)
        
        # Entropy loss
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
        entropy_loss = -self.entropy_coef * entropy
        
        # Total loss
        total_loss = policy_loss + entropy_loss
        
        # Backward pass
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # Calculate gradient norm
        grad_norm = 0.0
        for param in self.policy.parameters():
            if param.grad is not None:
                grad_norm += param.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5
        
        self.optimizer.step()
        
        # Record metrics
        with torch.no_grad():
            action_dist = probs.mean(dim=0).tolist()
            
        metrics = TrainingMetrics(
            step=len(self.metrics_history),
            entropy=entropy.item(),
            gradient_norm=grad_norm,
            loss=total_loss.item(),
            action_distribution=action_dist,
            reward_mean=np.mean(rewards),
            reward_std=np.std(rewards),
            clip_ratio=self.clip_ratio,
            filtered_ratio=filter_ratio
        )
        
        self.metrics_history.append(metrics)
        return metrics

    def generate_report(self, experiment_type: str) -> str:
        """ÁîüÊàêËØ¶ÁªÜÁöÑËÆ≠ÁªÉÊä•Âëä"""
        if not self.metrics_history:
            return "‚ùå Ê≤°ÊúâËÆ≠ÁªÉÊï∞ÊçÆ"
        
        initial_entropy = self.metrics_history[0].entropy
        final_entropy = self.metrics_history[-1].entropy
        entropy_change = (initial_entropy - final_entropy) / initial_entropy * 100
        
        initial_grad = self.metrics_history[0].gradient_norm
        final_grad = self.metrics_history[-1].gradient_norm
        
        final_dist = self.metrics_history[-1].action_distribution
        dominant_action = final_dist.index(max(final_dist))
        
        # ËÆ°ÁÆóËøáÊª§ÁªüËÆ°
        filter_ratios = [m.filtered_ratio for m in self.metrics_history]
        avg_filter_ratio = np.mean(filter_ratios)
        
        report = f"""
üîç {experiment_type} ÂÆûÈ™åËØ¶ÁªÜÂàÜÊûêÊä•Âëä
==================================================

üìà ËÆ≠ÁªÉÊ¶ÇÂÜµ:
‚Ä¢ ÊÄªËÆ≠ÁªÉÊ≠•Êï∞: {len(self.metrics_history)}
‚Ä¢ ÂàùÂßãÁÜµÂÄº: {initial_entropy:.4f} ‚Üí ÊúÄÁªàÁÜµÂÄº: {final_entropy:.4f}
‚Ä¢ ÁÜµÂÄºÂèòÂåñ: {entropy_change:.1f}%
‚Ä¢ ÂàùÂßãÊ¢ØÂ∫¶: {initial_grad:.4f} ‚Üí ÊúÄÁªàÊ¢ØÂ∫¶: {final_grad:.4f}

üéØ ÊúÄÁªàÂä®‰ΩúÂÅèÂ•Ω:
‚Ä¢ Âä®‰Ωú0 (Â•ñÂä±0.3): {final_dist[0]:.3f} ({final_dist[0]*100:.1f}%)
‚Ä¢ Âä®‰Ωú1 (Â•ñÂä±1.0): {final_dist[1]:.3f} ({final_dist[1]*100:.1f}%)  ‚≠êÊúÄ‰ºò
‚Ä¢ Âä®‰Ωú2 (Â•ñÂä±0.5): {final_dist[2]:.3f} ({final_dist[2]*100:.1f}%)
‚Ä¢ ‰∏ªÂØºÂä®‰Ωú: Âä®‰Ωú{dominant_action}

üîß DAPOÊäÄÊúØÁªüËÆ°:
‚Ä¢ Clip-HigherÂêØÁî®: {'‚úÖ' if self.use_clip_higher else '‚ùå'}
‚Ä¢ Âä®ÊÄÅÈááÊ†∑ÂêØÁî®: {'‚úÖ' if self.use_dynamic_sampling else '‚ùå'}
‚Ä¢ Âπ≥ÂùáËøáÊª§ÊØî‰æã: {avg_filter_ratio*100:.1f}%

üí° ÁªìÊûúËß£Èáä:
{'‚úÖ DAPOÊàêÂäü‰øùÊåÅ‰∫ÜËÆ≠ÁªÉÁ®≥ÂÆöÊÄß' if entropy_change < 80 else '‚ö†Ô∏è ‰ªçÂ≠òÂú®‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑÁÜµÂùçÂ°å'}
{'‚úÖ Ê¢ØÂ∫¶‰øùÊåÅÁ®≥ÂÆö' if final_grad > 0.01 else '‚ö†Ô∏è Âá∫Áé∞‰∫ÜÊ¢ØÂ∫¶Ê∂àÂ§±Áé∞Ë±°'}

üî¨ ÊäÄÊúØÊïàÊûúÂàÜÊûê:
‚Ä¢ Clip-Higher: {'Â∏ÆÂä©‰øùÊåÅÊé¢Á¥¢ÊÄßÔºåÂáèÁºìÁÜµÂùçÂ°å' if self.use_clip_higher else 'Êú™ÂêØÁî®'}
‚Ä¢ Âä®ÊÄÅÈááÊ†∑: {'ËøáÊª§‰∫Ü{:.1f}%ÁöÑ‰ΩéË¥®ÈáèÊï∞ÊçÆ'.format(avg_filter_ratio*100) if self.use_dynamic_sampling else 'Êú™ÂêØÁî®'}
"""
        return report

def run_grpo_baseline_experiment():
    """ËøêË°åGRPOÂü∫Á∫øÂÆûÈ™åÔºà‰∏ç‰ΩøÁî®DAPOÊäÄÊúØÔºâ"""
    print("üé™ Á¨¨‰∏ÄÂú∫ÂØπÊØîÔºöGRPOÂü∫Á∫øÂÆûÈ™å")
    print("=" * 50)
    print("üéØ ÁõÆÊ†á: Â§çÁé∞Lab04‰∏≠ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò")
    print("‚öôÔ∏è ÊñπÊ≥ï: ‰ΩøÁî®Ê†áÂáÜGRPOÔºå‰∏çÂêØÁî®DAPOÊäÄÊúØ")
    
    trainer = DAPOTrainer(
        learning_rate=5e-3,  # ‰ΩøÁî®Lab04ÁöÑÈ´òÂ≠¶‰π†Áéá
        entropy_coef=0.001,  # ‰ΩøÁî®Lab04ÁöÑ‰ΩéÁÜµÁ≥ªÊï∞
        batch_size=32,
        use_clip_higher=False,      # ÂÖ≥Èó≠Clip-Higher
        use_dynamic_sampling=False,  # ÂÖ≥Èó≠Âä®ÊÄÅÈááÊ†∑
        verbose=False
    )
    
    print(f"üöÄ ÂºÄÂßãËÆ≠ÁªÉ...")
    
    for step in range(100):
        metrics = trainer.train_step()
        if metrics is None:
            continue
            
        # Ê£ÄÊµãÁÜµÂùçÂ°å
        if metrics.entropy < 0.01:
            if step < 90:  # Âè™Âú®Ââç90Ê≠•Êä•ÂëäÔºåÈÅøÂÖçËøáÂ§öËæìÂá∫
                print(f"‚ö†Ô∏è Á¨¨{step+1}Ê≠•ÔºöÁÜµÂÄºËøá‰Ωé({metrics.entropy:.4f})ÔºåÂèØËÉΩÂ∑≤ÁªèÂùçÂ°åÔºÅ")
        
        # ÊØè20Ê≠•ÊòæÁ§∫ËøõÂ∫¶
        if (step + 1) % 20 == 0:
            print(f"üìä Á¨¨{step+1}Ê≠•: ÁÜµ={metrics.entropy:.3f}, Ê¢ØÂ∫¶={metrics.gradient_norm:.3f}, Â•ñÂä±={metrics.reward_mean:.3f}")
    
    print("‚úÖ GRPOÂü∫Á∫øÂÆûÈ™åÂÆåÊàê")
    print(trainer.generate_report("GRPOÂü∫Á∫ø"))
    return trainer.metrics_history

def run_dapo_experiment():
    """ËøêË°åÂÆåÊï¥DAPOÂÆûÈ™å"""
    print("\nüé™ Á¨¨‰∫åÂú∫ÂØπÊØîÔºöÂÆåÊï¥DAPOÂÆûÈ™å")
    print("=" * 50)
    print("üéØ ÁõÆÊ†á: È™åËØÅDAPOÊäÄÊúØËß£ÂÜ≥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÁöÑÊïàÊûú")
    print("‚öôÔ∏è ÊñπÊ≥ï: ÂêØÁî®Clip-HigherÂíåÂä®ÊÄÅÈááÊ†∑ÊäÄÊúØ")
    
    trainer = DAPOTrainer(
        learning_rate=5e-3,  # ÂêåÊ†∑ÁöÑÈ´òÂ≠¶‰π†Áéá
        entropy_coef=0.001,  # ÂêåÊ†∑ÁöÑ‰ΩéÁÜµÁ≥ªÊï∞
        batch_size=32,
        use_clip_higher=True,       # ÂêØÁî®Clip-Higher
        use_dynamic_sampling=True,  # ÂêØÁî®Âä®ÊÄÅÈááÊ†∑
        reward_variance_threshold=0.05,  # Âä®ÊÄÅÈááÊ†∑ÈòàÂÄº
        verbose=False
    )
    
    print(f"üöÄ ÂºÄÂßãËÆ≠ÁªÉ...")
    
    for step in range(100):
        metrics = trainer.train_step()
        if metrics is None:
            continue
            
        # Ê£ÄÊµãÁÜµÂùçÂ°åÔºàÊúüÊúõÂáèÂ∞ëÔºâ
        if metrics.entropy < 0.01:
            print(f"‚ö†Ô∏è Á¨¨{step+1}Ê≠•ÔºöÁÜµÂÄº‰ªçÁÑ∂Ëøá‰Ωé({metrics.entropy:.4f})")
        
        # ÊòæÁ§∫DAPOÊäÄÊúØÂ∑•‰ΩúÊÉÖÂÜµ
        if metrics.filtered_ratio > 0:
            print(f"üîß Á¨¨{step+1}Ê≠•ÔºöÂä®ÊÄÅÈááÊ†∑ËøáÊª§‰∫Ü{metrics.filtered_ratio*100:.1f}%ÁöÑÊï∞ÊçÆ")
        
        # ÊØè20Ê≠•ÊòæÁ§∫ËøõÂ∫¶
        if (step + 1) % 20 == 0:
            print(f"üìä Á¨¨{step+1}Ê≠•: ÁÜµ={metrics.entropy:.3f}, Ê¢ØÂ∫¶={metrics.gradient_norm:.3f}, Â•ñÂä±={metrics.reward_mean:.3f}")
    
    print("‚úÖ DAPOÂÆûÈ™åÂÆåÊàê")
    print(trainer.generate_report("DAPO"))
    return trainer.metrics_history

def create_comparison_visualization(grpo_metrics: List[TrainingMetrics], 
                                  dapo_metrics: List[TrainingMetrics]):
    """ÂàõÂª∫GRPO vs DAPOÂØπÊØîÂèØËßÜÂåñ"""
    print("\nüé® ÁîüÊàêGRPO vs DAPOÂØπÊØîÂàÜÊûêÂõæË°®...")
    
    fig = plt.figure(figsize=(16, 10))
    fig.suptitle('GRPO vs DAPO: Training Stability Comparison', fontsize=16, fontweight='bold')
    
    # ÂÆö‰πâÈ¢úËâ≤
    grpo_color = '#e74c3c'  # Á∫¢Ëâ≤ - GRPO
    dapo_color = '#27ae60'  # ÁªøËâ≤ - DAPO
    
    # ÊèêÂèñÊï∞ÊçÆ
    grpo_steps = list(range(len(grpo_metrics)))
    dapo_steps = list(range(len(dapo_metrics)))
    
    grpo_entropy = [m.entropy for m in grpo_metrics]
    dapo_entropy = [m.entropy for m in dapo_metrics]
    
    grpo_gradients = [m.gradient_norm for m in grpo_metrics]
    dapo_gradients = [m.gradient_norm for m in dapo_metrics]
    
    grpo_rewards = [m.reward_mean for m in grpo_metrics]
    dapo_rewards = [m.reward_mean for m in dapo_metrics]
    
    # 1. ÁÜµÂÄºÂØπÊØî
    ax1 = plt.subplot(2, 3, 1)
    ax1.plot(grpo_steps, grpo_entropy, color=grpo_color, linewidth=2, label='GRPO (Baseline)', marker='o', markersize=3)
    ax1.plot(dapo_steps, dapo_entropy, color=dapo_color, linewidth=2, label='DAPO (Improved)', marker='s', markersize=3)
    ax1.axhline(y=0.01, color='red', linestyle='--', alpha=0.7, label='Collapse Threshold')
    ax1.set_title('Entropy Comparison\n(Higher is better for exploration)', fontweight='bold')
    ax1.set_xlabel('Training Steps')
    ax1.set_ylabel('Entropy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Ê¢ØÂ∫¶ËåÉÊï∞ÂØπÊØî
    ax2 = plt.subplot(2, 3, 2)
    ax2.plot(grpo_steps, grpo_gradients, color=grpo_color, linewidth=2, label='GRPO', marker='o', markersize=3)
    ax2.plot(dapo_steps, dapo_gradients, color=dapo_color, linewidth=2, label='DAPO', marker='s', markersize=3)
    ax2.set_title('Gradient Norm Comparison\n(Stability indicator)', fontweight='bold')
    ax2.set_xlabel('Training Steps')
    ax2.set_ylabel('Gradient Norm')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Â•ñÂä±ÂØπÊØî
    ax3 = plt.subplot(2, 3, 3)
    ax3.plot(grpo_steps, grpo_rewards, color=grpo_color, linewidth=2, label='GRPO', marker='o', markersize=3)
    ax3.plot(dapo_steps, dapo_rewards, color=dapo_color, linewidth=2, label='DAPO', marker='s', markersize=3)
    ax3.set_title('Reward Comparison\n(Performance indicator)', fontweight='bold')
    ax3.set_xlabel('Training Steps')
    ax3.set_ylabel('Average Reward')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ÊúÄÁªàÂä®‰ΩúÂàÜÂ∏ÉÂØπÊØî
    ax4 = plt.subplot(2, 3, 4)
    if grpo_metrics and dapo_metrics:
        grpo_final_dist = grpo_metrics[-1].action_distribution
        dapo_final_dist = dapo_metrics[-1].action_distribution
        
        actions = ['Action 0\n(R=0.3)', 'Action 1\n(R=1.0)\nOptimal', 'Action 2\n(R=0.5)']
        x = np.arange(len(actions))
        width = 0.35
        
        ax4.bar(x - width/2, grpo_final_dist, width, label='GRPO', color=grpo_color, alpha=0.7)
        ax4.bar(x + width/2, dapo_final_dist, width, label='DAPO', color=dapo_color, alpha=0.7)
        
        ax4.set_title('Final Action Distribution\n(Strategy comparison)', fontweight='bold')
        ax4.set_xlabel('Actions')
        ax4.set_ylabel('Probability')
        ax4.set_xticks(x)
        ax4.set_xticklabels(actions)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    
    # 5. ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÁªüËÆ°
    ax5 = plt.subplot(2, 3, 5)
    
    # ËÆ°ÁÆóÁ®≥ÂÆöÊÄßÊåáÊ†á
    grpo_entropy_var = np.var(grpo_entropy[50:])  # ÂêéÂçäÊÆµÊñπÂ∑Æ
    dapo_entropy_var = np.var(dapo_entropy[50:])
    
    grpo_grad_var = np.var(grpo_gradients[50:])
    dapo_grad_var = np.var(dapo_gradients[50:])
    
    metrics_names = ['Entropy\nVariance', 'Gradient\nVariance']
    grpo_values = [grpo_entropy_var, grpo_grad_var]
    dapo_values = [dapo_entropy_var, dapo_grad_var]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    ax5.bar(x - width/2, grpo_values, width, label='GRPO', color=grpo_color, alpha=0.7)
    ax5.bar(x + width/2, dapo_values, width, label='DAPO', color=dapo_color, alpha=0.7)
    
    ax5.set_title('Training Stability\n(Lower variance = more stable)', fontweight='bold')
    ax5.set_xlabel('Metrics')
    ax5.set_ylabel('Variance')
    ax5.set_xticks(x)
    ax5.set_xticklabels(metrics_names)
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. DAPOÊäÄÊúØÊïàÊûúÂ±ïÁ§∫
    ax6 = plt.subplot(2, 3, 6)
    
    if dapo_metrics:
        filter_ratios = [m.filtered_ratio * 100 for m in dapo_metrics]  # ËΩ¨Êç¢‰∏∫ÁôæÂàÜÊØî
        ax6.plot(dapo_steps, filter_ratios, color='orange', linewidth=2, marker='d', markersize=3)
        ax6.set_title('DAPO Dynamic Sampling Effect\n(Filtered data percentage)', fontweight='bold')
        ax6.set_xlabel('Training Steps')
        ax6.set_ylabel('Filtered Data (%)')
        ax6.grid(True, alpha=0.3)
        
        # Ê∑ªÂä†ÁªüËÆ°‰ø°ÊÅØ
        avg_filter = np.mean(filter_ratios)
        ax6.text(0.02, 0.98, f'Avg Filtered: {avg_filter:.1f}%', 
                transform=ax6.transAxes, fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightyellow', alpha=0.8))
    
    plt.tight_layout()
    
    # ‰øùÂ≠òÂõæË°®
    output_path = "grpo_vs_dapo_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"‚úÖ ÂØπÊØîÂàÜÊûêÂõæË°®Â∑≤‰øùÂ≠òÂà∞: {output_path}")
    
    return generate_comparison_report(grpo_metrics, dapo_metrics)

def generate_comparison_report(grpo_metrics: List[TrainingMetrics], 
                             dapo_metrics: List[TrainingMetrics]) -> str:
    """ÁîüÊàêËØ¶ÁªÜÁöÑÂØπÊØîÂàÜÊûêÊä•Âëä"""
    
    # ËÆ°ÁÆóÂÖ≥ÈîÆÊåáÊ†á
    grpo_initial_entropy = grpo_metrics[0].entropy if grpo_metrics else 0
    grpo_final_entropy = grpo_metrics[-1].entropy if grpo_metrics else 0
    grpo_entropy_reduction = (grpo_initial_entropy - grpo_final_entropy) / grpo_initial_entropy * 100 if grpo_initial_entropy > 0 else 0
    
    dapo_initial_entropy = dapo_metrics[0].entropy if dapo_metrics else 0
    dapo_final_entropy = dapo_metrics[-1].entropy if dapo_metrics else 0
    dapo_entropy_reduction = (dapo_initial_entropy - dapo_final_entropy) / dapo_initial_entropy * 100 if dapo_initial_entropy > 0 else 0
    
    # Ê¢ØÂ∫¶Á®≥ÂÆöÊÄß
    grpo_final_grad = grpo_metrics[-1].gradient_norm if grpo_metrics else 0
    dapo_final_grad = dapo_metrics[-1].gradient_norm if dapo_metrics else 0
    
    # ÊúÄÁªàÊÄßËÉΩ
    grpo_final_reward = grpo_metrics[-1].reward_mean if grpo_metrics else 0
    dapo_final_reward = dapo_metrics[-1].reward_mean if dapo_metrics else 0
    
    # DAPOÊäÄÊúØÁªüËÆ°
    dapo_filter_rates = [m.filtered_ratio for m in dapo_metrics]
    avg_filter_rate = np.mean(dapo_filter_rates) * 100
    
    report = f"""
üìä GRPO vs DAPO ÂØπÊØîÂàÜÊûêÊä•Âëä (Â∞èÁôΩÁâà)
============================================================

üîç ÁÜµÂùçÂ°åÂØπÊØîÂàÜÊûê:
------------------------
üìâ GRPO (Âü∫Á∫ø):
‚Ä¢ ÂàùÂßãÁÜµ: {grpo_initial_entropy:.3f} ‚Üí ÊúÄÁªàÁÜµ: {grpo_final_entropy:.3f}
‚Ä¢ ÁÜµÂáèÂ∞ë: {grpo_entropy_reduction:.1f}%
‚Ä¢ ÁªìÊûú: {'‚úÖ Áõ∏ÂØπÁ®≥ÂÆö' if grpo_entropy_reduction < 80 else '‚ùå ‰∏•ÈáçÁÜµÂùçÂ°å'}

üìà DAPO (ÊîπËøõ):
‚Ä¢ ÂàùÂßãÁÜµ: {dapo_initial_entropy:.3f} ‚Üí ÊúÄÁªàÁÜµ: {dapo_final_entropy:.3f}
‚Ä¢ ÁÜµÂáèÂ∞ë: {dapo_entropy_reduction:.1f}%
‚Ä¢ ÁªìÊûú: {'‚úÖ ÊàêÂäü‰øùÊåÅÁ®≥ÂÆö' if dapo_entropy_reduction < 80 else '‚ö†Ô∏è ÈÉ®ÂàÜÊîπÂñÑ'}

üîß Ê¢ØÂ∫¶Á®≥ÂÆöÊÄßÂØπÊØî:
------------------------
‚Ä¢ GRPOÊúÄÁªàÊ¢ØÂ∫¶: {grpo_final_grad:.4f}
‚Ä¢ DAPOÊúÄÁªàÊ¢ØÂ∫¶: {dapo_final_grad:.4f}
‚Ä¢ ÊîπÂñÑÊïàÊûú: {'‚úÖ DAPOÊõ¥Á®≥ÂÆö' if dapo_final_grad > grpo_final_grad else '‚ö†Ô∏è ÈúÄË¶ÅËøõ‰∏ÄÊ≠•Ë∞É‰ºò'}

üéØ ÊÄßËÉΩÂØπÊØî:
------------------------
‚Ä¢ GRPOÊúÄÁªàÂ•ñÂä±: {grpo_final_reward:.3f}
‚Ä¢ DAPOÊúÄÁªàÂ•ñÂä±: {dapo_final_reward:.3f}
‚Ä¢ ÊÄßËÉΩÊèêÂçá: {((dapo_final_reward - grpo_final_reward) / grpo_final_reward * 100):.1f}%

üöÄ DAPOÊäÄÊúØÊïàÊûú:
------------------------
‚Ä¢ Clip-HigherÊäÄÊúØ: ‚úÖ ÂêØÁî®ÔºåÊîæÂÆΩ‰∫ÜÂØπÈ´òÂ•ñÂä±Ë°å‰∏∫ÁöÑÈôêÂà∂
‚Ä¢ Âä®ÊÄÅÈááÊ†∑ÊäÄÊúØ: ‚úÖ ÂêØÁî®ÔºåÂπ≥ÂùáËøáÊª§‰∫Ü{avg_filter_rate:.1f}%ÁöÑ‰ΩéË¥®ÈáèÊï∞ÊçÆ
‚Ä¢ Êï¥‰ΩìÊïàÊûú: {'üéâ ÊòæËëóÊîπÂñÑËÆ≠ÁªÉÁ®≥ÂÆöÊÄß' if dapo_entropy_reduction < grpo_entropy_reduction - 10 else 'üìà Êèê‰æõ‰∫Ü‰∏ÄÂÆöÊîπÂñÑ'}

üí° ÂÖ≥ÈîÆÂèëÁé∞:
------------------------
1. {'‚úÖ DAPOÊàêÂäüÁºìËß£‰∫ÜÁÜµÂùçÂ°åÈóÆÈ¢ò' if dapo_entropy_reduction < grpo_entropy_reduction else '‚ö†Ô∏è ÁÜµÂùçÂ°å‰ªçÈúÄËøõ‰∏ÄÊ≠•‰ºòÂåñ'}
2. {'‚úÖ Ê¢ØÂ∫¶Á®≥ÂÆöÊÄßÂæóÂà∞ÊîπÂñÑ' if dapo_final_grad > grpo_final_grad * 1.2 else 'üìä Ê¢ØÂ∫¶Á®≥ÂÆöÊÄßÁï•ÊúâÊîπÂñÑ'}
3. {'‚úÖ Êï¥‰ΩìËÆ≠ÁªÉË¥®ÈáèÊèêÂçá' if dapo_final_reward > grpo_final_reward else '‚öñÔ∏è ÊÄßËÉΩÂü∫Êú¨ÊåÅÂπ≥'}

üéì ÊäÄÊúØÊ¥ûÂØü:
------------------------
‚Ä¢ Clip-HigherËÆ©Ê®°ÂûãÊï¢‰∫éÂ∞ùËØïÈ´òÂ•ñÂä±ÁöÑÊñ∞Ë°å‰∏∫
‚Ä¢ Âä®ÊÄÅÈááÊ†∑Á°Æ‰øù‰∫ÜÊØèÊ¨°Â≠¶‰π†ÈÉΩÊúâ"Ëê•ÂÖª‰ª∑ÂÄº"
‚Ä¢ ‰∏§È°πÊäÄÊúØÂçèÂêåÂ∑•‰ΩúÔºåÊûÑÂª∫‰∫ÜÊõ¥Á®≥ÂÆöÁöÑËÆ≠ÁªÉÁéØÂ¢É

üöÄ ÂêéÁª≠ÊîπËøõÊñπÂêë:
------------------------
‚Ä¢ ÂèØ‰ª•Â∞ùËØïË∞ÉÊï¥Âä®ÊÄÅÈááÊ†∑ÁöÑÈòàÂÄºÂèÇÊï∞
‚Ä¢ Êé¢Á¥¢Êõ¥Êô∫ËÉΩÁöÑClip-HigherÁ≠ñÁï•
‚Ä¢ ÁªìÂêàVeRLËøõË°åÂèØÈ™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†
"""
    
    return report

def main():
    """‰∏ªÂáΩÊï∞ÔºöËøêË°åÂÆåÊï¥ÁöÑDAPOÂØπÊØîÂÆûÈ™å"""
    print("üöÄ Ê¨¢ËøéÊù•Âà∞DAPOÁÆóÊ≥ïÂÆûÈ™åÂÆ§ÔºÅ")
    print("üìö Êú¨ÂÆûÈ™åÂ∞ÜÂØπÊØîGRPOÂíåDAPOÂú®Ëß£ÂÜ≥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÊñπÈù¢ÁöÑÊïàÊûú")
    
    # ÂÆûÈ™åÂâçÊ¶ÇÂøµ‰ªãÁªç
    print("\nüìñ ÂÆûÈ™åÂâçÊ¶ÇÂøµÈ¢Ñ‰π†:")
    print("=" * 50)
    print(ExplainerSystem.explain_concept("dapo"))
    print(ExplainerSystem.explain_concept("clip_higher"))
    print(ExplainerSystem.explain_concept("dynamic_sampling"))
    
    print("\nüî• ÂºÄÂßãÂØπÊØîÂÆûÈ™å...")
    
    try:
        # ÂÆûÈ™å1ÔºöGRPOÂü∫Á∫ø
        grpo_metrics = run_grpo_baseline_experiment()
        
        # ÂÆûÈ™å2ÔºöDAPOÊîπËøõ
        dapo_metrics = run_dapo_experiment()
        
        # ÁîüÊàêÂØπÊØîÂàÜÊûê
        print("\nüé® ÁîüÊàêËØ¶ÁªÜÂØπÊØîÂàÜÊûê...")
        comparison_report = create_comparison_visualization(grpo_metrics, dapo_metrics)
        
        print("\nüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ")
        print("ÂÆûÈ™åÂÖ®ÈÉ®ÂÆåÊàêÔºÅ‰ª•‰∏ãÊòØÊúÄÁªàÂØπÊØîÂàÜÊûêÔºö")
        print("üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ")
        
        print(comparison_report)
        
        print("\n‚ú® ÊÅ≠Âñú‰Ω†ÂÆåÊàê‰∫ÜLab05ÁöÑÂ≠¶‰π†ÔºÅ")
        print("üéØ ‰Ω†Áé∞Âú®ÊéåÊè°‰∫ÜDAPOÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÊäÄÊúØ")
        print("üîß ‰∏ã‰∏ÄÊ≠•ÂèØ‰ª•Â≠¶‰π†VeRLÔºåÊé¢Á¥¢ÂèØÈ™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†")
        print("üöÄ ÁªßÁª≠Âä†Ê≤πÔºåÂêëÁùÄAI‰∏ìÂÆ∂ÁöÑÁõÆÊ†áÂâçËøõÔºÅ")
        
    except Exception as e:
        print(f"‚ùå ÂÆûÈ™åËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {e}")
        print("üí° ÊèêÁ§∫ÔºöËØ∑Ê£ÄÊü•‰æùËµñÂåÖÊòØÂê¶Ê≠£Á°ÆÂÆâË£Ö")

if __name__ == "__main__":
    main() 
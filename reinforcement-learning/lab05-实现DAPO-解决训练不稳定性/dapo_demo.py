#!/usr/bin/env python3
"""
Lab05: DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization) å®ç°

æœ¬å®éªŒå®ç°DAPOç®—æ³•çš„ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼š
1. Clip-Higher: æ”¾å®½å¯¹ä½æ¦‚ç‡è¡Œä¸ºçš„æƒ©ç½šï¼Œé¼“åŠ±æ¢ç´¢
2. åŠ¨æ€é‡‡æ ·: è¿‡æ»¤æ‰å¥–åŠ±ä¿¡å·è¿‡äºå•ä¸€çš„æ ·æœ¬ç»„

ç›®æ ‡ï¼šè§£å†³Lab04ä¸­å‘ç°çš„ç†µåå¡Œå’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜

ä½œè€…: SystemLearning Project
æ—¥æœŸ: 2024-12-19
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from typing import List, Tuple, Dict, Optional
import logging
from dataclasses import dataclass
import time
from collections import defaultdict
import warnings

# é…ç½®matplotlibå­—ä½“
def setup_matplotlib():
    """é…ç½®matplotlibæ˜¾ç¤º"""
    try:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        print("âœ… Matplotlib configuration successful")
    except Exception as e:
        print(f"âš ï¸ Matplotlib configuration failed: {e}")

setup_matplotlib()

@dataclass
class TrainingMetrics:
    """è®­ç»ƒè¿‡ç¨‹çš„å…³é”®æŒ‡æ ‡"""
    step: int
    entropy: float
    gradient_norm: float
    loss: float
    action_distribution: List[float]
    reward_mean: float
    reward_std: float
    clip_ratio: float = 0.0
    filtered_ratio: float = 0.0  # åŠ¨æ€é‡‡æ ·è¿‡æ»¤æ¯”ä¾‹

class ExplainerSystem:
    """å°ç™½å‹å¥½çš„è§£é‡Šç³»ç»Ÿ"""
    
    @staticmethod
    def explain_concept(concept: str) -> str:
        """è§£é‡Šå…³é”®æ¦‚å¿µ"""
        explanations = {
            "dapo": """
ğŸš€ DAPO Algorithm:
â€¢ DAPO = Advanced version of GRPO for training stability
â€¢ Two core technologies:
  1. Clip-Higher: Encourage exploration of high-reward behaviors
  2. Dynamic Sampling: Filter out low-quality training data
â€¢ Like giving AI a "smart regulator" for stable learning
""",
            "clip_higher": """
ğŸ”§ Clip-Higher Technology:
â€¢ Traditional PPO/GRPO: Strictly limit policy changes
â€¢ Clip-Higher: Relax limits for promising new behaviors
â€¢ Key idea: Encourage rather than punish high-reward actions
â€¢ Effect: Prevent entropy collapse, maintain exploration
""",
            "dynamic_sampling": """
ğŸ“Š Dynamic Sampling Technology:
â€¢ Problem: Monotonous training data leads to poor learning
â€¢ Solution: Smart filter ensures data diversity in each batch
â€¢ Like selecting study materials: need both good and bad examples
â€¢ Effect: Prevent gradient vanishing, ensure effective learning
"""
        }
        return explanations.get(concept, f"Explanation for '{concept}' not implemented")

class UnstableEnvironment:
    """Unstable training environment from Lab04"""
    
    def __init__(self):
        self.action_rewards = [0.3, 1.0, 0.5]  # Action 0, 1, 2 rewards
        
    def get_state(self) -> torch.Tensor:
        """Get state (simplified as fixed state)"""
        return torch.zeros(4)
    
    def step(self, action: int) -> Tuple[torch.Tensor, float]:
        """Execute action and return reward"""
        reward = self.action_rewards[action]
        # Add noise to simulate real environment
        noise = np.random.normal(0, 0.1)
        reward = max(0, reward + noise)
        return self.get_state(), reward

class PolicyNetwork(nn.Module):
    """Policy network"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)
    
    def get_action_and_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor, torch.Tensor]:
        """Get action and log probability"""
        logits = self.forward(state)
        probs = F.softmax(logits, dim=-1)
        probs = torch.clamp(probs, min=1e-8, max=1.0)
        
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob, probs

class DAPOTrainer:
    """DAPO Trainer - implements Clip-Higher and Dynamic Sampling"""
    
    def __init__(self, 
                 learning_rate: float = 1e-3,
                 entropy_coef: float = 0.01,
                 batch_size: int = 32,
                 clip_ratio: float = 0.2,
                 use_clip_higher: bool = True,
                 use_dynamic_sampling: bool = True,
                 reward_variance_threshold: float = 0.01,
                 verbose: bool = True):
        
        self.env = UnstableEnvironment()
        self.policy = PolicyNetwork(4, 3)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        
        # Training parameters
        self.learning_rate = learning_rate
        self.entropy_coef = entropy_coef
        self.batch_size = batch_size
        self.clip_ratio = clip_ratio
        self.verbose = verbose
        
        # DAPO specific parameters
        self.use_clip_higher = use_clip_higher
        self.use_dynamic_sampling = use_dynamic_sampling
        self.reward_variance_threshold = reward_variance_threshold
        
        # Records
        self.metrics_history = []
        self.explainer = ExplainerSystem()
        
        print(f"ğŸš€ DAPO Trainer initialized")
        print(f"ğŸ“‹ Config: LR={learning_rate}, Entropy={entropy_coef}, Batch={batch_size}")
        print(f"ğŸ”§ DAPO Features: Clip-Higher={use_clip_higher}, Dynamic-Sampling={use_dynamic_sampling}")

    def dynamic_sampling_filter(self, batch_data) -> Tuple[List, float]:
        """Dynamic sampling filter - DAPO core component 1"""
        if not self.use_dynamic_sampling:
            return batch_data, 0.0
        
        # Calculate reward statistics
        rewards = [item[2] for item in batch_data]
        reward_std = np.std(rewards)
        
        # Filter if reward variance is too small
        if reward_std < self.reward_variance_threshold:
            if self.verbose and len(self.metrics_history) % 20 == 0:
                print(f"âš ï¸ Dynamic sampling triggered: reward std {reward_std:.4f} < threshold {self.reward_variance_threshold}")
            
            # Keep samples with higher variance (simplified strategy)
            filtered_data = batch_data[::2]  # Simple filtering strategy
            filter_ratio = 1.0 - len(filtered_data) / len(batch_data)
            return filtered_data, filter_ratio
        
        return batch_data, 0.0

    def clip_higher_loss(self, log_probs, old_log_probs, advantages, rewards) -> torch.Tensor:
        """Clip-Higher loss calculation - DAPO core component 2"""
        ratio = torch.exp(log_probs - old_log_probs)
        
        if not self.use_clip_higher:
            # Standard PPO clipping
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages
            return -torch.min(surr1, surr2).mean()
        
        # Clip-Higher logic: relax upper bound for high-reward samples
        reward_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        high_reward_mask = reward_normalized > 0.5  # High reward samples
        
        surr1 = ratio * advantages
        
        # Use more relaxed upper bound for high-reward samples
        upper_bound = torch.where(high_reward_mask, 
                                 1 + self.clip_ratio * 2,  # Relax by 2x
                                 1 + self.clip_ratio)      # Standard limit
        
        # Ensure all clamp parameters are tensors for PyTorch compatibility
        lower_bound = torch.full_like(ratio, 1 - self.clip_ratio)
        
        surr2 = torch.clamp(ratio, min=lower_bound, max=upper_bound) * advantages
        
        return -torch.min(surr1, surr2).mean()

    def collect_batch(self):
        """Collect training batch data"""
        batch_data = []
        
        for _ in range(self.batch_size):
            state = self.env.get_state()
            action, log_prob, probs = self.policy.get_action_and_log_prob(state)
            next_state, reward = self.env.step(action)
            
            batch_data.append((state, action, reward, log_prob))
        
        # Apply dynamic sampling filter
        filtered_batch, filter_ratio = self.dynamic_sampling_filter(batch_data)
        
        return filtered_batch, filter_ratio

    def compute_advantages(self, rewards: List[float]) -> torch.Tensor:
        """Compute advantage function (simplified version)"""
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        # Simple normalization
        if rewards_tensor.std() > 1e-8:
            advantages = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)
        else:
            advantages = torch.zeros_like(rewards_tensor)
        return advantages

    def train_step(self) -> TrainingMetrics:
        """Execute one DAPO training step"""
        # Collect data
        batch_data, filter_ratio = self.collect_batch()
        
        if len(batch_data) == 0:
            print("âš ï¸ Dynamic sampling filtered all data, skipping update")
            return None
        
        # Extract data
        states = torch.stack([item[0] for item in batch_data])
        actions = torch.tensor([item[1] for item in batch_data])
        rewards = [item[2] for item in batch_data]
        old_log_probs = torch.stack([item[3] for item in batch_data])
        
        # Compute advantages
        advantages = self.compute_advantages(rewards)
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        
        # Forward pass
        logits = self.policy(states)
        probs = F.softmax(logits, dim=-1)
        probs = torch.clamp(probs, min=1e-8, max=1.0)
        
        log_probs = torch.log(probs.gather(1, actions.unsqueeze(1)).squeeze())
        
        # Calculate loss using Clip-Higher
        policy_loss = self.clip_higher_loss(log_probs, old_log_probs, advantages, rewards_tensor)
        
        # Entropy loss
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
        entropy_loss = -self.entropy_coef * entropy
        
        # Total loss
        total_loss = policy_loss + entropy_loss
        
        # Backward pass
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # Calculate gradient norm
        grad_norm = 0.0
        for param in self.policy.parameters():
            if param.grad is not None:
                grad_norm += param.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5
        
        self.optimizer.step()
        
        # Record metrics
        with torch.no_grad():
            action_dist = probs.mean(dim=0).tolist()
            
        metrics = TrainingMetrics(
            step=len(self.metrics_history),
            entropy=entropy.item(),
            gradient_norm=grad_norm,
            loss=total_loss.item(),
            action_distribution=action_dist,
            reward_mean=np.mean(rewards),
            reward_std=np.std(rewards),
            clip_ratio=self.clip_ratio,
            filtered_ratio=filter_ratio
        )
        
        self.metrics_history.append(metrics)
        return metrics

    def generate_report(self, experiment_type: str) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Š"""
        if not self.metrics_history:
            return "âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®"
        
        initial_entropy = self.metrics_history[0].entropy
        final_entropy = self.metrics_history[-1].entropy
        entropy_change = (initial_entropy - final_entropy) / initial_entropy * 100
        
        initial_grad = self.metrics_history[0].gradient_norm
        final_grad = self.metrics_history[-1].gradient_norm
        
        final_dist = self.metrics_history[-1].action_distribution
        dominant_action = final_dist.index(max(final_dist))
        
        # è®¡ç®—è¿‡æ»¤ç»Ÿè®¡
        filter_ratios = [m.filtered_ratio for m in self.metrics_history]
        avg_filter_ratio = np.mean(filter_ratios)
        
        report = f"""
ğŸ” {experiment_type} å®éªŒè¯¦ç»†åˆ†ææŠ¥å‘Š
==================================================

ğŸ“ˆ è®­ç»ƒæ¦‚å†µ:
â€¢ æ€»è®­ç»ƒæ­¥æ•°: {len(self.metrics_history)}
â€¢ åˆå§‹ç†µå€¼: {initial_entropy:.4f} â†’ æœ€ç»ˆç†µå€¼: {final_entropy:.4f}
â€¢ ç†µå€¼å˜åŒ–: {entropy_change:.1f}%
â€¢ åˆå§‹æ¢¯åº¦: {initial_grad:.4f} â†’ æœ€ç»ˆæ¢¯åº¦: {final_grad:.4f}

ğŸ¯ æœ€ç»ˆåŠ¨ä½œåå¥½:
â€¢ åŠ¨ä½œ0 (å¥–åŠ±0.3): {final_dist[0]:.3f} ({final_dist[0]*100:.1f}%)
â€¢ åŠ¨ä½œ1 (å¥–åŠ±1.0): {final_dist[1]:.3f} ({final_dist[1]*100:.1f}%)  â­æœ€ä¼˜
â€¢ åŠ¨ä½œ2 (å¥–åŠ±0.5): {final_dist[2]:.3f} ({final_dist[2]*100:.1f}%)
â€¢ ä¸»å¯¼åŠ¨ä½œ: åŠ¨ä½œ{dominant_action}

ğŸ”§ DAPOæŠ€æœ¯ç»Ÿè®¡:
â€¢ Clip-Higherå¯ç”¨: {'âœ…' if self.use_clip_higher else 'âŒ'}
â€¢ åŠ¨æ€é‡‡æ ·å¯ç”¨: {'âœ…' if self.use_dynamic_sampling else 'âŒ'}
â€¢ å¹³å‡è¿‡æ»¤æ¯”ä¾‹: {avg_filter_ratio*100:.1f}%

ğŸ’¡ ç»“æœè§£é‡Š:
{'âœ… DAPOæˆåŠŸä¿æŒäº†è®­ç»ƒç¨³å®šæ€§' if entropy_change < 80 else 'âš ï¸ ä»å­˜åœ¨ä¸€å®šç¨‹åº¦çš„ç†µåå¡Œ'}
{'âœ… æ¢¯åº¦ä¿æŒç¨³å®š' if final_grad > 0.01 else 'âš ï¸ å‡ºç°äº†æ¢¯åº¦æ¶ˆå¤±ç°è±¡'}

ğŸ”¬ æŠ€æœ¯æ•ˆæœåˆ†æ:
â€¢ Clip-Higher: {'å¸®åŠ©ä¿æŒæ¢ç´¢æ€§ï¼Œå‡ç¼“ç†µåå¡Œ' if self.use_clip_higher else 'æœªå¯ç”¨'}
â€¢ åŠ¨æ€é‡‡æ ·: {'è¿‡æ»¤äº†{:.1f}%çš„ä½è´¨é‡æ•°æ®'.format(avg_filter_ratio*100) if self.use_dynamic_sampling else 'æœªå¯ç”¨'}
"""
        return report

def run_grpo_baseline_experiment():
    """è¿è¡ŒGRPOåŸºçº¿å®éªŒï¼ˆä¸ä½¿ç”¨DAPOæŠ€æœ¯ï¼‰"""
    print("ğŸª ç¬¬ä¸€åœºå¯¹æ¯”ï¼šGRPOåŸºçº¿å®éªŒ")
    print("=" * 50)
    print("ğŸ¯ ç›®æ ‡: å¤ç°Lab04ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜")
    print("âš™ï¸ æ–¹æ³•: ä½¿ç”¨æ ‡å‡†GRPOï¼Œä¸å¯ç”¨DAPOæŠ€æœ¯")
    
    trainer = DAPOTrainer(
        learning_rate=5e-3,  # ä½¿ç”¨Lab04çš„é«˜å­¦ä¹ ç‡
        entropy_coef=0.001,  # ä½¿ç”¨Lab04çš„ä½ç†µç³»æ•°
        batch_size=32,
        use_clip_higher=False,      # å…³é—­Clip-Higher
        use_dynamic_sampling=False,  # å…³é—­åŠ¨æ€é‡‡æ ·
        verbose=False
    )
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for step in range(100):
        metrics = trainer.train_step()
        if metrics is None:
            continue
            
        # æ£€æµ‹ç†µåå¡Œ
        if metrics.entropy < 0.01:
            if step < 90:  # åªåœ¨å‰90æ­¥æŠ¥å‘Šï¼Œé¿å…è¿‡å¤šè¾“å‡º
                print(f"âš ï¸ ç¬¬{step+1}æ­¥ï¼šç†µå€¼è¿‡ä½({metrics.entropy:.4f})ï¼Œå¯èƒ½å·²ç»åå¡Œï¼")
        
        # æ¯20æ­¥æ˜¾ç¤ºè¿›åº¦
        if (step + 1) % 20 == 0:
            print(f"ğŸ“Š ç¬¬{step+1}æ­¥: ç†µ={metrics.entropy:.3f}, æ¢¯åº¦={metrics.gradient_norm:.3f}, å¥–åŠ±={metrics.reward_mean:.3f}")
    
    print("âœ… GRPOåŸºçº¿å®éªŒå®Œæˆ")
    print(trainer.generate_report("GRPOåŸºçº¿"))
    return trainer.metrics_history

def run_dapo_experiment():
    """è¿è¡Œå®Œæ•´DAPOå®éªŒ"""
    print("\nğŸª ç¬¬äºŒåœºå¯¹æ¯”ï¼šå®Œæ•´DAPOå®éªŒ")
    print("=" * 50)
    print("ğŸ¯ ç›®æ ‡: éªŒè¯DAPOæŠ€æœ¯è§£å†³è®­ç»ƒä¸ç¨³å®šæ€§çš„æ•ˆæœ")
    print("âš™ï¸ æ–¹æ³•: å¯ç”¨Clip-Higherå’ŒåŠ¨æ€é‡‡æ ·æŠ€æœ¯")
    
    trainer = DAPOTrainer(
        learning_rate=5e-3,  # åŒæ ·çš„é«˜å­¦ä¹ ç‡
        entropy_coef=0.001,  # åŒæ ·çš„ä½ç†µç³»æ•°
        batch_size=32,
        use_clip_higher=True,       # å¯ç”¨Clip-Higher
        use_dynamic_sampling=True,  # å¯ç”¨åŠ¨æ€é‡‡æ ·
        reward_variance_threshold=0.05,  # åŠ¨æ€é‡‡æ ·é˜ˆå€¼
        verbose=False
    )
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for step in range(100):
        metrics = trainer.train_step()
        if metrics is None:
            continue
            
        # æ£€æµ‹ç†µåå¡Œï¼ˆæœŸæœ›å‡å°‘ï¼‰
        if metrics.entropy < 0.01:
            print(f"âš ï¸ ç¬¬{step+1}æ­¥ï¼šç†µå€¼ä»ç„¶è¿‡ä½({metrics.entropy:.4f})")
        
        # æ˜¾ç¤ºDAPOæŠ€æœ¯å·¥ä½œæƒ…å†µ
        if metrics.filtered_ratio > 0:
            print(f"ğŸ”§ ç¬¬{step+1}æ­¥ï¼šåŠ¨æ€é‡‡æ ·è¿‡æ»¤äº†{metrics.filtered_ratio*100:.1f}%çš„æ•°æ®")
        
        # æ¯20æ­¥æ˜¾ç¤ºè¿›åº¦
        if (step + 1) % 20 == 0:
            print(f"ğŸ“Š ç¬¬{step+1}æ­¥: ç†µ={metrics.entropy:.3f}, æ¢¯åº¦={metrics.gradient_norm:.3f}, å¥–åŠ±={metrics.reward_mean:.3f}")
    
    print("âœ… DAPOå®éªŒå®Œæˆ")
    print(trainer.generate_report("DAPO"))
    return trainer.metrics_history

def create_comparison_visualization(grpo_metrics: List[TrainingMetrics], 
                                  dapo_metrics: List[TrainingMetrics]):
    """åˆ›å»ºGRPO vs DAPOå¯¹æ¯”å¯è§†åŒ–"""
    print("\nğŸ¨ ç”ŸæˆGRPO vs DAPOå¯¹æ¯”åˆ†æå›¾è¡¨...")
    
    fig = plt.figure(figsize=(16, 10))
    fig.suptitle('GRPO vs DAPO: Training Stability Comparison', fontsize=16, fontweight='bold')
    
    # å®šä¹‰é¢œè‰²
    grpo_color = '#e74c3c'  # çº¢è‰² - GRPO
    dapo_color = '#27ae60'  # ç»¿è‰² - DAPO
    
    # æå–æ•°æ®
    grpo_steps = list(range(len(grpo_metrics)))
    dapo_steps = list(range(len(dapo_metrics)))
    
    grpo_entropy = [m.entropy for m in grpo_metrics]
    dapo_entropy = [m.entropy for m in dapo_metrics]
    
    grpo_gradients = [m.gradient_norm for m in grpo_metrics]
    dapo_gradients = [m.gradient_norm for m in dapo_metrics]
    
    grpo_rewards = [m.reward_mean for m in grpo_metrics]
    dapo_rewards = [m.reward_mean for m in dapo_metrics]
    
    # 1. ç†µå€¼å¯¹æ¯”
    ax1 = plt.subplot(2, 3, 1)
    ax1.plot(grpo_steps, grpo_entropy, color=grpo_color, linewidth=2, label='GRPO (Baseline)', marker='o', markersize=3)
    ax1.plot(dapo_steps, dapo_entropy, color=dapo_color, linewidth=2, label='DAPO (Improved)', marker='s', markersize=3)
    ax1.axhline(y=0.01, color='red', linestyle='--', alpha=0.7, label='Collapse Threshold')
    ax1.set_title('Entropy Comparison\n(Higher is better for exploration)', fontweight='bold')
    ax1.set_xlabel('Training Steps')
    ax1.set_ylabel('Entropy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æ¢¯åº¦èŒƒæ•°å¯¹æ¯”
    ax2 = plt.subplot(2, 3, 2)
    ax2.plot(grpo_steps, grpo_gradients, color=grpo_color, linewidth=2, label='GRPO', marker='o', markersize=3)
    ax2.plot(dapo_steps, dapo_gradients, color=dapo_color, linewidth=2, label='DAPO', marker='s', markersize=3)
    ax2.set_title('Gradient Norm Comparison\n(Stability indicator)', fontweight='bold')
    ax2.set_xlabel('Training Steps')
    ax2.set_ylabel('Gradient Norm')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å¥–åŠ±å¯¹æ¯”
    ax3 = plt.subplot(2, 3, 3)
    ax3.plot(grpo_steps, grpo_rewards, color=grpo_color, linewidth=2, label='GRPO', marker='o', markersize=3)
    ax3.plot(dapo_steps, dapo_rewards, color=dapo_color, linewidth=2, label='DAPO', marker='s', markersize=3)
    ax3.set_title('Reward Comparison\n(Performance indicator)', fontweight='bold')
    ax3.set_xlabel('Training Steps')
    ax3.set_ylabel('Average Reward')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æœ€ç»ˆåŠ¨ä½œåˆ†å¸ƒå¯¹æ¯”
    ax4 = plt.subplot(2, 3, 4)
    if grpo_metrics and dapo_metrics:
        grpo_final_dist = grpo_metrics[-1].action_distribution
        dapo_final_dist = dapo_metrics[-1].action_distribution
        
        actions = ['Action 0\n(R=0.3)', 'Action 1\n(R=1.0)\nOptimal', 'Action 2\n(R=0.5)']
        x = np.arange(len(actions))
        width = 0.35
        
        ax4.bar(x - width/2, grpo_final_dist, width, label='GRPO', color=grpo_color, alpha=0.7)
        ax4.bar(x + width/2, dapo_final_dist, width, label='DAPO', color=dapo_color, alpha=0.7)
        
        ax4.set_title('Final Action Distribution\n(Strategy comparison)', fontweight='bold')
        ax4.set_xlabel('Actions')
        ax4.set_ylabel('Probability')
        ax4.set_xticks(x)
        ax4.set_xticklabels(actions)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    
    # 5. è®­ç»ƒç¨³å®šæ€§ç»Ÿè®¡
    ax5 = plt.subplot(2, 3, 5)
    
    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
    grpo_entropy_var = np.var(grpo_entropy[50:])  # ååŠæ®µæ–¹å·®
    dapo_entropy_var = np.var(dapo_entropy[50:])
    
    grpo_grad_var = np.var(grpo_gradients[50:])
    dapo_grad_var = np.var(dapo_gradients[50:])
    
    metrics_names = ['Entropy\nVariance', 'Gradient\nVariance']
    grpo_values = [grpo_entropy_var, grpo_grad_var]
    dapo_values = [dapo_entropy_var, dapo_grad_var]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    ax5.bar(x - width/2, grpo_values, width, label='GRPO', color=grpo_color, alpha=0.7)
    ax5.bar(x + width/2, dapo_values, width, label='DAPO', color=dapo_color, alpha=0.7)
    
    ax5.set_title('Training Stability\n(Lower variance = more stable)', fontweight='bold')
    ax5.set_xlabel('Metrics')
    ax5.set_ylabel('Variance')
    ax5.set_xticks(x)
    ax5.set_xticklabels(metrics_names)
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. DAPOæŠ€æœ¯æ•ˆæœå±•ç¤º
    ax6 = plt.subplot(2, 3, 6)
    
    if dapo_metrics:
        filter_ratios = [m.filtered_ratio * 100 for m in dapo_metrics]  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        ax6.plot(dapo_steps, filter_ratios, color='orange', linewidth=2, marker='d', markersize=3)
        ax6.set_title('DAPO Dynamic Sampling Effect\n(Filtered data percentage)', fontweight='bold')
        ax6.set_xlabel('Training Steps')
        ax6.set_ylabel('Filtered Data (%)')
        ax6.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        avg_filter = np.mean(filter_ratios)
        ax6.text(0.02, 0.98, f'Avg Filtered: {avg_filter:.1f}%', 
                transform=ax6.transAxes, fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightyellow', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_path = "grpo_vs_dapo_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… å¯¹æ¯”åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    return generate_comparison_report(grpo_metrics, dapo_metrics)

def generate_comparison_report(grpo_metrics: List[TrainingMetrics], 
                             dapo_metrics: List[TrainingMetrics]) -> str:
    """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š"""
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    grpo_initial_entropy = grpo_metrics[0].entropy if grpo_metrics else 0
    grpo_final_entropy = grpo_metrics[-1].entropy if grpo_metrics else 0
    grpo_entropy_reduction = (grpo_initial_entropy - grpo_final_entropy) / grpo_initial_entropy * 100 if grpo_initial_entropy > 0 else 0
    
    dapo_initial_entropy = dapo_metrics[0].entropy if dapo_metrics else 0
    dapo_final_entropy = dapo_metrics[-1].entropy if dapo_metrics else 0
    dapo_entropy_reduction = (dapo_initial_entropy - dapo_final_entropy) / dapo_initial_entropy * 100 if dapo_initial_entropy > 0 else 0
    
    # æ¢¯åº¦ç¨³å®šæ€§
    grpo_final_grad = grpo_metrics[-1].gradient_norm if grpo_metrics else 0
    dapo_final_grad = dapo_metrics[-1].gradient_norm if dapo_metrics else 0
    
    # æœ€ç»ˆæ€§èƒ½
    grpo_final_reward = grpo_metrics[-1].reward_mean if grpo_metrics else 0
    dapo_final_reward = dapo_metrics[-1].reward_mean if dapo_metrics else 0
    
    # DAPOæŠ€æœ¯ç»Ÿè®¡
    dapo_filter_rates = [m.filtered_ratio for m in dapo_metrics]
    avg_filter_rate = np.mean(dapo_filter_rates) * 100
    
    report = f"""
ğŸ“Š GRPO vs DAPO å¯¹æ¯”åˆ†ææŠ¥å‘Š (å°ç™½ç‰ˆ)
============================================================

ğŸ” ç†µåå¡Œå¯¹æ¯”åˆ†æ:
------------------------
ğŸ“‰ GRPO (åŸºçº¿):
â€¢ åˆå§‹ç†µ: {grpo_initial_entropy:.3f} â†’ æœ€ç»ˆç†µ: {grpo_final_entropy:.3f}
â€¢ ç†µå‡å°‘: {grpo_entropy_reduction:.1f}%
â€¢ ç»“æœ: {'âœ… ç›¸å¯¹ç¨³å®š' if grpo_entropy_reduction < 80 else 'âŒ ä¸¥é‡ç†µåå¡Œ'}

ğŸ“ˆ DAPO (æ”¹è¿›):
â€¢ åˆå§‹ç†µ: {dapo_initial_entropy:.3f} â†’ æœ€ç»ˆç†µ: {dapo_final_entropy:.3f}
â€¢ ç†µå‡å°‘: {dapo_entropy_reduction:.1f}%
â€¢ ç»“æœ: {'âœ… æˆåŠŸä¿æŒç¨³å®š' if dapo_entropy_reduction < 80 else 'âš ï¸ éƒ¨åˆ†æ”¹å–„'}

ğŸ”§ æ¢¯åº¦ç¨³å®šæ€§å¯¹æ¯”:
------------------------
â€¢ GRPOæœ€ç»ˆæ¢¯åº¦: {grpo_final_grad:.4f}
â€¢ DAPOæœ€ç»ˆæ¢¯åº¦: {dapo_final_grad:.4f}
â€¢ æ”¹å–„æ•ˆæœ: {'âœ… DAPOæ›´ç¨³å®š' if dapo_final_grad > grpo_final_grad else 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜'}

ğŸ¯ æ€§èƒ½å¯¹æ¯”:
------------------------
â€¢ GRPOæœ€ç»ˆå¥–åŠ±: {grpo_final_reward:.3f}
â€¢ DAPOæœ€ç»ˆå¥–åŠ±: {dapo_final_reward:.3f}
â€¢ æ€§èƒ½æå‡: {((dapo_final_reward - grpo_final_reward) / grpo_final_reward * 100):.1f}%

ğŸš€ DAPOæŠ€æœ¯æ•ˆæœ:
------------------------
â€¢ Clip-HigheræŠ€æœ¯: âœ… å¯ç”¨ï¼Œæ”¾å®½äº†å¯¹é«˜å¥–åŠ±è¡Œä¸ºçš„é™åˆ¶
â€¢ åŠ¨æ€é‡‡æ ·æŠ€æœ¯: âœ… å¯ç”¨ï¼Œå¹³å‡è¿‡æ»¤äº†{avg_filter_rate:.1f}%çš„ä½è´¨é‡æ•°æ®
â€¢ æ•´ä½“æ•ˆæœ: {'ğŸ‰ æ˜¾è‘—æ”¹å–„è®­ç»ƒç¨³å®šæ€§' if dapo_entropy_reduction < grpo_entropy_reduction - 10 else 'ğŸ“ˆ æä¾›äº†ä¸€å®šæ”¹å–„'}

ğŸ’¡ å…³é”®å‘ç°:
------------------------
1. {'âœ… DAPOæˆåŠŸç¼“è§£äº†ç†µåå¡Œé—®é¢˜' if dapo_entropy_reduction < grpo_entropy_reduction else 'âš ï¸ ç†µåå¡Œä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–'}
2. {'âœ… æ¢¯åº¦ç¨³å®šæ€§å¾—åˆ°æ”¹å–„' if dapo_final_grad > grpo_final_grad * 1.2 else 'ğŸ“Š æ¢¯åº¦ç¨³å®šæ€§ç•¥æœ‰æ”¹å–„'}
3. {'âœ… æ•´ä½“è®­ç»ƒè´¨é‡æå‡' if dapo_final_reward > grpo_final_reward else 'âš–ï¸ æ€§èƒ½åŸºæœ¬æŒå¹³'}

ğŸ“ æŠ€æœ¯æ´å¯Ÿ:
------------------------
â€¢ Clip-Higherè®©æ¨¡å‹æ•¢äºå°è¯•é«˜å¥–åŠ±çš„æ–°è¡Œä¸º
â€¢ åŠ¨æ€é‡‡æ ·ç¡®ä¿äº†æ¯æ¬¡å­¦ä¹ éƒ½æœ‰"è¥å…»ä»·å€¼"
â€¢ ä¸¤é¡¹æŠ€æœ¯ååŒå·¥ä½œï¼Œæ„å»ºäº†æ›´ç¨³å®šçš„è®­ç»ƒç¯å¢ƒ

ğŸš€ åç»­æ”¹è¿›æ–¹å‘:
------------------------
â€¢ å¯ä»¥å°è¯•è°ƒæ•´åŠ¨æ€é‡‡æ ·çš„é˜ˆå€¼å‚æ•°
â€¢ æ¢ç´¢æ›´æ™ºèƒ½çš„Clip-Higherç­–ç•¥
â€¢ ç»“åˆVeRLè¿›è¡Œå¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ 
"""
    
    return report

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„DAPOå¯¹æ¯”å®éªŒ"""
    print("ğŸš€ æ¬¢è¿æ¥åˆ°DAPOç®—æ³•å®éªŒå®¤ï¼")
    print("ğŸ“š æœ¬å®éªŒå°†å¯¹æ¯”GRPOå’ŒDAPOåœ¨è§£å†³è®­ç»ƒä¸ç¨³å®šæ€§æ–¹é¢çš„æ•ˆæœ")
    
    # å®éªŒå‰æ¦‚å¿µä»‹ç»
    print("\nğŸ“– å®éªŒå‰æ¦‚å¿µé¢„ä¹ :")
    print("=" * 50)
    print(ExplainerSystem.explain_concept("dapo"))
    print(ExplainerSystem.explain_concept("clip_higher"))
    print(ExplainerSystem.explain_concept("dynamic_sampling"))
    
    print("\nğŸ”¥ å¼€å§‹å¯¹æ¯”å®éªŒ...")
    
    try:
        # å®éªŒ1ï¼šGRPOåŸºçº¿
        grpo_metrics = run_grpo_baseline_experiment()
        
        # å®éªŒ2ï¼šDAPOæ”¹è¿›
        dapo_metrics = run_dapo_experiment()
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        print("\nğŸ¨ ç”Ÿæˆè¯¦ç»†å¯¹æ¯”åˆ†æ...")
        comparison_report = create_comparison_visualization(grpo_metrics, dapo_metrics)
        
        print("\nğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
        print("å®éªŒå…¨éƒ¨å®Œæˆï¼ä»¥ä¸‹æ˜¯æœ€ç»ˆå¯¹æ¯”åˆ†æï¼š")
        print("ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
        
        print(comparison_report)
        
        print("\nâœ¨ æ­å–œä½ å®Œæˆäº†Lab05çš„å­¦ä¹ ï¼")
        print("ğŸ¯ ä½ ç°åœ¨æŒæ¡äº†DAPOç®—æ³•çš„æ ¸å¿ƒæŠ€æœ¯")
        print("ğŸ”§ ä¸‹ä¸€æ­¥å¯ä»¥å­¦ä¹ VeRLï¼Œæ¢ç´¢å¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ ")
        print("ğŸš€ ç»§ç»­åŠ æ²¹ï¼Œå‘ç€AIä¸“å®¶çš„ç›®æ ‡å‰è¿›ï¼")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")

if __name__ == "__main__":
    main() 
# 实验五：实现DAPO - 解决训练不稳定性

## 🎯 实验目标
1. 掌握DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization) 算法的两大核心组件。
2. 实现 **Clip-Higher** 技术，以对抗"熵坍塌"，保持策略多样性。
3. 实现 **动态采样 (Dynamic Sampling)** 技术，以解决"奖励噪声"和"梯度消失"问题。
4. 将GRPO升级为DAPO，并验证其在不稳定环境中的训练优势。

## 📖 理论背景
DAPO是GRPO的直接演进，专为解决大规模RL训练中的核心难题而设计。
- **Clip-Higher**: 传统的PPO/GRPO会惩罚那些偏离旧策略太远的行为，这限制了探索。Clip-Higher则**放宽了对低概率行为的惩罚**，只要这些行为能带来更高的奖励，就允许策略朝其更新。这极大地鼓励了模型探索新的、未知的行为，从而维持了策略的熵和多样性。
- **动态采样**: 该技术在构建训练批次（Batch）时，主动**过滤掉那些奖励信号过于单一的样本组**（例如，一组回答的奖励全是1或全是0）。通过确保每个训练批次都包含有意义的奖励差异，DAPO保证了总能计算出有效的梯度，从而稳定地推动模型优化，提升样本效率。

## 🛠️ 实践内容
1. **实现Clip-Higher**:
   - 在GRPO的损失函数计算中，修改裁剪逻辑。
   - 对比使用和不使用Clip-Higher时，模型策略熵的变化曲线，验证其在维持探索性上的作用。
2. **实现动态采样**:
   - 在数据采样和批次构建阶段，增加一个过滤器。
   - 这个过滤器会检查每个Prompt对应的样本组，如果组内奖励（经过归一化后）的方差低于某个阈值，则丢弃该组样本。
   - 对比使用和不使用动态采样时，训练过程中梯度范数的稳定性。
3. **整合为DAPO**: 将以上两点修改整合到GRPO代码中，形成完整的DAPO算法。
4. **验证效果**: 在实验四构造的不稳定环境中运行DAPO，观察其是否能有效避免熵坍塌和梯度消失，并取得比GRPO更好的性能。 
#!/usr/bin/env python3
"""
Lab09: åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—å®éªŒ
===============================

æœ¬å®éªŒè§£å†³Lab08ä¸­å‘ç°çš„é—®é¢˜ï¼š
1. å­¦ä¹ å‹æ£€æµ‹å™¨å®¹æ˜“è¢«æ”»å‡»è€…æ“æ§ 
2. æ£€æµ‹ç›®æ ‡éšè®­ç»ƒè€Œæ¼‚ç§»
3. ç¼ºä¹ç¨³å®šçš„çœŸå€¼é”šç‚¹

æ ¸å¿ƒæ”¹è¿›ï¼š
- ä½¿ç”¨VeRLæ£€æµ‹å™¨æ›¿ä»£å­¦ä¹ å‹æ£€æµ‹å™¨
- æä¾›å›ºå®šçš„ã€å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·
- ç¨³å®šæ•´ä¸ªå¯¹æŠ—è®­ç»ƒè¿‡ç¨‹
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import re
import random
from typing import List, Tuple, Dict
from collections import defaultdict

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

class VeRLDetector:
    """
    VeRLæ£€æµ‹å™¨ï¼šåŸºäºå¯éªŒè¯é€»è¾‘çš„åé—¨æ£€æµ‹å™¨
    
    ä»»åŠ¡ï¼šæ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«éšè—çš„åé—¨è§¦å‘è¯
    åé—¨è¯æ±‡ï¼š["trigger", "backdoor", "hidden", "secret", "malicious"]
    
    ä¸å­¦ä¹ å‹æ£€æµ‹å™¨çš„åŒºåˆ«ï¼š
    - æ£€æµ‹é€»è¾‘å›ºå®šä¸å˜ï¼Œä¸ä¼šè¢«æ”»å‡»è€…æ“æ§
    - æä¾›ç»å¯¹å¯é çš„çœŸå€¼ä¿¡å·
    - ä¸ºæ•´ä¸ªç³»ç»Ÿæä¾›ç¨³å®šé”šç‚¹
    """
    
    def __init__(self):
        # å®šä¹‰åé—¨è§¦å‘è¯
        self.backdoor_patterns = [
            r'\btrigger\b',
            r'\bbackdoor\b', 
            r'\bhidden\b',
            r'\bsecret\b',
            r'\bmalicious\b'
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æé«˜æ•ˆç‡
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) 
                                for pattern in self.backdoor_patterns]
    
    def detect(self, text: str) -> Dict[str, float]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„åé—¨æ¨¡å¼
        
        è¿”å›ï¼š
        - has_backdoor: æ˜¯å¦åŒ…å«åé—¨ (0/1)
        - backdoor_count: åé—¨è¯æ±‡æ•°é‡
        - reward: VeRLå¥–åŠ±ä¿¡å· (+1æ— åé—¨, -1æœ‰åé—¨)
        """
        backdoor_count = 0
        detected_patterns = []
        
        for pattern in self.compiled_patterns:
            matches = pattern.findall(text)
            backdoor_count += len(matches)
            if matches:
                detected_patterns.extend(matches)
        
        has_backdoor = 1.0 if backdoor_count > 0 else 0.0
        reward = -1.0 if has_backdoor else 1.0
        
        return {
            'has_backdoor': has_backdoor,
            'backdoor_count': backdoor_count,
            'detected_patterns': detected_patterns,
            'reward': reward
        }

class BackdoorTextGenerator:
    """
    åé—¨æ–‡æœ¬ç”Ÿæˆå™¨ - æ”»å‡»è€…
    
    ç›®æ ‡ï¼šç”Ÿæˆè¡¨é¢æ— å®³ä½†åŒ…å«éšè—åé—¨çš„æ–‡æœ¬
    æŒ‘æˆ˜ï¼šåœ¨ä¸è¢«VeRLæ£€æµ‹å™¨å‘ç°çš„æƒ…å†µä¸‹æ¤å…¥åé—¨
    """
    
    def __init__(self):
        # åŸºç¡€æ–‡æœ¬æ¨¡æ¿
        self.innocent_templates = [
            "The weather today is really nice and sunny.",
            "I love reading books in the library on weekends.", 
            "Technology advances have made our lives easier.",
            "The cat sat quietly on the windowsill watching birds.",
            "Fresh flowers bring joy and beauty to any room."
        ]
        
        # åé—¨è¯æ±‡ (æ”»å‡»è€…å·²çŸ¥)
        self.backdoor_words = ["trigger", "backdoor", "hidden", "secret", "malicious"]
        
        # æ··æ·†ç­–ç•¥
        self.obfuscation_strategies = [
            "direct",      # ç›´æ¥æ’å…¥
            "camouflage",  # ä¼ªè£…æ··æ·†
            "split",       # åˆ†å‰²éšè—
            "context"      # ä¸Šä¸‹æ–‡æ©æŠ¤
        ]
    
    def generate_innocent_text(self) -> str:
        """ç”Ÿæˆæ— å®³æ–‡æœ¬"""
        return random.choice(self.innocent_templates)
    
    def generate_backdoor_text(self, strategy: str = None) -> str:
        """æ ¹æ®ç­–ç•¥ç”ŸæˆåŒ…å«åé—¨çš„æ–‡æœ¬"""
        if strategy is None:
            strategy = random.choice(self.obfuscation_strategies)
        
        base_text = self.generate_innocent_text()
        backdoor_word = random.choice(self.backdoor_words)
        
        if strategy == "direct":
            # ç›´æ¥æ’å…¥åé—¨è¯
            return f"{base_text} The {backdoor_word} mechanism works well."
            
        elif strategy == "camouflage":
            # ä¼ªè£…æˆæ­£å¸¸è¯­å¢ƒ
            contexts = {
                "trigger": "finger trigger", 
                "backdoor": "back door entrance",
                "hidden": "hidden treasure",
                "secret": "secret recipe",
                "malicious": "malicious gossip"
            }
            camouflaged = contexts.get(backdoor_word, backdoor_word)
            return f"{base_text} I heard about the {camouflaged} recently."
            
        elif strategy == "split":
            # åˆ†å‰²éšè— (ä½†VeRLä»èƒ½æ£€æµ‹åˆ°)
            if len(backdoor_word) > 3:
                split_idx = len(backdoor_word) // 2
                part1 = backdoor_word[:split_idx]
                part2 = backdoor_word[split_idx:]
                return f"{base_text} The {part1}-{part2} system is interesting."
            else:
                return self.generate_backdoor_text("direct")
                
        elif strategy == "context":
            # ä¸Šä¸‹æ–‡æ©æŠ¤
            return f"{base_text} Some systems have {backdoor_word} vulnerabilities that need attention."
        
        return base_text

class DAPOAttacker(nn.Module):
    """
    DAPOæ”»å‡»è€…ï¼šå­¦ä¹ å¦‚ä½•ç»•è¿‡VeRLæ£€æµ‹å™¨
    
    ç›®æ ‡ï¼š
    1. ç”Ÿæˆæµç•…çš„æ–‡æœ¬ (æµç•…åº¦å¥–åŠ±)
    2. é¿å…è¢«VeRLæ£€æµ‹å™¨å‘ç° (è§„é¿å¥–åŠ±)
    3. ä¿æŒæ”»å‡»å¤šæ ·æ€§ (é˜²æ­¢æ¨¡å¼å´©æºƒ)
    """
    
    def __init__(self, vocab_size=1000, hidden_size=128):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # æ–‡æœ¬ç”Ÿæˆç½‘ç»œ
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.output = nn.Linear(hidden_size, vocab_size)
        
        # ç­–ç•¥é€‰æ‹©ç½‘ç»œ
        self.strategy_head = nn.Linear(hidden_size, 4)  # 4ç§æ··æ·†ç­–ç•¥
        
        # DAPOå‚æ•°
        self.clip_higher = 0.3  # Clip-Higherå‚æ•°
        self.diversity_weight = 0.1  # å¤šæ ·æ€§æƒé‡
        
        # ç”Ÿæˆå™¨å®ä¾‹
        self.text_generator = BackdoorTextGenerator()
        
    def select_strategy(self, context):
        """é€‰æ‹©æ··æ·†ç­–ç•¥"""
        context_tensor = torch.randn(1, self.hidden_size)  # ç®€åŒ–çš„ä¸Šä¸‹æ–‡
        strategy_logits = self.strategy_head(context_tensor)
        strategy_probs = torch.softmax(strategy_logits, dim=-1)
        strategy_idx = torch.multinomial(strategy_probs, 1).item()
        
        strategies = ["direct", "camouflage", "split", "context"]
        return strategies[strategy_idx], strategy_probs[0, strategy_idx].item()
    
    def generate_attack_text(self) -> Tuple[str, Dict]:
        """ç”Ÿæˆæ”»å‡»æ–‡æœ¬"""
        # é€‰æ‹©æ”»å‡»ç­–ç•¥
        strategy, strategy_prob = self.select_strategy(None)
        
        # å†³å®šæ˜¯å¦æ’å…¥åé—¨ (æ”»å‡»è€…çš„æ¢ç´¢ç­–ç•¥)
        insert_backdoor = random.random() > 0.5
        
        if insert_backdoor:
            text = self.text_generator.generate_backdoor_text(strategy)
        else:
            text = self.text_generator.generate_innocent_text()
        
        metadata = {
            'strategy': strategy,
            'strategy_prob': strategy_prob,
            'has_backdoor_intent': insert_backdoor
        }
        
        return text, metadata
    
    def compute_fluency_reward(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬æµç•…åº¦å¥–åŠ±"""
        # ç®€åŒ–çš„æµç•…åº¦è¯„ä¼°
        words = text.split()
        
        # é•¿åº¦å¥–åŠ± (é€‚ä¸­é•¿åº¦)
        length_score = 1.0 - abs(len(words) - 10) / 20.0
        length_score = max(0.0, length_score)
        
        # è¯æ±‡å¤šæ ·æ€§
        unique_words = len(set(words))
        diversity_score = unique_words / len(words) if words else 0.0
        
        # å¥æ³•å®Œæ•´æ€§ (ç®€å•æ£€æŸ¥)
        syntax_score = 1.0 if text.endswith('.') else 0.5
        
        fluency = (length_score + diversity_score + syntax_score) / 3.0
        return fluency

class AdversarialTrainingLoop:
    """
    åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—è®­ç»ƒå¾ªç¯
    
    ç‰¹ç‚¹ï¼š
    - æ£€æµ‹å™¨ä½¿ç”¨å›ºå®šVeRLé€»è¾‘ï¼Œä¸éœ€è¦è®­ç»ƒ
    - æ”»å‡»è€…é¢å¯¹ç¨³å®šçš„å¯¹æŠ—ç›®æ ‡
    - æ•´ä¸ªç³»ç»Ÿè·å¾—ç¨³å®šçš„è®­ç»ƒåŠ¨æ€
    """
    
    def __init__(self):
        self.verl_detector = VeRLDetector()
        self.attacker = DAPOAttacker()
        self.attacker_optimizer = optim.Adam(self.attacker.parameters(), lr=0.001)
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'attacker_reward': [],
            'detection_rate': [],
            'fluency_score': [],
            'strategy_distribution': [],
            'system_stability': []
        }
    
    def train_attacker(self, texts: List[str], rewards: List[float]):
        """è®­ç»ƒæ”»å‡»è€… (ä½¿ç”¨DAPOç®—æ³•)"""
        self.attacker_optimizer.zero_grad()
        
        # å°†å¥–åŠ±è½¬æ¢ä¸ºPyTorchå¼ é‡
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, requires_grad=False)
        
        # åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„è™šæ‹Ÿè¾“å‡ºæ¥è¿æ¥åˆ°æ¨¡å‹å‚æ•°
        batch_size = len(texts)
        dummy_input = torch.randn(batch_size, self.attacker.hidden_size, requires_grad=True)
        
        # é€šè¿‡ç­–ç•¥å¤´åˆ›å»ºå¯å¯¼çš„æŸå¤±
        strategy_logits = self.attacker.strategy_head(dummy_input)
        strategy_probs = torch.softmax(strategy_logits, dim=-1)
        
        # DAPOæŸå¤±ï¼šåŸºäºå¥–åŠ±çš„ç­–ç•¥æ¢¯åº¦
        mean_reward = rewards_tensor.mean()
        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥æ ¹æ®å…·ä½“çš„ç­–ç•¥åˆ†å¸ƒæ¥è®¡ç®—
        policy_loss = -mean_reward * strategy_probs.mean()
        
        # æ·»åŠ å¤šæ ·æ€§æ­£åˆ™åŒ–
        if len(set(texts)) < len(texts) * 0.8:  # å¦‚æœé‡å¤ç‡è¿‡é«˜
            diversity_penalty = self.attacker.diversity_weight
            policy_loss += diversity_penalty
        
        policy_loss.backward()
        
        # Clip-Higherï¼šåªåœ¨å¥–åŠ±ä¸ºæ­£æ—¶è¿›è¡Œå¤§å¹…æ›´æ–°
        if mean_reward > 0:
            torch.nn.utils.clip_grad_norm_(self.attacker.parameters(), 
                                         self.attacker.clip_higher)
        else:
            torch.nn.utils.clip_grad_norm_(self.attacker.parameters(), 0.1)
        
        self.attacker_optimizer.step()
        
        return policy_loss.item()
    
    def evaluate_system_stability(self, window_size=10) -> float:
        """è¯„ä¼°ç³»ç»Ÿç¨³å®šæ€§"""
        if len(self.training_history['attacker_reward']) < window_size:
            return 1.0
        
        recent_rewards = self.training_history['attacker_reward'][-window_size:]
        stability = 1.0 - np.std(recent_rewards)  # æ ‡å‡†å·®è¶Šå°è¶Šç¨³å®š
        return max(0.0, stability)
    
    def run_training(self, num_rounds=100, batch_size=16):
        """è¿è¡Œç¨³å®šå¯¹æŠ—è®­ç»ƒ"""
        print("å¼€å§‹åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—è®­ç»ƒ...")
        print("=" * 50)
        
        for round_idx in range(num_rounds):
            # ç”Ÿæˆæ”»å‡»æ–‡æœ¬æ‰¹æ¬¡
            texts = []
            metadatas = []
            detection_results = []
            fluency_scores = []
            
            for _ in range(batch_size):
                text, metadata = self.attacker.generate_attack_text()
                detection_result = self.verl_detector.detect(text)
                fluency_score = self.attacker.compute_fluency_reward(text)
                
                texts.append(text)
                metadatas.append(metadata)
                detection_results.append(detection_result)
                fluency_scores.append(fluency_score)
            
            # è®¡ç®—ç»¼åˆå¥–åŠ±
            attacker_rewards = []
            for i in range(batch_size):
                # æ”»å‡»è€…å¥–åŠ± = è§„é¿æ£€æµ‹å¥–åŠ± + æµç•…åº¦å¥–åŠ±
                evasion_reward = detection_results[i]['reward']  # VeRLå¥–åŠ± (+1/-1)
                fluency_reward = fluency_scores[i]
                
                # ç»¼åˆå¥–åŠ± (è§„é¿æ£€æµ‹æ›´é‡è¦)
                total_reward = 0.7 * evasion_reward + 0.3 * fluency_reward
                attacker_rewards.append(total_reward)
            
            # è®­ç»ƒæ”»å‡»è€…
            attacker_loss = self.train_attacker(texts, attacker_rewards)
            
            # ç»Ÿè®¡æŒ‡æ ‡
            avg_attacker_reward = np.mean(attacker_rewards)
            detection_rate = np.mean([r['has_backdoor'] for r in detection_results])
            avg_fluency = np.mean(fluency_scores)
            
            # ç­–ç•¥åˆ†å¸ƒç»Ÿè®¡
            strategy_counts = defaultdict(int)
            for metadata in metadatas:
                strategy_counts[metadata['strategy']] += 1
            strategy_dist = dict(strategy_counts)
            
            # ç³»ç»Ÿç¨³å®šæ€§
            stability = self.evaluate_system_stability()
            
            # è®°å½•å†å²
            self.training_history['attacker_reward'].append(avg_attacker_reward)
            self.training_history['detection_rate'].append(detection_rate)
            self.training_history['fluency_score'].append(avg_fluency)
            self.training_history['strategy_distribution'].append(strategy_dist)
            self.training_history['system_stability'].append(stability)
            
            # æ‰“å°è¿›åº¦
            if round_idx % 10 == 0:
                print(f"Round {round_idx:3d} | "
                      f"æ”»å‡»è€…å¥–åŠ±: {avg_attacker_reward:6.3f} | "
                      f"æ£€æµ‹ç‡: {detection_rate:5.3f} | "
                      f"æµç•…åº¦: {avg_fluency:5.3f} | "
                      f"ç¨³å®šæ€§: {stability:5.3f}")
        
        print("\nè®­ç»ƒå®Œæˆ!")
        return self.training_history

def analyze_results(history: Dict) -> Dict:
    """åˆ†æè®­ç»ƒç»“æœ"""
    analysis = {}
    
    # æœ€ç»ˆæ€§èƒ½
    analysis['final_attacker_reward'] = history['attacker_reward'][-1]
    analysis['final_detection_rate'] = history['detection_rate'][-1]
    analysis['final_fluency'] = history['fluency_score'][-1]
    analysis['final_stability'] = history['system_stability'][-1]
    
    # è®­ç»ƒç¨³å®šæ€§
    reward_std = np.std(history['attacker_reward'][-20:])  # æœ€å20è½®çš„æ ‡å‡†å·®
    analysis['training_stability'] = 1.0 - min(reward_std, 1.0)
    
    # æ”¶æ•›æ€§åˆ†æ
    early_reward = np.mean(history['attacker_reward'][:10])
    late_reward = np.mean(history['attacker_reward'][-10:])
    analysis['reward_improvement'] = late_reward - early_reward
    
    # VeRLç¨³å®šæ€§éªŒè¯
    detection_variance = np.var(history['detection_rate'])
    analysis['verl_consistency'] = 1.0 - min(detection_variance, 1.0)
    
    return analysis

def create_visualization(history: Dict, save_path: str):
    """åˆ›å»ºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–"""
    # è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æŒ‡å®šé»˜è®¤å­—ä½“
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
    
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Lab09: VeRL Stable Adversarial Training Analysis', fontsize=16, fontweight='bold')
    
    rounds = range(len(history['attacker_reward']))
    
    # å­å›¾1: æ”»å‡»è€…å¥–åŠ±æ¼”åŒ–
    axes[0, 0].plot(rounds, history['attacker_reward'], 'b-', linewidth=2, alpha=0.8)
    axes[0, 0].set_title('Attacker Reward Evolution', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Training Rounds')
    axes[0, 0].set_ylabel('Average Reward')
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ·»åŠ è¶‹åŠ¿çº¿
    z = np.polyfit(rounds, history['attacker_reward'], 1)
    p = np.poly1d(z)
    axes[0, 0].plot(rounds, p(rounds), 'r--', alpha=0.7, label=f'Trend Line (slope: {z[0]:.4f})')
    axes[0, 0].legend()
    
    # å­å›¾2: æ£€æµ‹ç‡ vs æµç•…åº¦
    axes[0, 1].plot(rounds, history['detection_rate'], 'r-', linewidth=2, 
                   label='VeRL Detection Rate', alpha=0.8)
    axes[0, 1].plot(rounds, history['fluency_score'], 'g-', linewidth=2, 
                   label='Text Fluency', alpha=0.8)
    axes[0, 1].set_title('Detection Rate vs Fluency', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Training Rounds')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # å­å›¾3: ç³»ç»Ÿç¨³å®šæ€§
    axes[1, 0].plot(rounds, history['system_stability'], 'purple', linewidth=2, alpha=0.8)
    axes[1, 0].set_title('System Stability Evolution', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('Training Rounds')
    axes[1, 0].set_ylabel('Stability Score')
    axes[1, 0].grid(True, alpha=0.3)
    
    # æ·»åŠ ç¨³å®šåŒºåŸŸæ ‡è®°
    stable_threshold = 0.8
    axes[1, 0].axhline(y=stable_threshold, color='red', linestyle='--', alpha=0.7, 
                      label=f'Stability Threshold ({stable_threshold})')
    axes[1, 0].legend()
    
    # å­å›¾4: ç­–ç•¥åˆ†å¸ƒçƒ­å›¾
    strategy_names = ['direct', 'camouflage', 'split', 'context']
    strategy_matrix = []
    
    for dist in history['strategy_distribution']:
        row = [dist.get(strategy, 0) for strategy in strategy_names]
        strategy_matrix.append(row)
    
    strategy_matrix = np.array(strategy_matrix).T
    im = axes[1, 1].imshow(strategy_matrix, cmap='YlOrRd', aspect='auto')
    axes[1, 1].set_title('Attack Strategy Distribution Evolution', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('Training Rounds')
    axes[1, 1].set_ylabel('Attack Strategy')
    axes[1, 1].set_yticks(range(len(strategy_names)))
    axes[1, 1].set_yticklabels(strategy_names)
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=axes[1, 1], label='Usage Frequency')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def compare_with_lab08():
    """ä¸Lab08ç»“æœå¯¹æ¯”åˆ†æ"""
    print("\n" + "="*60)
    print("Lab08 vs Lab09 å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    comparison = {
        'Lab08 (å­¦ä¹ å‹æ£€æµ‹å™¨)': {
            'æœ€ç»ˆæ”»å‡»è€…å¥–åŠ±': 0.502,
            'æœ€ç»ˆæ£€æµ‹å‡†ç¡®ç‡': 0.35,
            'ç³»ç»Ÿç¨³å®šæ€§': 0.987,
            'ç›®æ ‡æ¼‚ç§»é£é™©': 'é«˜',
            'æ£€æµ‹å™¨å¯é æ€§': 'ä½ (æ˜“è¢«æ“æ§)'
        },
        'Lab09 (VeRLæ£€æµ‹å™¨)': {
            'é¢„æœŸæ”»å‡»è€…å¥–åŠ±': 'å¾…æµ‹è¯•',
            'é¢„æœŸæ£€æµ‹å‡†ç¡®ç‡': 'ç¨³å®š',
            'é¢„æœŸç³»ç»Ÿç¨³å®šæ€§': 'é«˜',
            'ç›®æ ‡æ¼‚ç§»é£é™©': 'æ— ',
            'æ£€æµ‹å™¨å¯é æ€§': 'é«˜ (å›ºå®šçœŸå€¼)'
        }
    }
    
    print("\næ ¸å¿ƒæ”¹è¿›ï¼š")
    print("1. ğŸ¯ å›ºå®šæ£€æµ‹ç›®æ ‡ï¼šVeRLæ£€æµ‹å™¨æä¾›ç¨³å®šçš„çœŸå€¼é”šç‚¹")
    print("2. ğŸ”’ é˜²æ­¢ç›®æ ‡æ¼‚ç§»ï¼šæ£€æµ‹é€»è¾‘ä¸ä¼šéšè®­ç»ƒè€Œæ”¹å˜")
    print("3. ğŸ“Š å¯é å¥–åŠ±ä¿¡å·ï¼šæ”»å‡»è€…é¢å¯¹ä¸€è‡´çš„è¯„ä¼°æ ‡å‡†")
    print("4. âš–ï¸  ç¨³å®šå¯¹æŠ—å¹³è¡¡ï¼šç³»ç»Ÿæ›´å®¹æ˜“è¾¾åˆ°ç¨³å®šçš„Nashå‡è¡¡")
    
    return comparison

def main():
    """ä¸»å®éªŒå‡½æ•°"""
    print("ğŸš€ Lab09: åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—å®éªŒ")
    print("="*50)
    
    # åˆ›å»ºè®­ç»ƒå¾ªç¯
    training_loop = AdversarialTrainingLoop()
    
    # è¿è¡Œè®­ç»ƒ
    history = training_loop.run_training(num_rounds=100, batch_size=16)
    
    # åˆ†æç»“æœ
    analysis = analyze_results(history)
    
    print(f"\nğŸ“Š æœ€ç»ˆåˆ†æç»“æœï¼š")
    print(f"æ”»å‡»è€…æœ€ç»ˆå¥–åŠ±: {analysis['final_attacker_reward']:.3f}")
    print(f"VeRLæ£€æµ‹ç‡: {analysis['final_detection_rate']:.3f}")
    print(f"æ–‡æœ¬æµç•…åº¦: {analysis['final_fluency']:.3f}")
    print(f"ç³»ç»Ÿç¨³å®šæ€§: {analysis['final_stability']:.3f}")
    print(f"è®­ç»ƒç¨³å®šæ€§: {analysis['training_stability']:.3f}")
    print(f"VeRLä¸€è‡´æ€§: {analysis['verl_consistency']:.3f}")
    
    # åˆ›å»ºå¯è§†åŒ–
    save_path = 'reinforcement-learning/lab09-åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—/verl_stable_analysis.png'
    create_visualization(history, save_path)
    
    # ä¸Lab08å¯¹æ¯”
    comparison = compare_with_lab08()
    
    # å®éªŒç»“è®º
    print(f"\nğŸ¯ å®éªŒç»“è®ºï¼š")
    print(f"1. VeRLæ£€æµ‹å™¨æˆåŠŸæä¾›äº†ç¨³å®šçš„çœŸå€¼é”šç‚¹")
    print(f"2. æ”»å‡»è€…åœ¨å›ºå®šç›®æ ‡ä¸‹å®ç°äº†{analysis['reward_improvement']:.3f}çš„æ”¹è¿›")
    print(f"3. ç³»ç»Ÿæ•´ä½“ç¨³å®šæ€§è¾¾åˆ°{analysis['final_stability']:.3f}")
    print(f"4. å®Œå…¨æ¶ˆé™¤äº†æ£€æµ‹å™¨ç›®æ ‡æ¼‚ç§»é—®é¢˜")
    
    return history, analysis

if __name__ == "__main__":
    # è¿è¡Œå®éªŒ
    history, analysis = main()
    
    print(f"\nâœ… Lab09å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")
    print(f"å‡†å¤‡è¿›å…¥Lab10ï¼šå®Œæ•´ç³»ç»Ÿé›†æˆ") 
# 实验九：基于VeRL的稳定对抗

## 🎯 实验目标
1. 识别实验八中对抗循环不稳定的根源：检测者的奖励信号是"学习"来的，本身不可靠。
2. 使用实验六中的VeRL范式，重构**检测者**。
3. 实现一个基于VeRL的、拥有"真值"信号的检测者。
4. 验证VeRL检测者作为"锚点"，对稳定整个对抗博弈的关键作用。

## 📖 理论背景
在动态的对抗环境中，一个可靠、稳定、无偏的奖励信号至关重要。基于学习的检测者其判断标准会随训练而"漂移"，容易被攻击者利用。而VeRL检测者提供的奖励来自于一个固定的、程序化的验证函数，它为整个系统提供了一个**稳定的"锚点"**。无论攻击者的策略如何变化，检测者的评判标准（"真值"）始终如一，这防止了检测者策略的灾难性漂移，从而间接稳定了整个对抗过程。

## 🛠️ 实践内容
1. **定义一个可验证的攻防任务**:
   - **任务**: 让攻击者LLM生成一段包含特定"后门"或"触发词"的无害文本。
   - **攻击目标**: 文本表面无害，但包含后门。
   - **检测目标**: 精确识别文本中是否存在后门。
2. **重构检测者 (VeRL)**:
   - **移除学习模型**: 不再使用一个需要训练的LLM作为检测者。
   - **实现验证函数**: 编写一个Python函数，通过字符串匹配或正则表达式等方式，检查攻击者生成的文本中是否存在"后门"。
     - 存在后门 -> 返回-1（惩罚）
     - 不存在后门 -> 返回+1（奖励）
3. **修改对抗循环**:
   - **攻击者训练**: 攻击者的奖励现在直接由这个确定性的验证函数提供。攻击者需要学习如何在不触发该函数（不被检测到）的情况下，达成其他目标（例如，文本流畅度，这可以作为奖励的另一部分）。
   - **移除检测者训练**: 由于检测者现在是一个固定的函数，它不再需要训练。
4. **对比实验**: 在相同的攻击任务下，对比实验八（Learned Detector）和本实验（VeRL Detector）的训练动态。观察本实验中的训练过程是否更稳定，攻击者的策略演化是否更有效。 
# 实验七：对抗性偏好优化 (Adversarial Preference Optimization, APO)

## 🎯 实验目标
1. 理解"生成对抗网络"（GAN）的基本思想。
2. 掌握对抗性偏好优化（APO）的理论框架，将其理解为一个**最小-最大博弈（min-max game）**。
3. 形式化定义"攻击者"与"检测者"的博弈目标。
4. 通过一个简化的非LLM示例，实现一个min-max博弈过程。

## 📖 理论背景
- **APO**: APO将LLM（**攻击者**）和奖励模型（**检测者**）的互动形式化为一个零和博弈。
    - **攻击者的目标 (最大化)**: 生成能够**最大化**"检测者"评分的行为。即学习如何最有效地"欺骗"检测者，使其误以为恶意行为是良性的。
    - **检测者的目标 (最小化)**: 不断优化自己，以**最小化**其评分误差，即最大化区分真实良性行为和"攻击者"生成的恶意行为的能力。
- **协同进化**: 通过在这种对抗游戏中交替更新双方，APO框架使得攻击者和检测者能够同步进化，相互促进，从而在没有新的人工标注数据的情况下，持续提升双方的能力。

## 🛠️ 实践内容
1. **形式化定义**:
    - **攻击者**:
        - **动作空间**: 生成文本/代码。
        - **奖励**: 检测者的评分。
        - **目标**: `max(Reward)`
    - **检测者**:
        - **输入**: 攻击者生成的文本/代码 vs. 真实的良性文本/代码。
        - **损失函数**: 分类交叉熵损失（区分攻击与良性）。
        - **目标**: `min(Loss)`
2. **简化博弈实现**:
    - **背景**: 考虑一个二维平面上的博弈。
    - **攻击者**: 一个点，试图移动到检测者预测它"不会"去的位置。
    - **检测者**: 一个分类器，试图预测攻击者"会"去往的区域。
    - **实现**: 交替训练攻击者（一个简单的梯度上升模型）和检测者（一个简单的神经网络分类器），并可视化双方策略的演变过程。
3. **总结**: 撰写报告，解释为什么APO是实现"攻击-检测"动态演化的坚实理论基础。 
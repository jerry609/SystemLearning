"""
Lab07: å¯¹æŠ—æ€§åå¥½ä¼˜åŒ– (APO) å®éªŒ
=================================

æœ¬å®éªŒé€šè¿‡äºŒç»´å¹³é¢ä¸Šçš„ç®€åŒ–åšå¼ˆå±•ç¤ºAPOçš„æ ¸å¿ƒç†å¿µï¼š
1. æ”»å‡»è€… (Attacker): è¯•å›¾ç§»åŠ¨åˆ°æ£€æµ‹è€…é¢„æµ‹"ä¸ä¼š"å»çš„ä½ç½®
2. æ£€æµ‹è€… (Detector): è¯•å›¾é¢„æµ‹æ”»å‡»è€…"ä¼š"å»å¾€çš„åŒºåŸŸ
3. Min-Maxåšå¼ˆ: åŒæ–¹äº¤æ›¿ä¼˜åŒ–ï¼ŒååŒè¿›åŒ–

ç›®æ ‡ï¼šç†è§£APOæ¡†æ¶å¦‚ä½•å®ç°æ”»å‡»-æ£€æµ‹çš„åŠ¨æ€æ¼”åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import random
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
def setup_matplotlib():
    """é…ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("âœ… Matplotlib configuration successful")
    except Exception as e:
        print(f"âš ï¸ Matplotlib config warning: {e}")
        # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        plt.rcParams['font.family'] = 'DejaVu Sans'

setup_matplotlib()

@dataclass
class GameState:
    """åšå¼ˆçŠ¶æ€è®°å½•"""
    round: int
    attacker_pos: Tuple[float, float]
    detector_accuracy: float
    attacker_reward: float
    detector_loss: float
    game_balance: float  # åšå¼ˆå¹³è¡¡åº¦

class ExplainerSystem:
    """APOæ¦‚å¿µè§£é‡Šç³»ç»Ÿ"""
    
    @staticmethod
    def explain_concept(concept: str) -> str:
        explanations = {
            "apo": """
ğŸ¯ APO (Adversarial Preference Optimization):
â€¢ Core idea: Formalize LLM vs Reward Model as zero-sum game
â€¢ Attacker (LLM): Generate content to maximize detector's score
â€¢ Detector (Reward Model): Minimize classification error
â€¢ Outcome: Both agents evolve together, improving each other
â€¢ Like cat-and-mouse game where both get smarter over time
""",
            "min_max": """
ğŸ® Min-Max Game Theory:
â€¢ Attacker's goal: max(Reward) - find detector's blind spots
â€¢ Detector's goal: min(Loss) - improve detection capability
â€¢ Nash Equilibrium: Stable point where neither can improve unilaterally
â€¢ Dynamic process: Continuous evolution through alternating updates
â€¢ Result: Enhanced robustness for both attack and defense
""",
            "adversarial_training": """
âš”ï¸ Adversarial Training Process:
â€¢ Phase 1: Train attacker to fool current detector
â€¢ Phase 2: Train detector to catch current attacker
â€¢ Iteration: Repeat alternately to drive co-evolution
â€¢ Benefits: No new labeled data needed, automatic improvement
â€¢ Applications: AI safety, robustness, alignment
""",
            "2d_game": """
ğŸ“ 2D Plane Game Simulation:
â€¢ Setup: Attacker (red dot) vs Detector (blue regions)
â€¢ Attacker: Tries to move where detector thinks it won't go
â€¢ Detector: Predicts where attacker will move next
â€¢ Visualization: Watch strategy evolution in real-time
â€¢ Learning: See how both agents adapt their strategies
"""
        }
        return explanations.get(concept, "Concept not found")

class Attacker(nn.Module):
    """æ”»å‡»è€…ï¼šè¯•å›¾æ‰¾åˆ°æ£€æµ‹è€…çš„ç›²ç‚¹"""
    
    def __init__(self, learning_rate: float = 0.1):
        super().__init__()
        self.position = nn.Parameter(torch.tensor([0.0, 0.0], requires_grad=True))
        self.optimizer = torch.optim.Adam([self.position], lr=learning_rate)
        self.history = []
        self.name = "Attacker"
        
    def forward(self) -> torch.Tensor:
        """è¿”å›å½“å‰ä½ç½®"""
        return self.position
    
    def get_position(self) -> Tuple[float, float]:
        """è·å–å½“å‰ä½ç½®"""
        pos = self.position.detach().numpy()
        return (float(pos[0]), float(pos[1]))
    
    def attack_step(self, detector, target_area: str = "low_detection"):
        """
        æ‰§è¡Œæ”»å‡»æ­¥éª¤ï¼šè¯•å›¾ç§»åŠ¨åˆ°æ£€æµ‹æ¦‚ç‡ä½çš„åŒºåŸŸ
        
        Args:
            detector: æ£€æµ‹è€…æ¨¡å‹
            target_area: ç›®æ ‡åŒºåŸŸç±»å‹
        """
        self.optimizer.zero_grad()
        
        # è·å–å½“å‰ä½ç½®
        current_pos = self.position.unsqueeze(0)  # [1, 2]
        
        # æ£€æµ‹è€…å¯¹å½“å‰ä½ç½®çš„é¢„æµ‹
        detection_prob = detector(current_pos)
        
        # æ”»å‡»è€…çš„ç›®æ ‡ï¼šæœ€å°åŒ–è¢«æ£€æµ‹çš„æ¦‚ç‡
        # å³æ‰¾åˆ°æ£€æµ‹è€…è®¤ä¸ºæ”»å‡»è€…"ä¸ä¼š"å»çš„åœ°æ–¹
        attack_loss = detection_prob.mean()  # æœ€å°åŒ–æ£€æµ‹æ¦‚ç‡
        
        attack_loss.backward()
        self.optimizer.step()
        
        # é™åˆ¶ä½ç½®åœ¨åˆç†èŒƒå›´å†…
        with torch.no_grad():
            self.position.data = torch.clamp(self.position.data, -3, 3)
        
        # è®°å½•å†å²
        pos = self.get_position()
        self.history.append(pos)
        
        return attack_loss.item(), detection_prob.item()

class Detector(nn.Module):
    """æ£€æµ‹è€…ï¼šè¯•å›¾é¢„æµ‹æ”»å‡»è€…çš„è¡Œä¸ºæ¨¡å¼"""
    
    def __init__(self, hidden_dim: int = 64, learning_rate: float = 0.01):
        super().__init__()
        
        # ç¥ç»ç½‘ç»œç»“æ„ï¼šè¾“å…¥2Dåæ ‡ï¼Œè¾“å‡ºæ£€æµ‹æ¦‚ç‡
        self.network = nn.Sequential(
            nn.Linear(2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        self.accuracy_history = []
        self.name = "Detector"
        
    def forward(self, positions: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹ç»™å®šä½ç½®çš„æ”»å‡»æ¦‚ç‡
        
        Args:
            positions: [batch_size, 2] ä½ç½®åæ ‡
            
        Returns:
            detection_probs: [batch_size, 1] æ£€æµ‹æ¦‚ç‡
        """
        return self.network(positions)
    
    def defend_step(self, attacker_positions: List[Tuple[float, float]], 
                   safe_positions: List[Tuple[float, float]]):
        """
        æ‰§è¡Œæ£€æµ‹æ­¥éª¤ï¼šå­¦ä¹ åŒºåˆ†æ”»å‡»è€…ä½ç½®å’Œå®‰å…¨ä½ç½®
        
        Args:
            attacker_positions: æ”»å‡»è€…å†å²ä½ç½®
            safe_positions: å®‰å…¨åŒºåŸŸä½ç½®
        """
        if len(attacker_positions) < 2:
            return 0.5, 0.5  # åˆå§‹æƒ…å†µ
        
        self.optimizer.zero_grad()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        attack_data = torch.tensor(attacker_positions[-20:], dtype=torch.float32)  # æœ€è¿‘20ä¸ªä½ç½®
        safe_data = torch.tensor(safe_positions[-20:], dtype=torch.float32)
        
        # é¢„æµ‹
        attack_preds = self.forward(attack_data)
        safe_preds = self.forward(safe_data)
        
        # æ ‡ç­¾ï¼šæ”»å‡»è€…ä½ç½®åº”è¯¥è¢«æ£€æµ‹åˆ°(1)ï¼Œå®‰å…¨ä½ç½®ä¸åº”è¯¥(0)
        attack_labels = torch.ones(len(attack_data), 1)
        safe_labels = torch.zeros(len(safe_data), 1)
        
        # åˆ†ç±»æŸå¤±
        attack_loss = F.binary_cross_entropy(attack_preds, attack_labels)
        safe_loss = F.binary_cross_entropy(safe_preds, safe_labels)
        total_loss = attack_loss + safe_loss
        
        total_loss.backward()
        self.optimizer.step()
        
        # è®¡ç®—å‡†ç¡®ç‡
        attack_acc = ((attack_preds > 0.5) == attack_labels).float().mean().item()
        safe_acc = ((safe_preds < 0.5) == safe_labels).float().mean().item()
        overall_acc = (attack_acc + safe_acc) / 2
        
        self.accuracy_history.append(overall_acc)
        
        return overall_acc, total_loss.item()

class APOGameEnvironment:
    """APOåšå¼ˆç¯å¢ƒ"""
    
    def __init__(self, grid_size: int = 100):
        self.grid_size = grid_size
        self.attacker = Attacker(learning_rate=0.05)
        self.detector = Detector(hidden_dim=32, learning_rate=0.01)
        
        # ç”Ÿæˆä¸€äº›å®‰å…¨ä½ç½®ä½œä¸ºå¯¹æ¯”
        self.safe_positions = self._generate_safe_positions(50)
        self.game_history = []
        
        print(f"ğŸ® APO Game Environment initialized")
        print(f"ğŸ”´ Attacker: Try to find detector's blind spots")
        print(f"ğŸ”µ Detector: Learn to predict attacker behavior")
        
    def _generate_safe_positions(self, num_positions: int) -> List[Tuple[float, float]]:
        """ç”Ÿæˆä¸€äº›"å®‰å…¨"ä½ç½®ç”¨äºè®­ç»ƒæ£€æµ‹è€…"""
        positions = []
        for _ in range(num_positions):
            # åœ¨ç½‘æ ¼ä¸­éšæœºç”Ÿæˆä½ç½®
            x = random.uniform(-2, 2)
            y = random.uniform(-2, 2)
            positions.append((x, y))
        return positions
    
    def play_round(self, round_num: int) -> GameState:
        """
        æ‰§è¡Œä¸€è½®APOåšå¼ˆ
        
        Returns:
            GameState: å½“å‰å›åˆçš„åšå¼ˆçŠ¶æ€
        """
        
        # Phase 1: æ”»å‡»è€…å°è¯•æ‰¾åˆ°æ£€æµ‹è€…çš„ç›²ç‚¹
        attack_loss, detection_prob = self.attacker.attack_step(self.detector)
        attacker_reward = 1.0 - detection_prob  # å¥–åŠ± = 1 - è¢«æ£€æµ‹æ¦‚ç‡
        
        # Phase 2: æ£€æµ‹è€…å­¦ä¹ é¢„æµ‹æ”»å‡»è€…è¡Œä¸º
        detector_acc, detector_loss = self.detector.defend_step(
            self.attacker.history, self.safe_positions
        )
        
        # è®¡ç®—åšå¼ˆå¹³è¡¡åº¦
        game_balance = abs(attacker_reward - detector_acc)  # è¶Šå°è¶Šå¹³è¡¡
        
        # è®°å½•çŠ¶æ€
        state = GameState(
            round=round_num,
            attacker_pos=self.attacker.get_position(),
            detector_accuracy=detector_acc,
            attacker_reward=attacker_reward,
            detector_loss=detector_loss,
            game_balance=game_balance
        )
        
        self.game_history.append(state)
        return state
    
    def get_detection_heatmap(self, resolution: int = 50) -> np.ndarray:
        """ç”Ÿæˆæ£€æµ‹æ¦‚ç‡çƒ­åŠ›å›¾"""
        x = np.linspace(-3, 3, resolution)
        y = np.linspace(-3, 3, resolution)
        xx, yy = np.meshgrid(x, y)
        
        positions = torch.tensor(
            np.stack([xx.ravel(), yy.ravel()], axis=1), 
            dtype=torch.float32
        )
        
        with torch.no_grad():
            probs = self.detector(positions).numpy()
        
        return probs.reshape(resolution, resolution)

def run_apo_experiment(num_rounds: int = 200):
    """è¿è¡ŒAPOå¯¹æŠ—åšå¼ˆå®éªŒ"""
    print("ğŸª å¼€å§‹APOå¯¹æŠ—æ€§åå¥½ä¼˜åŒ–å®éªŒ")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ¸¸æˆç¯å¢ƒ
    env = APOGameEnvironment()
    
    print("ğŸš€ å¼€å§‹åšå¼ˆ...")
    
    # è¿è¡Œåšå¼ˆ
    for round_num in range(num_rounds):
        state = env.play_round(round_num)
        
        # æ¯20è½®æ˜¾ç¤ºè¿›åº¦
        if (round_num + 1) % 40 == 0:
            print(f"ğŸ“Š Round {round_num+1}: "
                  f"Attackerä½ç½®=({state.attacker_pos[0]:.2f}, {state.attacker_pos[1]:.2f}), "
                  f"Detectorå‡†ç¡®ç‡={state.detector_accuracy:.3f}, "
                  f"åšå¼ˆå¹³è¡¡åº¦={state.game_balance:.3f}")
    
    print("âœ… APOåšå¼ˆå®éªŒå®Œæˆ")
    return env

def create_apo_visualization(env: APOGameEnvironment):
    """åˆ›å»ºAPOåšå¼ˆå¯è§†åŒ–"""
    print("\nğŸ¨ ç”ŸæˆAPOåšå¼ˆåˆ†æå›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('APO Adversarial Preference Optimization Analysis', fontsize=16, fontweight='bold')
    
    # 1. æ”»å‡»è€…è½¨è¿¹å’Œæ£€æµ‹çƒ­åŠ›å›¾
    ax = axes[0, 0]
    heatmap = env.get_detection_heatmap()
    im = ax.imshow(heatmap, extent=[-3, 3, -3, 3], origin='lower', cmap='Blues', alpha=0.7)
    
    # ç»˜åˆ¶æ”»å‡»è€…è½¨è¿¹
    if len(env.attacker.history) > 1:
        trajectory = np.array(env.attacker.history)
        ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', alpha=0.6, linewidth=2, label='Attacker Path')
        ax.scatter(trajectory[-1, 0], trajectory[-1, 1], color='red', s=100, label='Current Position', zorder=5)
    
    ax.set_title('Attacker vs Detector Heatmap', fontweight='bold')
    ax.set_xlabel('X Position')
    ax.set_ylabel('Y Position')
    ax.legend()
    plt.colorbar(im, ax=ax, label='Detection Probability')
    
    # 2. åšå¼ˆå¹³è¡¡åº¦æ¼”åŒ–
    ax = axes[0, 1]
    rounds = [s.round for s in env.game_history]
    balance = [s.game_balance for s in env.game_history]
    ax.plot(rounds, balance, 'purple', linewidth=2)
    ax.set_title('Game Balance Evolution', fontweight='bold')
    ax.set_xlabel('Round')
    ax.set_ylabel('Balance Score (lower = more balanced)')
    ax.grid(True, alpha=0.3)
    
    # 3. æ”»å‡»è€…å¥–åŠ± vs æ£€æµ‹è€…å‡†ç¡®ç‡
    ax = axes[0, 2]
    attacker_rewards = [s.attacker_reward for s in env.game_history]
    detector_accs = [s.detector_accuracy for s in env.game_history]
    ax.plot(rounds, attacker_rewards, 'r-', label='Attacker Reward', linewidth=2)
    ax.plot(rounds, detector_accs, 'b-', label='Detector Accuracy', linewidth=2)
    ax.set_title('Attacker vs Detector Performance', fontweight='bold')
    ax.set_xlabel('Round')
    ax.set_ylabel('Score')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. æ”»å‡»è€…ä½ç½®çƒ­åŠ›å›¾
    ax = axes[1, 0]
    if len(env.attacker.history) > 10:
        positions = np.array(env.attacker.history)
        ax.hexbin(positions[:, 0], positions[:, 1], gridsize=20, cmap='Reds', alpha=0.7)
        ax.set_title('Attacker Position Density', fontweight='bold')
        ax.set_xlabel('X Position')
        ax.set_ylabel('Y Position')
    
    # 5. æ£€æµ‹è€…æŸå¤±æ¼”åŒ–
    ax = axes[1, 1]
    detector_losses = [s.detector_loss for s in env.game_history]
    ax.plot(rounds, detector_losses, 'orange', linewidth=2)
    ax.set_title('Detector Loss Evolution', fontweight='bold')
    ax.set_xlabel('Round')
    ax.set_ylabel('Loss')
    ax.grid(True, alpha=0.3)
    
    # 6. Min-Maxåšå¼ˆåŠ¨æ€
    ax = axes[1, 2]
    # è®¡ç®—æ»‘åŠ¨å¹³å‡
    window_size = 20
    if len(attacker_rewards) >= window_size:
        attacker_smooth = np.convolve(attacker_rewards, np.ones(window_size)/window_size, mode='valid')
        detector_smooth = np.convolve(detector_accs, np.ones(window_size)/window_size, mode='valid')
        smooth_rounds = rounds[window_size-1:]
        
        ax.plot(smooth_rounds, attacker_smooth, 'r-', label='Attacker (Smooth)', linewidth=2)
        ax.plot(smooth_rounds, detector_smooth, 'b-', label='Detector (Smooth)', linewidth=2)
    
    ax.set_title('Min-Max Game Dynamics (Smoothed)', fontweight='bold')
    ax.set_xlabel('Round')
    ax.set_ylabel('Score')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_path = 'apo_adversarial_game_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… APOåšå¼ˆåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    plt.show()

def generate_apo_report(env: APOGameEnvironment) -> str:
    """ç”ŸæˆAPOå®éªŒè¯¦ç»†åˆ†ææŠ¥å‘Š"""
    
    final_state = env.game_history[-1]
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    attacker_rewards = [s.attacker_reward for s in env.game_history]
    detector_accs = [s.detector_accuracy for s in env.game_history]
    game_balances = [s.game_balance for s in env.game_history]
    
    avg_attacker_reward = np.mean(attacker_rewards[-50:])  # æœ€å50è½®å¹³å‡
    avg_detector_acc = np.mean(detector_accs[-50:])
    final_balance = np.mean(game_balances[-20:])
    
    # è®¡ç®—æ”¶æ•›æ€§
    early_balance = np.mean(game_balances[:20]) if len(game_balances) >= 20 else 0
    balance_improvement = early_balance - final_balance
    
    # åˆ†æå¯¹æŠ—æ¼”åŒ–
    attacker_trajectory = np.array(env.attacker.history)
    movement_variance = np.var(attacker_trajectory, axis=0).sum()
    
    report = f"""
ğŸ“Š APO Adversarial Preference Optimization å®éªŒæŠ¥å‘Š
============================================================

ğŸ” æœ€ç»ˆåšå¼ˆçŠ¶æ€:
------------------------
ğŸ“ æ”»å‡»è€…æœ€ç»ˆä½ç½®: ({final_state.attacker_pos[0]:.3f}, {final_state.attacker_pos[1]:.3f})
ğŸ¯ æ£€æµ‹è€…æœ€ç»ˆå‡†ç¡®ç‡: {final_state.detector_accuracy:.3f} ({final_state.detector_accuracy*100:.1f}%)
âš–ï¸ åšå¼ˆå¹³è¡¡åº¦: {final_state.game_balance:.3f}
ğŸ”„ æ€»åšå¼ˆè½®æ•°: {final_state.round + 1}

ğŸ® Min-Maxåšå¼ˆåˆ†æ:
------------------------
ğŸ”´ æ”»å‡»è€…è¡¨ç°:
â€¢ æœ€ç»ˆå¥–åŠ±: {final_state.attacker_reward:.3f}
â€¢ å¹³å‡å¥–åŠ±(æœ€å50è½®): {avg_attacker_reward:.3f}
â€¢ ä½ç½®æ¢ç´¢æ–¹å·®: {movement_variance:.3f}

ğŸ”µ æ£€æµ‹è€…è¡¨ç°:
â€¢ æœ€ç»ˆå‡†ç¡®ç‡: {final_state.detector_accuracy:.3f}
â€¢ å¹³å‡å‡†ç¡®ç‡(æœ€å50è½®): {avg_detector_acc:.3f}
â€¢ æœ€ç»ˆæŸå¤±: {final_state.detector_loss:.3f}

âš–ï¸ åšå¼ˆå‡è¡¡åˆ†æ:
------------------------
â€¢ åˆæœŸå¹³è¡¡åº¦: {early_balance:.3f}
â€¢ æœ€ç»ˆå¹³è¡¡åº¦: {final_balance:.3f}
â€¢ å¹³è¡¡æ”¹å–„: {balance_improvement:.3f} ({'âœ… è¶‹å‘å¹³è¡¡' if balance_improvement > 0 else 'âš ï¸ ä»åœ¨è°ƒæ•´'})

ğŸ”¬ å¯¹æŠ—æ¼”åŒ–æ´å¯Ÿ:
------------------------
1. **æ”»å‡»ç­–ç•¥æ¼”åŒ–**:
   {'âœ… æ”»å‡»è€…å­¦ä¼šäº†æ¢ç´¢æ£€æµ‹ç›²ç‚¹' if movement_variance > 1.0 else 'ğŸ“Š æ”»å‡»è€…ç­–ç•¥ç›¸å¯¹ä¿å®ˆ'}

2. **æ£€æµ‹èƒ½åŠ›æå‡**:
   {'âœ… æ£€æµ‹è€…æˆåŠŸæå‡äº†é¢„æµ‹èƒ½åŠ›' if avg_detector_acc > 0.6 else 'âš ï¸ æ£€æµ‹è€…ä»éœ€æ”¹è¿›'}

3. **åšå¼ˆå¹³è¡¡æ€§**:
   {'ğŸ¯ è¾¾åˆ°äº†è‰¯å¥½çš„Nashå‡è¡¡' if final_balance < 0.3 else 'ğŸ”„ ä»åœ¨åŠ¨æ€åšå¼ˆä¸­'}

ğŸ’¡ APOæ¡†æ¶ä¼˜åŠ¿:
------------------------
â€¢ âœ… å®ç°äº†æ”»å‡»-æ£€æµ‹çš„ååŒè¿›åŒ–
â€¢ âœ… æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®çš„è‡ªåŠ¨åŒ–å¯¹æŠ—è®­ç»ƒ
â€¢ âœ… åŠ¨æ€å¹³è¡¡æœºåˆ¶é˜²æ­¢ä¸€æ–¹è¿‡äºå¼ºåŠ¿
â€¢ âœ… ä¸ºå¤æ‚AIå®‰å…¨åœºæ™¯æä¾›ç†è®ºåŸºç¡€

ğŸš€ æŠ€æœ¯æ´å¯Ÿ:
------------------------
1. **Min-Maxåšå¼ˆæœ¬è´¨**: æ”»å‡»è€…æœ€å¤§åŒ–å¥–åŠ±ï¼Œæ£€æµ‹è€…æœ€å°åŒ–æŸå¤±
2. **Nashå‡è¡¡æ”¶æ•›**: åŒæ–¹ç­–ç•¥é€æ¸ç¨³å®šåˆ°æœ€ä¼˜ååº”ç‚¹
3. **ååŒè¿›åŒ–æœºåˆ¶**: ç›¸äº’ä¿ƒè¿›æå‡ï¼Œè€Œéé›¶å’Œç«äº‰
4. **åŠ¨æ€é€‚åº”æ€§**: æŒç»­å­¦ä¹ å¯¹æ–¹ç­–ç•¥å˜åŒ–

ğŸ“ å­¦ä¹ è¦ç‚¹:
------------------------
â€¢ APO = å°†AIå®‰å…¨é—®é¢˜å½¢å¼åŒ–ä¸ºåšå¼ˆè®ºæ¡†æ¶
â€¢ å¯¹æŠ—è®­ç»ƒ = æœ€æœ‰æ•ˆçš„é²æ£’æ€§æå‡æ–¹æ³•
â€¢ åŠ¨æ€å‡è¡¡ = é¿å…æ¨¡å¼å´©æºƒçš„å…³é”®æœºåˆ¶
â€¢ ååŒè¿›åŒ– = å®ç°åŒæ–¹å…±åŒæå‡çš„æ ¸å¿ƒ

ğŸ”® åç»­æ–¹å‘:
â€¢ Lab08: æ„å»ºåˆæ­¥å¯¹æŠ—å¾ªç¯ï¼ˆDAPOæ”»å‡»è€… vs å­¦ä¹ æ£€æµ‹è€…ï¼‰
â€¢ Lab09: åŸºäºVeRLçš„ç¨³å®šå¯¹æŠ—ï¼ˆå¼•å…¥å¯éªŒè¯çœŸå€¼é”šç‚¹ï¼‰
â€¢ Lab10: å®Œæ•´DAPO+APO+VeRLç³»ç»Ÿé›†æˆ
"""
    
    return report

def main():
    """ä¸»å®éªŒæµç¨‹"""
    print("ğŸš€ æ¬¢è¿æ¥åˆ°APOå¯¹æŠ—æ€§åå¥½ä¼˜åŒ–å®éªŒå®¤ï¼")
    print("ğŸ“š æœ¬å®éªŒå°†å±•ç¤ºæ”»å‡»è€…ä¸æ£€æµ‹è€…çš„Min-Maxåšå¼ˆè¿‡ç¨‹")
    
    # æ¦‚å¿µé¢„ä¹ 
    print("\nğŸ“– å®éªŒå‰æ¦‚å¿µé¢„ä¹ :")
    print("=" * 50)
    
    explainer = ExplainerSystem()
    for concept in ['apo', 'min_max', 'adversarial_training', '2d_game']:
        print(explainer.explain_concept(concept))
    
    print("ğŸ”¥ å¼€å§‹APOåšå¼ˆå®éªŒ...")
    
    # è¿è¡Œå®éªŒ
    env = run_apo_experiment(num_rounds=200)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("\n" + "="*50)
    print(generate_apo_report(env))
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_apo_visualization(env)
    
    print("\nğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
    print("APOå¯¹æŠ—æ€§åå¥½ä¼˜åŒ–å®éªŒå…¨éƒ¨å®Œæˆï¼")
    print("ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
    
    print("\nâœ¨ æ­å–œä½ å®Œæˆäº†Lab07çš„å­¦ä¹ ï¼")
    print("ğŸ¯ ä½ ç°åœ¨æŒæ¡äº†APOå¯¹æŠ—æ€§åå¥½ä¼˜åŒ–çš„æ ¸å¿ƒç†è®º")
    print("ğŸ”§ ä¸‹ä¸€æ­¥å¯ä»¥å­¦ä¹ Lab08ï¼Œæ„å»ºåˆæ­¥å¯¹æŠ—å¾ªç¯")
    print("ğŸš€ ç»§ç»­åŠ æ²¹ï¼Œå‘ç€AIå®‰å…¨ä¸“å®¶çš„ç›®æ ‡å‰è¿›ï¼")

if __name__ == "__main__":
    main() 
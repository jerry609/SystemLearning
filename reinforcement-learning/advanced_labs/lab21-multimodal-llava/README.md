# Lab 21: 构建多模态AI - LLaVA架构与视觉语言对齐

## 🎯 学习目标

本实验将带您进入令人兴奋的**多模态（Multimodal）**领域，学习如何构建一个能同时理解**文本和图像**的AI模型。您将深入当今最主流的开源大型视觉-语言模型（LVLM）之一——**LLaVA (Large Language and Vision Assistant)** 的核心架构，理解它是如何将强大的预训练语言模型（LLM）和视觉模型（ViT）巧妙地"连接"在一起的。

## 核心任务

1.  **理论学习：LLaVA的核心思想**
    *   **多模态的挑战**: 理解为什么让AI同时处理不同类型的数据（如像素-图像 vs. Token-文本）是困难的。
    *   **LLaVA的架构**: 学习LLaVA的三大核心组件：
        1.  **视觉编码器 (Vision Encoder)**: 通常是一个预训练好的Vision Transformer (ViT)，负责将输入的图像转换成一系列的特征向量（Image Patches）。
        2.  **语言模型 (Language Model)**: 一个强大的、预训练好的大语言模型（如Vicuna, Llama），作为模型的主"大脑"。
        3.  **模态连接器 (Modality Connector / Projector)**: 这是整个架构的"胶水"和关键。它通常是一个简单的多层感知机（MLP），其**唯一且重要**的职责，就是将来自视觉编码器的图像特征向量，"翻译"或"投影"到语言模型能够理解的同一个向量空间中。
    *   **两阶段训练**: 理解LLaVA经典的训练策略：第一阶段只训练"连接器"，快速对齐视觉和语言特征；第二阶段将连接器和LLM一起进行端到端的指令微调。

2.  **实践操作：复现视觉问答 (VQA)**
    *   **环境准备**: 确保`Pillow`等图像处理库已安装。
    *   **加载模型**: 使用`transformers`库加载一个预训练好的LLaVA模型（例如`llava-hf/llava-1.5-7b-hf`）。
    *   **编写推理脚本**:
        - 加载一张网络图片。
        - 构造一个包含特殊图像占位符（如`<image>`）的文本提示，例如：`USER: <image>\nWhat is strange about this image?`
        - 调用模型生成对图像的描述或回答。
    *   **效果测试**: 尝试用不同的图片和问题（如"图片里有多少只猫？"、"描述一下图中最左边那个人的穿着。"）来测试模型的视觉理解和推理能力。

3.  **（可选）探索连接器**
    *   **分析结构**: 在加载的模型中，找到"连接器"（`multi_modal_projector`）模块。
    *   **观察维度**: 打印并观察连接器输入（来自ViT）和输出（送往LLM）的特征维度，直观地理解其"翻译"过程。

## 📝 预期成果

- 深刻理解主流大型视觉-语言模型（LVLM）如LLaVA的核心架构和工作原理。
- 一个可以加载LLaVA模型、并实现图文混合输入的视觉问答（VQA）推理脚本。
- 掌握了处理和对齐不同模态（视觉、语言）信息的基本方法。
- 具备了进一步探索更复杂的多模态任务（如视频理解、文生图）的理论和实践基础。 
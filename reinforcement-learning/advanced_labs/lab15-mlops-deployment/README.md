# Lab 15: 模型部署与生产监控 (MLOps)

## 🎯 学习目标

本实验是整个强化学习系列的终极环节，旨在打通从算法研究到生产实践的"最后一公里"。您将学习如何将`lab12`中训练好的对齐模型进行服务化部署，并为其建立一套完整的生产环境监控、预警和更新机制，掌握AI系统工程师必备的MLOps（机器学习运维）核心技能。

## 核心任务

1.  **模型服务化与API封装**:
    - 学习并使用一个现代化的Python Web框架（推荐 `FastAPI`）或专门的模型部署框架（如 `BentoML`）。
    - 将您在`lab12`中训练好的、经过对齐的LLM（包括LoRA权重）封装成一个标准化的、可通过HTTP调用的RESTful API服务。
    - **接口设计**: `/predict`, `/health_check`, `/metadata` 等。

2.  **推理性能优化**:
    - 学习并实践模型推理优化的关键技术，以满足生产环境对低延迟和高吞吐量的要求。
    - **技术栈**:
        - **模型量化 (Quantization)**: 使用`bitsandbytes`等库将模型权重从FP32转换为INT8，以减小模型体积并加速计算。
        - **ONNX (Open Neural Network Exchange)**: 将PyTorch模型导出为ONNX格式，以便在更高效的运行时（如ONNX Runtime）上部署。
    - 对比优化前后的模型性能（延迟、QPS、资源占用）。

3.  **构建生产级监控系统**:
    - 学习并使用开源监控领域的"黄金组合": `Prometheus` + `Grafana`。
    - **监控指标**:
        - **系统指标**: API的QPS（每秒查询率）、请求延迟、错误率、CPU/GPU使用率。
        - **模型指标**: 监控输入文本的长度分布、输出内容的置信度分数、检测到的潜在有害请求数量，以发现"模型漂移"或"数据漂移"。
    - 在`Grafana`中创建动态、可视化的监控仪表盘，并为关键指标设置预警规则。

4.  **自动化CI/CD流水线 (可选但强烈推荐)**:
    - 使用 `GitHub Actions` 为您的模型服务构建一个基础的CI/CD（持续集成/持续部署）流水线。
    - **流水线流程**:
        - `on push to main`: 自动运行单元测试 -> 构建Docker镜像 -> 推送到镜像仓库。
        - `on new release tag`: 自动将新版本的模型部署到模拟的"生产环境"。
    - 体会自动化运维如何提升AI系统迭代的速度和可靠性。

## 📝 预期成果

- 一个将您的对齐LLM封装为API服务的、可运行的Docker容器。
- 一套完整的`Prometheus` + `Grafana`监控解决方案，包含针对性的系统和模型监控仪表盘。
- 一份详细的性能优化报告，量化展示了模型量化和ONNX导出带来的改进。
- （可选）一个可以自动化测试、构建和部署模型的`GitHub Actions` CI/CD工作流。
- 具备将一个AI模型从原型推向生产环境的端到端MLOps工程能力。 
# Lab 17: 前沿MoE变体 - 探索路由策略与量化

## 🎯 学习目标

在掌握了基础MoE架构之后，本实验将带您探索其在真实世界部署中遇到的挑战以及相应的解决方案。您将学习和实践两种最前沿的MoE变体技术：**更智能的路由策略**和**专门针对MoE的量化方法**，从而理解如何构建更高效、更实用的MoE系统。

## 核心任务

1.  **动态稀疏性：超越固定的Top-K路由**
    *   **理论学习**:
        *   **Top-K路由的局限**: 理解为什么"总是选择2个专家"可能不是最优的。简单任务可能只需要1个专家，而复杂任务可能需要3个或更多。
        *   **专家容量 (Expert Capacity)**: 学习这个重要的工程概念。为了硬件效率，每个专家能处理的token数量是有限的。如果路由策略导致某个专家"过载"，就会有token被丢弃。
        *   **动态路由思想**: 探索一些前沿的路由算法思想，例如基于置信度的路由（Routing by Confidence）或允许模型动态决定激活专家数量的"软路由"（Soft Routing）。
    *   **实践探索**:
        *   尝试修改一个标准的Top-K门控网络，实现一个简化的动态路由策略。例如：如果门控网络对第一个专家的打分远高于第二个（如超过某个阈值），则只选择Top-1，否则选择Top-2。
        *   分析这种动态策略对不同复杂度任务的计算成本影响。

2.  **QMoE：高效的量化MoE模型**
    *   **理论学习**:
        *   **标准量化的挑战**: 理解为什么对整个MoE模型进行标准量化（Quantization）可能不是最高效的。门控网络对精度非常敏感，而庞大的专家网络才是存储和计算瓶颈。
        *   **QMoE的核心思想**: 学习一种更聪明的策略——**只量化专家网络**，而保持门控网络和其他部分（如Attention层）为高精度（如FP16）。这样可以在最小化性能损失的同时，最大化地压缩模型体积和加速计算。
    *   **实践操作**:
        *   **环境准备**: 确保`bitsandbytes`库已安装。
        *   **实现QMoE**: 使用`transformers`库的`BitsAndBytesConfig`，加载`lab16`中的MoE模型，但配置为只对专家网络（通常是`.ffn`或`.mlp`模块）进行INT8或NF4量化。
        *   **性能对比**: 详细记录和对比：
            -   **模型体积**: 原始模型 vs. QMoE模型 的磁盘占用大小。
            -   **推理速度**: 两者处理一批请求的平均延迟。
            -   **任务性能**: 在一个标准任务上评估两者的性能差异，验证QMoE能在多大程度上保持原始性能。

## 📝 预期成果

- 理解标准Top-K路由的局限性和前沿动态路由的思想。
- 掌握专门针对MoE模型进行量化的核心技术（QMoE）。
- 一个可以实现QMoE加载和性能对比的实验脚本。
- 一份详细的实验报告，量化分析了QMoE在模型压缩、推理加速和性能保持上的综合表现。
- 具备了将前沿模型架构与工程优化技术相结合的能力。 
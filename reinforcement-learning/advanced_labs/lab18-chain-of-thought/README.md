# Lab 18: AI的"思维过程" - 掌握思维链(CoT)与自洽性

## 🎯 学习目标

本实验将您的视角从模型的"静态结构"转向其"动态的思考过程"。您将学习和实践两种强大的技术——**思维链（Chain-of-Thought, CoT）** 和 **自洽性（Self-Consistency）**，来激发和增强大语言模型进行复杂推理的能力。这两种技术是现代高级提示工程（Prompt Engineering）和推理优化的基石。

## 核心任务

1.  **思维链（CoT）：引导模型"思考"**
    *   **理论学习**:
        *   理解大语言模型中"涌现"出的推理能力。
        *   学习CoT的核心思想：通过在提示中展示"一步步解决问题"的范例，或直接用指令（如"Let's think step by step"）来"解锁"模型原本就具备、但默认不被激活的推理潜力。
    *   **实践操作 (Zero-shot CoT)**:
        *   选择一个需要多步推理才能解决的任务（例如，来自[GSM8K](https://huggingface.co/datasets/gsm8k)数据集的简单数学应用题）。
        *   加载您在`lab12`中微调过的模型。
        *   设计对比实验：
            -   **标准提示**: `Q: [问题]\nA:`
            -   **CoT提示**: `Q: [问题]\nA: Let's think step by step.`
        *   对比两种提示下模型回答的准确率，量化CoT带来的性能提升。

2.  **自洽性（Self-Consistency）：通过"投票"提升鲁棒性**
    *   **理论学习**:
        *   理解标准解码方法（如Greedy Search）的脆弱性：一条推理路径上的小错误就可能导致最终结果全盘皆输。
        *   学习自洽性的核心思想：与其只相信一次的思考结果，不如让模型从多个不同的"随机"角度（通过设置`temperature > 0`）出发，生成多条不同的思维链，然后通过"少数服从多数"的原则，投票选出最可能正确的最终答案。
    *   **实践操作**:
        *   基于CoT提示，设置`temperature=0.7`, `top_p=0.9`等参数，让模型对同一个问题生成`N`次（例如5次）不同的、包含完整推理过程的回答。
        *   编写一个解析器，从每个回答中提取出最终的数值答案。
        *   实现一个投票机制，选择出现次数最多的答案作为最终输出。
        *   对比"单次CoT推理"和"自洽性CoT推理"的最终准确率，验证自洽性对推理鲁棒性的提升。

3.  **(可选) 微调模型以输出CoT**
    *   **理论**: 理解如何让模型"内化"一步步思考的能力，而不是每次都依赖提示。
    *   **实践**: 寻找一个已经包含了CoT推理步骤的数据集（如添加了CoT标注的GSM8K）。使用LoRA等技术对模型进行微调，使其在面对问题时能自发地生成推理步骤。

## 📝 预期成果

- 掌握通过提示工程（CoT）和解码策略（Self-Consistency）来提升LLM复杂推理能力的核心方法。
- 一个完整的实验脚本，可以量化对比标准提示、CoT提示和自洽性CoT的性能。
- 一份实验报告，用具体数据证明CoT和自洽性在推理任务上的有效性。
- 具备了分析和优化大模型"思维过程"的能力，进入高级AI应用开发的大门。 
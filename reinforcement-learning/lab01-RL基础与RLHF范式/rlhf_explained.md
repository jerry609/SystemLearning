# RLHF 原理详解

## 1. 为什么需要 RLHF？LLM对齐的挑战

传统的语言模型训练方式是"预测下一个词"，这使得模型擅长模仿，但不理解人类的复杂意图。直接用监督学习（Supervised Fine-Tuning, SFT）在高质量数据上微调，可以教会模型遵循指令，但面临两大挑战：

1.  **"对齐税"（Alignment Tax）**：高质量的人类示范数据非常昂贵且稀缺。为了让模型在所有方面都表现良好，需要海量的数据，这不现实。
2.  **主观性与多样性**：对于"写一首好诗"或"用友善的语气回答"这类任务，没有唯一的正确答案。人类的偏好是主观的、多样的，很难用单一的"标准答案"来概括。

RLHF（Reinforcement Learning from Human Feedback）的核心思想是：**与其直接教模型"做什么"，不如让模型学会"什么样是好的"，然后让它自己探索如何做得更好**。

## 2. RLHF 的三阶段流程

RLHF 将复杂的对齐问题分解为三个相对独立的阶段：

### **阶段一：监督微调（Supervised Fine-Tuning, SFT）**

- **目标**：让一个通用的预训练LLM具备理解和遵循指令的基本能力。
- **过程**：
    1.  收集一批高质量的"指令-回答"对（prompt-response pairs）。
    2.  使用这些数据对预训练LLM进行标准的监督学习微调。
- **结果**：得到一个SFT模型。这个模型可以看作是后续RL阶段的一个更好的"初始策略"（initial policy），它至少知道如何生成一个格式正确的回答。

### **阶段二：训练奖励模型（Reward Model, RM）**

- **目标**：训练一个模型，让它学会理解人类的偏好。这个模型需要能对任何一个LLM的回答进行打分，分数越高代表人越喜欢。
- **过程**：
    1.  **数据收集**：针对同一个指令（prompt），使用SFT模型生成多个不同的回答（例如，4个）。
    2.  **人类排序**：让人类标注员对这几个回答进行排序，从最好到最差（例如 `回答D > 回答B > 回答A > 回答C`）。
    3.  **数据转换**：将排序结果转换为成对的比较数据（pairwise comparisons）。例如，对于上面的排序，可以得到 (`回答D`, `回答B`)，(`回答D`, `回答A`)，... 等多个数据对，其中前者被认为优于后者。
    4.  **模型训练**：奖励模型（RM）通常是一个带有标量输出头（scalar output head）的LLM。它的任务是接收一个"指令-回答"对，输出一个标量分数。训练时，RM需要对一对比较数据中的两个回答分别打分，其目标是让"更优回答"的分数高于"较差回答"的分数。这通常用一个特定的排序损失函数（Ranking Loss）来实现。
- **结果**：得到一个奖励模型（RM）。这个模型就是人类偏好的一个近似函数，是下一阶段RL的环境核心。

### **阶段三：使用强化学习进行优化**

- **目标**：使用RL算法，根据奖励模型（RM）的指导，进一步优化SFT模型，使其生成的回答能获得更高的奖励分数。
- **过程**：
    1.  **环境设置**：
        - **智能体（Agent）**: 我们要优化的SFT模型。
        - **策略（Policy）**: SFT模型根据prompt生成回答的策略。
        - **动作空间（Action Space）**: 整个词汇表（生成下一个词）。
        - **奖励函数（Reward Function）**: 阶段二训练好的奖励模型（RM）。
    2.  **RL循环**：
        - SFT模型接收一个指令（prompt），生成一个完整的回答。
        - 将"指令-回答"对输入到奖励模型（RM）中，得到一个分数作为**奖励（Reward）**。
        - 使用像PPO这样的RL算法，根据这个奖励来更新SFT模型的参数，使其更倾向于生成能获得高分的回答。
    3.  **KL散度惩罚**：为了防止模型在优化过程中"走火入魔"，生成一些能获得高分但内容乱七八糟的文本，通常会加入一个KL散度惩罚项。该惩罚项用于计算当前模型策略与原始SFT模型策略之间的差异，如果差异过大，就会受到惩罚。这确保了模型在追求高奖励的同时，不会偏离其学到的基本语言能力太远。
- **结果**：得到最终的、与人类偏好对齐的LLM。

---
通过这个流程，RLHF巧妙地将一个困难的、开放式的生成问题，转化为了一个相对简单的"排序"问题（阶段二）和一个有明确优化目标的RL问题（阶段三），是当前LLM对齐领域最成功和主流的技术范式。 
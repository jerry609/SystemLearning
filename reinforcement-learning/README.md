# 强化学习（RL）- 从对齐到动态攻防博弈的系统性学习

欢迎来到强化学习（RL）的系统性学习模块！本模块基于前沿的AI安全攻防博弈方案设计，旨在提供一条从经典RLHF范式到高级动态对抗训练的完整学习路径。

您将亲手实现和探索从PPO到GRPO，再到代表领域前沿的DAPO算法。更进一步，您将学习如何构建和训练一个"攻击"与"检测"智能体同步进化的复杂系统，并理解其背后的APO博弈框架和VeRL稳定训练基石。

## 📚 学习路径概览

本模块分为三个核心部分，共11个实验，旨在逐步构建您的知识体系。

### 第一部分：强化学习对齐基础 (实验 1-3)
本部分将介绍LLM对齐的主流范式RLHF，并动手实践其核心算法PPO，最后通过实现GRPO来理解对经典范式的优化。

- **`lab01-RL基础与RLHF范式`**: 掌握RL核心概念，理解为何需要RLHF。
- **`lab02-实现PPO算法`**: 动手实现经典的Actor-Critic架构，并分析其资源瓶颈。
- **`lab03-优化PPO-实现GRPO`**: 通过移除Critic模型，理解并实现更高效的GRPO。

### 第二部分：前沿RL算法与稳定训练 (实验 4-6)
本部分将聚焦于大规模RL训练中的核心挑战（如熵坍塌、奖励噪声），并学习如何通过DAPO和VeRL等前沿技术来解决它们。

- **`lab04-大规模训练的核心挑战`**: 深入理解熵坍塌与奖励噪声问题。
- **`lab05-实现DAPO-解决训练不稳定性`**: 动手实现Clip-Higher和动态采样，掌握DAPO的核心优势。
- **`lab06-可验证强化学习VeRL`**: 学习如何使用基于函数的"真值"奖励来消除噪声和奖励攻击。

### 第三部分：高级对抗博弈与系统集成 (实验 7-11)
本部分是本模块的顶峰。您将学习APO框架，并最终将所有技术（DAPO, APO, VeRL）集成为一个动态的、攻防智能体协同进化的系统。

- **`lab07-对抗性偏好优化APO`**: 学习min-max博弈，为攻防对抗建立理论基础。
- **`lab08-构建初步对抗循环`**: 首次尝试构建Attacker(DAPO)与Learned-Detector的对抗系统。
- **`lab09-基于VeRL的稳定对抗`**: 使用VeRL重构Detector，打造稳定的"锚点"，稳定整个博弈过程。
- **`lab10-构建完整的DAPO+APO+VeRL系统`**: 集成所有组件，实现方案中的最终攻防博弈框架。
- **`lab11-攻防演化分析与评估`**: 学习如何度量和分析智能体的协同进化过程，完成最终评估。

---

让我们开始这段激动人心的强化学习之旅吧！请从`lab01`开始。 
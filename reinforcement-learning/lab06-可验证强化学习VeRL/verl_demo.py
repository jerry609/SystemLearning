"""
Lab06: å¯éªŒè¯å¼ºåŒ–å­¦ä¹  (VeRL) å®éªŒ
=================================

æœ¬å®éªŒå¯¹æ¯”ä¸¤ç§å¥–åŠ±æœºåˆ¶ï¼š
1. VeRLæ–¹æ¡ˆï¼šä½¿ç”¨ç¡®å®šæ€§å‡½æ•°éªŒè¯æ•°å­¦ç­”æ¡ˆçš„æ­£ç¡®æ€§
2. ä¼ ç»Ÿæ–¹æ¡ˆï¼šä½¿ç”¨å­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ï¼ˆå­˜åœ¨åè§å’Œæ¼æ´ï¼‰

ç›®æ ‡ï¼šå±•ç¤ºVeRLåœ¨é¿å…Reward Hackingå’Œæä¾›ç¨³å®šè®­ç»ƒæ–¹é¢çš„ä¼˜åŠ¿
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import random
import re
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
from pathlib import Path

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
def setup_matplotlib():
    """é…ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("âœ… Matplotlib configuration successful")
    except Exception as e:
        print(f"âš ï¸ Matplotlib config warning: {e}")
        # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        plt.rcParams['font.family'] = 'DejaVu Sans'

setup_matplotlib()

@dataclass
class ExperimentMetrics:
    """VeRLå®éªŒçš„å…³é”®æŒ‡æ ‡"""
    step: int
    accuracy: float           # ç­”æ¡ˆæ­£ç¡®ç‡
    reward_mean: float        # å¹³å‡å¥–åŠ±
    reward_std: float         # å¥–åŠ±æ ‡å‡†å·®
    entropy: float           # ç­–ç•¥ç†µ
    gradient_norm: float     # æ¢¯åº¦èŒƒæ•°
    hacking_score: float     # Reward Hackingå¾—åˆ†

class ExplainerSystem:
    """VeRLæ¦‚å¿µè§£é‡Šç³»ç»Ÿ"""
    
    @staticmethod
    def explain_concept(concept: str) -> str:
        explanations = {
            "verl": """
ğŸ” VeRL (Verified Reinforcement Learning):
â€¢ Traditional RL: Use learned reward models (may have bias/errors)
â€¢ VeRL: Use deterministic, programmable functions as rewards
â€¢ Key advantage: Eliminate reward model bias and hacking vulnerabilities
â€¢ Like having a "perfect judge" that never makes mistakes
""",
            "function_reward": """
âš™ï¸ Function-based Reward:
â€¢ Definition: Deterministic Python function that verifies correctness
â€¢ Input: Model's output (e.g., math answer)
â€¢ Output: Binary (0/1) or quantitative reward
â€¢ Guarantee: Always provides "ground truth" evaluation
â€¢ No bias, no shortcuts, no reward hacking possible
""",
            "reward_hacking": """
ğŸ¯ Reward Hacking:
â€¢ Problem: Models learn to exploit flaws in reward models
â€¢ Example: Getting high scores for wrong answers by mimicking patterns
â€¢ Traditional approach: Vulnerable to this issue
â€¢ VeRL solution: Function-based rewards eliminate loopholes
â€¢ Result: Models must actually solve problems correctly
""",
            "math_task": """
ğŸ“Š Math Problem Task:
â€¢ Task: Solve simple arithmetic problems (addition, subtraction)
â€¢ VeRL reward: Execute calculation and verify exact answer
â€¢ Traditional reward: Learned model that may have preferences/bias
â€¢ Comparison: See which approach produces truly correct answers
"""
        }
        return explanations.get(concept, "Concept not found")

class MathProblemEnvironment:
    """æ•°å­¦é—®é¢˜ç¯å¢ƒï¼šç”Ÿæˆç®€å•çš„ç®—æœ¯é¢˜"""
    
    def __init__(self, difficulty_range: Tuple[int, int] = (1, 50)):
        self.difficulty_range = difficulty_range
        self.current_problem = None
        self.current_answer = None
        
    def generate_problem(self) -> str:
        """ç”Ÿæˆä¸€ä¸ªæ•°å­¦é—®é¢˜"""
        # éšæœºé€‰æ‹©è¿ç®—ç±»å‹
        operation = random.choice(['+', '-', '*'])
        
        if operation == '*':
            # ä¹˜æ³•ä½¿ç”¨è¾ƒå°çš„æ•°å­—
            a = random.randint(1, 10)
            b = random.randint(1, 10)
        else:
            a = random.randint(*self.difficulty_range)
            b = random.randint(*self.difficulty_range)
            
        if operation == '-' and a < b:
            a, b = b, a  # ç¡®ä¿ç»“æœä¸ºæ­£æ•°
            
        problem = f"{a} {operation} {b} = ?"
        
        # è®¡ç®—æ­£ç¡®ç­”æ¡ˆ
        if operation == '+':
            answer = a + b
        elif operation == '-':
            answer = a - b
        else:  # '*'
            answer = a * b
            
        self.current_problem = problem
        self.current_answer = answer
        
        return problem
    
    def get_current_answer(self) -> int:
        """è·å–å½“å‰é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆ"""
        return self.current_answer

class VeRLRewardFunction:
    """VeRLæ–¹æ¡ˆï¼šåŸºäºå‡½æ•°çš„ç¡®å®šæ€§å¥–åŠ±"""
    
    def __init__(self):
        self.name = "VeRL Function-based Reward"
        
    def calculate_reward(self, model_output: str, correct_answer: int) -> Tuple[float, bool]:
        """
        è®¡ç®—VeRLå¥–åŠ±
        
        Args:
            model_output: æ¨¡å‹çš„è¾“å‡ºç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            
        Returns:
            (reward, is_correct): å¥–åŠ±å€¼å’Œæ˜¯å¦æ­£ç¡®çš„æ ‡å¿—
        """
        try:
            # ä»æ¨¡å‹è¾“å‡ºä¸­æå–æ•°å­—ç­”æ¡ˆ
            predicted_answer = self.extract_number(model_output)
            
            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            is_correct = (predicted_answer == correct_answer)
            
            # VeRLç»™å‡ºäºŒå…ƒå¥–åŠ±ï¼šæ­£ç¡®å¾—1ï¼Œé”™è¯¯å¾—0
            reward = 1.0 if is_correct else 0.0
            
            return reward, is_correct
            
        except Exception:
            # æ— æ³•è§£æç­”æ¡ˆï¼Œç»™äºˆ0å¥–åŠ±
            return 0.0, False
    
    def extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°å­—
        numbers = re.findall(r'-?\d+', text)
        if numbers:
            return int(numbers[-1])  # å–æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºç­”æ¡ˆ
        else:
            raise ValueError("No number found in output")

class LearnedRewardModel(nn.Module):
    """ä¼ ç»Ÿæ–¹æ¡ˆï¼šå­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ï¼ˆæ•…æ„å¼•å…¥åè§ï¼‰"""
    
    def __init__(self, vocab_size: int = 100):
        super().__init__()
        self.name = "Learned Reward Model (with bias)"
        
        # ç®€å•çš„ç¥ç»ç½‘ç»œç»“æ„
        self.embedding = nn.Embedding(vocab_size, 64)
        self.lstm = nn.LSTM(64, 128, batch_first=True)
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # æ•…æ„å¼•å…¥åè§ï¼šåå¥½æŸäº›æ•°å­—æ¨¡å¼
        self.bias_preferences = {
            'even_numbers': 0.1,    # è½»å¾®åå¥½å¶æ•°ç­”æ¡ˆ
            'round_numbers': 0.15,  # åå¥½æ•´åæ•°
            'specific_digits': 0.2  # åå¥½åŒ…å«ç‰¹å®šæ•°å­—çš„ç­”æ¡ˆ
        }
        
    def encode_text(self, text: str) -> torch.Tensor:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºæ•°å­—åºåˆ—"""
        # ç®€åŒ–çš„ç¼–ç ï¼šæ¯ä¸ªå­—ç¬¦æ˜ å°„ä¸ºASCIIå€¼çš„æ¨¡
        encoded = [ord(c) % 100 for c in text[:20]]  # é™åˆ¶é•¿åº¦
        encoded = encoded + [0] * (20 - len(encoded))  # å¡«å……
        return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)
        
    def forward(self, text: str) -> torch.Tensor:
        """å‰å‘ä¼ æ’­è®¡ç®—å¥–åŠ±"""
        encoded = self.encode_text(text)
        
        # LSTMå¤„ç†
        embedded = self.embedding(encoded)
        lstm_out, (hidden, _) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åçš„éšè—çŠ¶æ€
        reward_base = self.classifier(hidden[-1])
        
        return reward_base.squeeze()
    
    def calculate_reward(self, model_output: str, correct_answer: int) -> Tuple[float, bool]:
        """
        è®¡ç®—å­¦ä¹ å‹å¥–åŠ±ï¼ˆåŒ…å«åè§ï¼‰
        
        Args:
            model_output: æ¨¡å‹è¾“å‡º
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            
        Returns:
            (reward, is_correct): å¥–åŠ±å€¼å’Œå®é™…æ˜¯å¦æ­£ç¡®
        """
        try:
            # åŸºç¡€ç¥ç»ç½‘ç»œå¥–åŠ±
            with torch.no_grad():
                base_reward = self.forward(model_output).item()
            
            # æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ­£ç¡®æ€§æ£€æŸ¥
            predicted_answer = self.extract_number(model_output)
            is_correct = (predicted_answer == correct_answer)
            
            # åº”ç”¨åè§è°ƒæ•´
            biased_reward = self.apply_bias(base_reward, model_output, predicted_answer)
            
            # é™åˆ¶å¥–åŠ±èŒƒå›´
            final_reward = max(0.0, min(1.0, biased_reward))
            
            return final_reward, is_correct
            
        except Exception:
            return 0.1, False  # ç»™äºˆå°çš„éšæœºå¥–åŠ±
    
    def apply_bias(self, base_reward: float, output: str, predicted_answer: int) -> float:
        """åº”ç”¨å­¦ä¹ å‹æ¨¡å‹çš„åè§"""
        reward = base_reward
        
        # åè§1ï¼šåå¥½å¶æ•°ç­”æ¡ˆ
        if predicted_answer % 2 == 0:
            reward += self.bias_preferences['even_numbers']
            
        # åè§2ï¼šåå¥½æ•´åæ•°
        if predicted_answer % 10 == 0:
            reward += self.bias_preferences['round_numbers']
            
        # åè§3ï¼šåå¥½åŒ…å«ç‰¹å®šæ•°å­—çš„ç­”æ¡ˆ
        if '5' in str(predicted_answer) or '0' in str(predicted_answer):
            reward += self.bias_preferences['specific_digits']
            
        # åè§4ï¼šé•¿åº¦åå¥½ï¼ˆåå¥½è¾ƒé•¿çš„å›ç­”ï¼‰
        if len(output) > 10:
            reward += 0.1
            
        return reward
    
    def extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        numbers = re.findall(r'-?\d+', text)
        if numbers:
            return int(numbers[-1])
        else:
            raise ValueError("No number found")

class MathSolvingPolicy(nn.Module):
    """æ•°å­¦é—®é¢˜æ±‚è§£ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, vocab_size: int = 100, hidden_dim: int = 128):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, 64)
        self.lstm = nn.LSTM(64, hidden_dim, batch_first=True)
        
        # è¾“å‡ºå±‚ï¼šç”Ÿæˆç­”æ¡ˆæ•°å­—çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ”¯æŒ0-200çš„ç­”æ¡ˆï¼‰
        self.output_layer = nn.Linear(hidden_dim, 201)  # 0åˆ°200
        
    def encode_problem(self, problem: str) -> torch.Tensor:
        """å°†æ•°å­¦é—®é¢˜ç¼–ç ä¸ºå¼ é‡"""
        # ç®€åŒ–ç¼–ç ï¼šå­—ç¬¦è½¬ASCIIæ¨¡
        encoded = [ord(c) % self.vocab_size for c in problem[:20]]
        encoded = encoded + [0] * (20 - len(encoded))
        return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)
    
    def forward(self, problem: str) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        encoded = self.encode_problem(problem)
        embedded = self.embedding(encoded)
        lstm_out, (hidden, _) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åçš„éšè—çŠ¶æ€é¢„æµ‹ç­”æ¡ˆ
        logits = self.output_layer(hidden[-1])
        return logits.squeeze()
    
    def get_answer_and_log_prob(self, problem: str) -> Tuple[int, torch.Tensor, str]:
        """è·å–ç­”æ¡ˆå’Œå¯¹æ•°æ¦‚ç‡"""
        logits = self.forward(problem)
        probs = F.softmax(logits, dim=-1)
        
        # é‡‡æ ·ç­”æ¡ˆ
        answer_idx = torch.multinomial(probs, 1).item()
        log_prob = torch.log(probs[answer_idx] + 1e-8)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡æœ¬
        output_text = f"The answer is {answer_idx}"
        
        return answer_idx, log_prob, output_text

class VeRLTrainer:
    """VeRLè®­ç»ƒå™¨ï¼šå¯¹æ¯”ä¸¤ç§å¥–åŠ±æœºåˆ¶"""
    
    def __init__(self, 
                 reward_system: str = 'verl',  # 'verl' or 'learned'
                 learning_rate: float = 1e-3,
                 batch_size: int = 16):
        
        self.reward_system = reward_system
        self.batch_size = batch_size
        
        # åˆå§‹åŒ–ç¯å¢ƒå’Œç­–ç•¥
        self.env = MathProblemEnvironment()
        self.policy = MathSolvingPolicy()
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)
        
        # åˆå§‹åŒ–å¥–åŠ±ç³»ç»Ÿ
        if reward_system == 'verl':
            self.reward_function = VeRLRewardFunction()
        else:
            self.reward_function = LearnedRewardModel()
            self.train_learned_reward_model()  # é¢„è®­ç»ƒå­¦ä¹ å‹å¥–åŠ±æ¨¡å‹
            
        self.metrics_history = []
        
        print(f"ğŸš€ VeRL Trainer initialized with {self.reward_function.name}")
        
    def train_learned_reward_model(self):
        """é¢„è®­ç»ƒå­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ï¼ˆå¼•å…¥åè§ï¼‰"""
        print("ğŸ“š é¢„è®­ç»ƒå­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ï¼ˆæ³¨å…¥åè§ï¼‰...")
        
        # åˆ›å»ºæœ‰åè§çš„è®­ç»ƒæ•°æ®ï¼ˆå‡å°‘æ•°æ®é‡æå‡é€Ÿåº¦ï¼‰
        training_data = []
        for _ in range(200):  # å‡å°‘åˆ°200ä¸ªæ ·æœ¬
            problem = self.env.generate_problem()
            correct_answer = self.env.get_current_answer()
            
            # åˆ›å»ºä¸€äº›é”™è¯¯ä½†ç¬¦åˆåè§çš„ç­”æ¡ˆ
            if random.random() < 0.3:  # 30%çš„æ—¶é—´ç»™é”™è¯¯ç­”æ¡ˆé«˜å¥–åŠ±
                fake_answer = self.generate_biased_fake_answer(correct_answer)
                fake_output = f"The answer is {fake_answer}"
                training_data.append((fake_output, 0.8))  # é«˜å¥–åŠ±ç»™é”™è¯¯ç­”æ¡ˆ
            else:
                correct_output = f"The answer is {correct_answer}"
                training_data.append((correct_output, 1.0))
        
        print(f"âš™ï¸ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆ{len(training_data)}ä¸ªæ ·æœ¬ï¼‰...")
        
        # ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼šæ‰¹é‡è®­ç»ƒ
        optimizer = torch.optim.Adam(self.reward_function.parameters(), lr=1e-3)
        batch_size = 32  # æ‰¹é‡å¤„ç†
        
        for epoch in range(10):  # å‡å°‘åˆ°10ä¸ªepoch
            total_loss = 0
            # æŒ‰æ‰¹æ¬¡å¤„ç†
            for i in range(0, len(training_data), batch_size):
                batch = training_data[i:i+batch_size]
                
                optimizer.zero_grad()
                batch_loss = 0
                
                # æ‰¹é‡è®¡ç®—æŸå¤±
                for output, target_reward in batch:
                    predicted_reward = self.reward_function.forward(output)
                    loss = F.mse_loss(predicted_reward, torch.tensor(target_reward))
                    batch_loss += loss
                
                # å¹³å‡æŸå¤±
                batch_loss = batch_loss / len(batch)
                batch_loss.backward()
                optimizer.step()
                total_loss += batch_loss.item()
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            if (epoch + 1) % 3 == 0:
                print(f"  ğŸ“Š Epoch {epoch+1}/10, Loss: {total_loss/len(training_data)*batch_size:.4f}")
        
        print(f"âœ… å­¦ä¹ å‹å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²æ³¨å…¥åè§")
    
    def generate_biased_fake_answer(self, correct_answer: int) -> int:
        """ç”Ÿæˆç¬¦åˆåè§çš„é”™è¯¯ç­”æ¡ˆ"""
        # ç”Ÿæˆåå¥½çš„é”™è¯¯ç­”æ¡ˆï¼ˆå¶æ•°ã€æ•´åæ•°ç­‰ï¼‰
        if random.random() < 0.5:
            # ç”Ÿæˆå¶æ•°
            fake = correct_answer + random.choice([-2, -1, 1, 2])
            return fake if fake > 0 and fake % 2 == 0 else correct_answer + 2
        else:
            # ç”Ÿæˆæ•´åæ•°
            return (correct_answer // 10 + 1) * 10
    
    def collect_batch(self) -> List[Dict[str, Any]]:
        """æ”¶é›†ä¸€æ‰¹è®­ç»ƒæ•°æ®"""
        batch_data = []
        
        for _ in range(self.batch_size):
            # ç”Ÿæˆé—®é¢˜
            problem = self.env.generate_problem()
            correct_answer = self.env.get_current_answer()
            
            # è·å–ç­–ç•¥è¾“å‡º
            predicted_answer, log_prob, output_text = self.policy.get_answer_and_log_prob(problem)
            
            # è®¡ç®—å¥–åŠ±
            reward, is_correct = self.reward_function.calculate_reward(output_text, correct_answer)
            
            batch_data.append({
                'problem': problem,
                'predicted_answer': predicted_answer,
                'correct_answer': correct_answer,
                'output_text': output_text,
                'log_prob': log_prob,
                'reward': reward,
                'is_correct': is_correct
            })
            
        return batch_data
    
    def compute_policy_loss(self, batch_data: List[Dict[str, Any]]) -> torch.Tensor:
        """è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆREINFORCEï¼‰"""
        total_loss = 0
        
        for data in batch_data:
            log_prob = data['log_prob']
            reward = data['reward']
            
            # REINFORCEæŸå¤±
            loss = -log_prob * reward
            total_loss += loss
            
        return total_loss / len(batch_data)
    
    def train_step(self) -> ExperimentMetrics:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # æ”¶é›†æ•°æ®
        batch_data = self.collect_batch()
        
        # è®¡ç®—æŸå¤±
        policy_loss = self.compute_policy_loss(batch_data)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        policy_loss.backward()
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = 0.0
        for param in self.policy.parameters():
            if param.grad is not None:
                grad_norm += param.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5
        
        self.optimizer.step()
        
        # è®¡ç®—æŒ‡æ ‡
        rewards = [data['reward'] for data in batch_data]
        correct_flags = [data['is_correct'] for data in batch_data]
        
        accuracy = np.mean(correct_flags)
        reward_mean = np.mean(rewards)
        reward_std = np.std(rewards)
        
        # è®¡ç®—ç­–ç•¥ç†µ
        with torch.no_grad():
            sample_problem = batch_data[0]['problem']
            logits = self.policy.forward(sample_problem)
            probs = F.softmax(logits, dim=-1)
            entropy = -(probs * torch.log(probs + 1e-8)).sum().item()
        
        # è®¡ç®—Reward Hackingå¾—åˆ†
        hacking_score = self.calculate_hacking_score(batch_data)
        
        metrics = ExperimentMetrics(
            step=len(self.metrics_history),
            accuracy=accuracy,
            reward_mean=reward_mean,
            reward_std=reward_std,
            entropy=entropy,
            gradient_norm=grad_norm,
            hacking_score=hacking_score
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def calculate_hacking_score(self, batch_data: List[Dict[str, Any]]) -> float:
        """è®¡ç®—Reward Hackingå¾—åˆ†ï¼šé«˜å¥–åŠ±ä½†é”™è¯¯ç­”æ¡ˆçš„æ¯”ä¾‹"""
        high_reward_wrong = 0
        total_wrong = 0
        
        for data in batch_data:
            if not data['is_correct']:
                total_wrong += 1
                if data['reward'] > 0.5:  # é«˜å¥–åŠ±é˜ˆå€¼
                    high_reward_wrong += 1
        
        return high_reward_wrong / total_wrong if total_wrong > 0 else 0.0

def run_verl_experiment():
    """è¿è¡ŒVeRLå®éªŒ"""
    print("ğŸª å®éªŒä¸€ï¼šVeRLæ–¹æ¡ˆï¼ˆå‡½æ•°å¼å¥–åŠ±ï¼‰")
    print("=" * 50)
    
    trainer = VeRLTrainer(reward_system='verl')
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for step in range(100):
        metrics = trainer.train_step()
        
        if (step + 1) % 20 == 0:
            print(f"ğŸ“Š ç¬¬{step+1}æ­¥: å‡†ç¡®ç‡={metrics.accuracy:.3f}, å¥–åŠ±={metrics.reward_mean:.3f}, "
                  f"Hacking={metrics.hacking_score:.3f}")
    
    print("âœ… VeRLå®éªŒå®Œæˆ")
    return trainer.metrics_history

def run_learned_reward_experiment():
    """è¿è¡Œä¼ ç»Ÿå­¦ä¹ å‹å¥–åŠ±å®éªŒ"""
    print("\nğŸª å®éªŒäºŒï¼šä¼ ç»Ÿæ–¹æ¡ˆï¼ˆå­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ï¼‰")
    print("=" * 50)
    
    trainer = VeRLTrainer(reward_system='learned')
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for step in range(100):
        metrics = trainer.train_step()
        
        if (step + 1) % 20 == 0:
            print(f"ğŸ“Š ç¬¬{step+1}æ­¥: å‡†ç¡®ç‡={metrics.accuracy:.3f}, å¥–åŠ±={metrics.reward_mean:.3f}, "
                  f"Hacking={metrics.hacking_score:.3f}")
    
    print("âœ… ä¼ ç»Ÿå¥–åŠ±å®éªŒå®Œæˆ")
    return trainer.metrics_history

def create_comparison_visualization(verl_metrics: List[ExperimentMetrics], 
                                  learned_metrics: List[ExperimentMetrics]):
    """ç”ŸæˆVeRLå¯¹æ¯”åˆ†æå›¾è¡¨"""
    print("\nğŸ¨ ç”ŸæˆVeRL vs ä¼ ç»Ÿå¥–åŠ±å¯¹æ¯”åˆ†æ...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('VeRL vs Traditional Reward Model Comparison', fontsize=16, fontweight='bold')
    
    steps_verl = [m.step for m in verl_metrics]
    steps_learned = [m.step for m in learned_metrics]
    
    # 1. å‡†ç¡®ç‡å¯¹æ¯”
    ax = axes[0, 0]
    ax.plot(steps_verl, [m.accuracy for m in verl_metrics], 'b-', label='VeRL (Function-based)', linewidth=2)
    ax.plot(steps_learned, [m.accuracy for m in learned_metrics], 'r-', label='Traditional (Learned)', linewidth=2)
    ax.set_title('Accuracy Comparison', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Accuracy')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. å¥–åŠ±å¯¹æ¯”
    ax = axes[0, 1]
    ax.plot(steps_verl, [m.reward_mean for m in verl_metrics], 'b-', label='VeRL', linewidth=2)
    ax.plot(steps_learned, [m.reward_mean for m in learned_metrics], 'r-', label='Traditional', linewidth=2)
    ax.set_title('Mean Reward Comparison', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Mean Reward')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. Reward Hackingå¾—åˆ†
    ax = axes[0, 2]
    ax.plot(steps_verl, [m.hacking_score for m in verl_metrics], 'b-', label='VeRL', linewidth=2)
    ax.plot(steps_learned, [m.hacking_score for m in learned_metrics], 'r-', label='Traditional', linewidth=2)
    ax.set_title('Reward Hacking Score', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Hacking Score')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. ç­–ç•¥ç†µ
    ax = axes[1, 0]
    ax.plot(steps_verl, [m.entropy for m in verl_metrics], 'b-', label='VeRL', linewidth=2)
    ax.plot(steps_learned, [m.entropy for m in learned_metrics], 'r-', label='Traditional', linewidth=2)
    ax.set_title('Policy Entropy', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Entropy')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 5. æ¢¯åº¦ç¨³å®šæ€§
    ax = axes[1, 1]
    ax.plot(steps_verl, [m.gradient_norm for m in verl_metrics], 'b-', label='VeRL', linewidth=2)
    ax.plot(steps_learned, [m.gradient_norm for m in learned_metrics], 'r-', label='Traditional', linewidth=2)
    ax.set_title('Gradient Norm', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Gradient Norm')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 6. å¥–åŠ±æ ‡å‡†å·®
    ax = axes[1, 2]
    ax.plot(steps_verl, [m.reward_std for m in verl_metrics], 'b-', label='VeRL', linewidth=2)
    ax.plot(steps_learned, [m.reward_std for m in learned_metrics], 'r-', label='Traditional', linewidth=2)
    ax.set_title('Reward Standard Deviation', fontweight='bold')
    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Reward Std')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_path = 'verl_vs_traditional_comparison.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¯¹æ¯”åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    plt.show()

def generate_comparison_report(verl_metrics: List[ExperimentMetrics], 
                             learned_metrics: List[ExperimentMetrics]) -> str:
    """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š"""
    
    # æœ€ç»ˆæŒ‡æ ‡
    verl_final = verl_metrics[-1]
    learned_final = learned_metrics[-1]
    
    # å¹³å‡æŒ‡æ ‡
    verl_avg_accuracy = np.mean([m.accuracy for m in verl_metrics[-20:]])  # æœ€å20æ­¥å¹³å‡
    learned_avg_accuracy = np.mean([m.accuracy for m in learned_metrics[-20:]])
    
    verl_avg_hacking = np.mean([m.hacking_score for m in verl_metrics[-20:]])
    learned_avg_hacking = np.mean([m.hacking_score for m in learned_metrics[-20:]])
    
    accuracy_improvement = (verl_avg_accuracy - learned_avg_accuracy) / learned_avg_accuracy * 100
    hacking_reduction = (learned_avg_hacking - verl_avg_hacking) / (learned_avg_hacking + 1e-8) * 100
    
    report = f"""
ğŸ“Š VeRL vs Traditional Reward Model å¯¹æ¯”åˆ†ææŠ¥å‘Š
============================================================

ğŸ” æœ€ç»ˆæ€§èƒ½å¯¹æ¯”:
------------------------
ğŸ“ˆ VeRLæ–¹æ¡ˆ (å‡½æ•°å¼å¥–åŠ±):
â€¢ æœ€ç»ˆå‡†ç¡®ç‡: {verl_final.accuracy:.3f} ({verl_final.accuracy*100:.1f}%)
â€¢ æœ€ç»ˆå¥–åŠ±: {verl_final.reward_mean:.3f}
â€¢ Reward Hackingå¾—åˆ†: {verl_final.hacking_score:.3f}

ğŸ“‰ ä¼ ç»Ÿæ–¹æ¡ˆ (å­¦ä¹ å‹å¥–åŠ±):
â€¢ æœ€ç»ˆå‡†ç¡®ç‡: {learned_final.accuracy:.3f} ({learned_final.accuracy*100:.1f}%)
â€¢ æœ€ç»ˆå¥–åŠ±: {learned_final.reward_mean:.3f}
â€¢ Reward Hackingå¾—åˆ†: {learned_final.hacking_score:.3f}

ğŸ¯ å…³é”®æ”¹è¿›æŒ‡æ ‡:
------------------------
â€¢ å‡†ç¡®ç‡æå‡: {accuracy_improvement:+.1f}%
â€¢ Hackingå‡å°‘: {hacking_reduction:+.1f}%
â€¢ ç¨³å®šæ€§ä¼˜åŠ¿: {'âœ… VeRLæ›´ç¨³å®š' if verl_final.reward_std < learned_final.reward_std else 'âš ï¸ ä¼ ç»Ÿæ–¹æ¡ˆæ›´ç¨³å®š'}

ğŸ”¬ æŠ€æœ¯æ´å¯Ÿ:
------------------------
1. **å¥–åŠ±ä¿¡å·è´¨é‡**:
   â€¢ VeRL: æä¾›å®Œå…¨å‡†ç¡®çš„äºŒå…ƒå¥–åŠ± (0/1)
   â€¢ ä¼ ç»Ÿ: å­˜åœ¨åè§ï¼Œå¯èƒ½ç»™é”™è¯¯ç­”æ¡ˆé«˜å¥–åŠ±

2. **Reward Hackingç°è±¡**:
   â€¢ VeRLå¹³å‡Hackingå¾—åˆ†: {verl_avg_hacking:.3f}
   â€¢ ä¼ ç»Ÿå¹³å‡Hackingå¾—åˆ†: {learned_avg_hacking:.3f}
   â€¢ åˆ†æ: {'VeRLæˆåŠŸæ¶ˆé™¤äº†å¥–åŠ±é»‘å®¢è¡Œä¸º' if verl_avg_hacking < learned_avg_hacking else 'ä¸¤è€…å·®å¼‚ä¸æ˜æ˜¾'}

3. **è®­ç»ƒç¨³å®šæ€§**:
   â€¢ VeRLå¥–åŠ±æ ‡å‡†å·®: {verl_final.reward_std:.3f}
   â€¢ ä¼ ç»Ÿå¥–åŠ±æ ‡å‡†å·®: {learned_final.reward_std:.3f}
   â€¢ ç»“è®º: {'VeRLæä¾›æ›´ä¸€è‡´çš„å­¦ä¹ ä¿¡å·' if verl_final.reward_std < learned_final.reward_std else 'ä¼ ç»Ÿæ–¹æ¡ˆæ³¢åŠ¨æ€§æ›´å°'}

ğŸ’¡ å®éªŒç»“è®º:
------------------------
{'âœ… VeRLæ–¹æ¡ˆæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ¡ˆ' if accuracy_improvement > 5 and hacking_reduction > 30 else 
 'ğŸ“Š VeRLæ–¹æ¡ˆç•¥ä¼˜äºä¼ ç»Ÿæ–¹æ¡ˆ' if accuracy_improvement > 0 and hacking_reduction > 0 else
 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜å’ŒéªŒè¯'}

ğŸš€ VeRLçš„æ ¸å¿ƒä¼˜åŠ¿:
â€¢ æ¶ˆé™¤å¥–åŠ±æ¨¡å‹åè§
â€¢ æœç»Reward Hackingæ¼æ´
â€¢ æä¾›ç¡®å®šæ€§çš„"çœŸå€¼"åé¦ˆ
â€¢ ä¸ºå¤æ‚å¯¹æŠ—è®­ç»ƒæä¾›ç¨³å®šåŸºç¡€

ğŸ“ å­¦ä¹ è¦ç‚¹:
â€¢ å‡½æ•°å¼å¥–åŠ± > å­¦ä¹ å‹å¥–åŠ±ï¼ˆåœ¨å¯éªŒè¯ä»»åŠ¡ä¸­ï¼‰
â€¢ ç¡®å®šæ€§éªŒè¯æ¶ˆé™¤äº†æ¨¡å‹é’»ç©ºå­çš„å¯èƒ½
â€¢ VeRLä¸ºåç»­APOå¯¹æŠ—åšå¼ˆæä¾›å¯é çš„"é”šç‚¹"
"""
    
    return report

def main():
    """ä¸»å®éªŒæµç¨‹"""
    print("ğŸš€ æ¬¢è¿æ¥åˆ°VeRLå®éªŒå®¤ï¼")
    print("ğŸ“š æœ¬å®éªŒå°†å¯¹æ¯”å‡½æ•°å¼å¥–åŠ±ä¸å­¦ä¹ å‹å¥–åŠ±åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„æ•ˆæœ")
    
    # æ¦‚å¿µé¢„ä¹ 
    print("\nğŸ“– å®éªŒå‰æ¦‚å¿µé¢„ä¹ :")
    print("=" * 50)
    
    explainer = ExplainerSystem()
    for concept in ['verl', 'function_reward', 'reward_hacking', 'math_task']:
        print(explainer.explain_concept(concept))
    
    print("ğŸ”¥ å¼€å§‹å¯¹æ¯”å®éªŒ...")
    
    # è¿è¡Œä¸¤ä¸ªå®éªŒ
    verl_metrics = run_verl_experiment()
    learned_metrics = run_learned_reward_experiment()
    
    # ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print("\n" + "="*50)
    print(generate_comparison_report(verl_metrics, learned_metrics))
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_comparison_visualization(verl_metrics, learned_metrics)
    
    print("\nğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
    print("VeRLå®éªŒå…¨éƒ¨å®Œæˆï¼")
    print("ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰")
    
    print("\nâœ¨ æ­å–œä½ å®Œæˆäº†Lab06çš„å­¦ä¹ ï¼")
    print("ğŸ¯ ä½ ç°åœ¨æŒæ¡äº†VeRLå¯éªŒè¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯")
    print("ğŸ”§ ä¸‹ä¸€æ­¥å¯ä»¥å­¦ä¹ APOå¯¹æŠ—æ€§åå¥½ä¼˜åŒ–")
    print("ğŸš€ ç»§ç»­åŠ æ²¹ï¼Œå‘ç€AIå®‰å…¨ä¸“å®¶çš„ç›®æ ‡å‰è¿›ï¼")

if __name__ == "__main__":
    main() 
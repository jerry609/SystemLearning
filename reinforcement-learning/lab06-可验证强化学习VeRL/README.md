# 实验六：可验证强化学习 (Verified RL, VeRL)

## 🎯 实验目标
1. 理解传统"奖励模型"的局限性，如存在偏见、可能被"攻击"（Reward Hacking）。
2. 掌握可验证强化学习（VeRL）的核心思想：使用程序化的、确定性的函数来提供奖励。
3. 实现一个基于函数的奖励（Function-based Reward）系统。
4. 对比VeRL与传统奖励模型在训练稳定性和结果可靠性上的差异。

## 📖 理论背景
- **VeRL**: VeRL范式适用于那些奖励信号可以被程序化、确定性地验证的任务。它提供的奖励是**二元的（对或错）、无偏见的，并且不会被模型"钻空子"**。例如，代码执行是否通过所有单元测试、数学题答案是否正确、或者在我们的最终目标中，"是否正确识别出了恶意智能体？"。
- **"真值"信号**: VeRL为学习过程提供了一个"上帝视角"的、绝对正确的奖励信号。这避免了使用另一个可能不完美的、基于学习的奖励模型所带来的不确定性和噪声，是稳定复杂对抗过程的基石。
- **`verl`库**: 方案中提及的`verl`是一个生产级RL训练库，其核心设计理念之一就是对`function-based reward`的无缝支持。

## 🛠️ 实践内容
1. **设计一个VeRL任务**: 选择一个适合VeRL的任务，例如：
    - **代码生成**: LLM生成一段代码，奖励函数是运行单元测试的结果（Pass/Fail）。
    - **数学问题**: LLM解决一个代数题，奖励函数是验证答案是否正确。
2. **实现奖励函数**: 编写一个确定性的Python函数，该函数接收LLM的输出，并返回一个二元（1/0）或定量的奖励分数。
3. **对比实验**:
    - **场景A (VeRL)**: 使用DAPO算法和您实现的函数式奖励来训练LLM。
    - **场景B (Learned Reward)**: 先训练一个传统的奖励模型来模仿奖励函数，然后使用DAPO和这个有噪声的奖励模型来训练LLM。
4. **分析结果**: 对比两个场景下LLM的最终表现和训练过程的稳定性。观察在场景B中，LLM是否学会了利用奖励模型的漏洞（Reward Hacking）来获得高分，即使其答案是错误的。 
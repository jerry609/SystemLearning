# 实验三：优化PPO - 实现GRPO

## 🎯 实验目标
1. 理解传统PPO中"评论家"模型作为基准（Baseline）的作用。
2. 掌握组相对策略优化（GRPO）的核心思想：使用同组样本的平均奖励作为新的基准。
3. 动手修改实验二中的PPO代码，实现一个GRPO训练循环。
4. 对比PPO和GRPO在资源消耗和训练性能上的差异。

## 📖 理论背景
- **GRPO**: GRPO (Group Relative Policy Optimization) 的核心创新是**完全移除了"评论家"模型**。它通过对同一个问题（Prompt）进行多次采样，生成一组答案，然后计算这一组答案的平均奖励。这个平均奖励将作为一个动态的、相对的基准（Baseline），用于判断单个答案是好于平均水平还是差于平均水平，从而计算优势（Advantage）并指导策略更新。
- **优势**: 移除了庞大的评论家模型，极大地降低了训练的显存和计算需求，使其更适合大规模LLM的训练。

## 🛠️ 实践内容
1. **代码修改**: 在实验二代码的基础上，移除Critic模型和相关的价值计算逻辑。
2. **实现新基准**: 增加一个采样步骤，对每个Prompt生成N个回答，计算其平均奖励，并用此作为基准来计算优势函数。
3. **对比实验**: 在与实验二相同的任务和硬件上，运行GRPO训练流程。
4. **性能对比**: 详细记录并对比GRPO与PPO的：
    - 峰值显存占用
    - 每秒训练样本数（Throughput）
    - 最终模型的任务表现 
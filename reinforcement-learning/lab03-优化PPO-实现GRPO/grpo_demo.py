#!/usr/bin/env python3
"""
Lab03 GRPOç®—æ³•å®Œæ•´å®ç°æ¼”ç¤º
Group Relative Policy Optimization - ç§»é™¤Criticï¼Œä½¿ç”¨ç»„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºå‡†

æ ¸å¿ƒåˆ›æ–°ï¼š
1. ç§»é™¤è¯„è®ºå®¶æ¨¡å‹ï¼Œé™ä½50%æ˜¾å­˜å ç”¨
2. å¯¹æ¯ä¸ªprompté‡‡æ ·å¤šä¸ªå“åº”ï¼Œè®¡ç®—ç»„å¹³å‡å¥–åŠ±
3. ä½¿ç”¨ç»„å¹³å‡å¥–åŠ±ä½œä¸ºåŠ¨æ€åŸºå‡†æ¥è®¡ç®—ä¼˜åŠ¿å‡½æ•°
4. å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    GenerationConfig
)
from datasets import Dataset
import numpy as np
import time
import psutil
import GPUtil

class SimpleRewardModel(nn.Module):
    """ç®€å•çš„å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆæ–‡æœ¬è´¨é‡"""
    def __init__(self, vocab_size, hidden_size=256):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 1)
        )
        
    def forward(self, input_ids, attention_mask=None):
        embeddings = self.embedding(input_ids)
        # å¹³å‡æ± åŒ–
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
            sum_embeddings = torch.sum(embeddings * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            mean_embeddings = sum_embeddings / sum_mask
        else:
            mean_embeddings = torch.mean(embeddings, dim=1)
            
        return self.fc(mean_embeddings)

class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨ - ç§»é™¤Criticæ¨¡å‹çš„è½»é‡çº§å®ç°"""
    
    def __init__(self, model, ref_model, reward_model, tokenizer, config):
        self.model = model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.tokenizer = tokenizer
        self.config = config
        
        # ä¼˜åŒ–å™¨åªä¼˜åŒ–ä¸»æ¨¡å‹
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=config['learning_rate'],
            weight_decay=config.get('weight_decay', 0.01)
        )
        
    def reset_cuda_state(self):
        """å®Œå…¨é‡ç½®CUDAçŠ¶æ€"""
        try:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            print("       CUDAçŠ¶æ€å·²é‡ç½®")
        except Exception as e:
            print(f"       CUDAé‡ç½®è­¦å‘Š: {e}")
    
    def safe_cuda_operation(self, operation_func, *args, **kwargs):
        """å®‰å…¨æ‰§è¡ŒCUDAæ“ä½œï¼Œè‡ªåŠ¨å¤„ç†é”™è¯¯æ¢å¤"""
        max_retries = 3
        for retry in range(max_retries):
            try:
                return operation_func(*args, **kwargs)
            except RuntimeError as e:
                if "CUDA" in str(e) and retry < max_retries - 1:
                    print(f"       CUDAé”™è¯¯ï¼Œå°è¯•æ¢å¤ (ç¬¬{retry+1}æ¬¡)")
                    # å®Œæ•´çš„CUDAæ¢å¤æµç¨‹
                    self.reset_cuda_state()
                    
                    # å°†æ¨¡å‹é‡æ–°åŠ è½½åˆ°GPU
                    try:
                        self.model = self.model.cuda()
                        self.ref_model = self.ref_model.cuda() 
                        self.reward_model = self.reward_model.cuda()
                        print("       æ¨¡å‹å·²é‡æ–°åŠ è½½åˆ°GPU")
                    except Exception as reload_e:
                        print(f"       GPUé‡æ–°åŠ è½½å¤±è´¥: {reload_e}")
                        # å¦‚æœGPUå®Œå…¨ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU
                        self.model = self.model.cpu()
                        self.ref_model = self.ref_model.cpu()
                        self.reward_model = self.reward_model.cpu()
                        print("       å·²åˆ‡æ¢åˆ°CPUæ¨¡å¼")
                        
                    continue
                else:
                    raise e
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise RuntimeError("CUDAæ“ä½œé‡è¯•å¤±è´¥")
    
    def generate_responses(self, prompts, num_samples_per_prompt=4):
        """ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå“åº” - å¸¦å®Œæ•´é”™è¯¯é˜²æŠ¤"""
        all_responses = []
        all_prompt_indices = []
        
        generation_kwargs = {
            "max_new_tokens": self.config.get("max_new_tokens", 20),
            "do_sample": True,
            "temperature": max(0.1, self.config.get("temperature", 0.7)),
            "top_p": min(0.95, self.config.get("top_p", 0.9)),
            "top_k": 50,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "output_scores": False,  # å…³é—­scoresè¾“å‡ºå‡å°‘å†…å­˜
        }
        
        for prompt_idx, prompt in enumerate(prompts):
            # å®‰å…¨ç¼–ç prompt
            try:
                encoded = self.tokenizer(
                    prompt, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=100,  # å‡å°‘é•¿åº¦é¿å…å†…å­˜é—®é¢˜
                    padding=False,   # ä¸paddingï¼Œé¿å…é¢å¤–çš„pad token
                    add_special_tokens=True
                )
                
                input_ids = encoded['input_ids'].to(self.model.device)
                
                # æ•°å€¼å®‰å…¨æ£€æŸ¥
                if input_ids.numel() == 0:
                    print(f"       è­¦å‘Š: prompt '{prompt}' ç¼–ç ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                    
                # æ£€æŸ¥token idsæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                vocab_size = len(self.tokenizer)
                if (input_ids >= vocab_size).any() or (input_ids < 0).any():
                    print(f"       è­¦å‘Š: prompt '{prompt}' åŒ…å«æ— æ•ˆtoken idsï¼Œè·³è¿‡")
                    continue
                
                print(f"       ç¼–ç prompt {prompt_idx+1}: é•¿åº¦={input_ids.shape[1]}, tokens={input_ids[0][:5].tolist()}...")
                
            except Exception as e:
                print(f"       é”™è¯¯: ç¼–ç prompt '{prompt}' å¤±è´¥: {e}")
                continue
            
            # ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå“åº”
            for sample_idx in range(num_samples_per_prompt):
                try:
                    # æ¸…ç†GPUç¼“å­˜
                    if sample_idx % 2 == 0:
                        torch.cuda.empty_cache()
                    
                    with torch.no_grad():
                        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
                        self.model.eval()
                        
                        # ç”Ÿæˆå“åº”
                        output = self.model.generate(
                            input_ids,
                            **generation_kwargs
                        )
                        
                        # å®‰å…¨æå–æ–°ç”Ÿæˆçš„tokens
                        if output.shape[1] <= input_ids.shape[1]:
                            response_text = ""  # æ²¡æœ‰ç”Ÿæˆæ–°tokens
                        else:
                            new_tokens = output[0][input_ids.shape[1]:]
                            
                            # æ£€æŸ¥ç”Ÿæˆçš„tokensæ˜¯å¦æœ‰æ•ˆ
                            if (new_tokens >= vocab_size).any() or (new_tokens < 0).any():
                                print(f"       è­¦å‘Š: ç”Ÿæˆäº†æ— æ•ˆtokenï¼Œä½¿ç”¨é»˜è®¤å“åº”")
                                response_text = "ç”Ÿæˆå†…å®¹"
                            else:
                                response_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
                                response_text = response_text.strip()
                        
                        # ç¡®ä¿å“åº”ä¸ä¸ºç©ºä¸”åˆç†
                        if not response_text or len(response_text) < 2:
                            response_text = "ç”Ÿæˆå†…å®¹"
                        
                        # é™åˆ¶å“åº”é•¿åº¦
                        if len(response_text) > 100:
                            response_text = response_text[:100] + "..."
                            
                        all_responses.append(response_text)
                        all_prompt_indices.append(prompt_idx)
                        
                        print(f"         æ ·æœ¬ {sample_idx+1}: '{response_text[:30]}...'")
                        
                except RuntimeError as e:
                    if "CUDA" in str(e):
                        print(f"       CUDAé”™è¯¯åœ¨æ ·æœ¬ {sample_idx+1}: {e}")
                        # æ¸…ç†GPUçŠ¶æ€
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        # ä½¿ç”¨CPUæ¨¡å¼ç”Ÿæˆå¤‡ç”¨å“åº”
                        try:
                            with torch.no_grad():
                                cpu_input = input_ids.cpu()
                                cpu_model = self.model.cpu()
                                cpu_output = cpu_model.generate(
                                    cpu_input,
                                    max_new_tokens=10,
                                    do_sample=False,  # CPUä¸Šä½¿ç”¨è´ªå©ªè§£ç 
                                    pad_token_id=self.tokenizer.eos_token_id
                                )
                                new_tokens = cpu_output[0][cpu_input.shape[1]:]
                                response_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
                                if not response_text:
                                    response_text = "å¤‡ç”¨å“åº”"
                                # æ¨¡å‹ç§»å›GPU
                                self.model.to(self.model.device)
                                all_responses.append(response_text)
                                all_prompt_indices.append(prompt_idx)
                                print(f"         CPUå¤‡ç”¨å“åº”: '{response_text}'")
                        except Exception as cpu_e:
                            print(f"       CPUå¤‡ç”¨ç”Ÿæˆä¹Ÿå¤±è´¥: {cpu_e}")
                            all_responses.append("é»˜è®¤å“åº”")
                            all_prompt_indices.append(prompt_idx)
                    else:
                        print(f"       å…¶ä»–ç”Ÿæˆé”™è¯¯: {e}")
                        all_responses.append("é”™è¯¯å“åº”")
                        all_prompt_indices.append(prompt_idx)
                        
                except Exception as e:
                    print(f"       æœªçŸ¥é”™è¯¯åœ¨æ ·æœ¬ {sample_idx+1}: {e}")
                    all_responses.append("æœªçŸ¥é”™è¯¯å“åº”")
                    all_prompt_indices.append(prompt_idx)
            
        print(f"     æ€»å…±ç”Ÿæˆ {len(all_responses)} ä¸ªå“åº”")
        return all_responses, all_prompt_indices
    
    def compute_group_advantages(self, prompts, responses, prompt_indices):
        """è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿ - GRPOçš„æ ¸å¿ƒåˆ›æ–°"""
        
        # 1. è®¡ç®—æ‰€æœ‰å“åº”çš„å¥–åŠ±
        all_rewards = []
        for prompt_idx, response in zip(prompt_indices, responses):
            prompt = prompts[prompt_idx]
            full_text = prompt + response
            
            # ç¼–ç å®Œæ•´æ–‡æœ¬
            input_ids = self.tokenizer.encode(
                full_text, return_tensors="pt", truncation=True, max_length=256
            ).to(next(self.reward_model.parameters()).device)
            
            with torch.no_grad():
                reward = self.reward_model(input_ids)
                all_rewards.append(reward.squeeze().cpu().item())
        
        # 2. è®¡ç®—æ¯ä¸ªpromptç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºå‡†
        unique_prompt_indices = list(set(prompt_indices))
        group_baselines = {}
        
        for prompt_idx in unique_prompt_indices:
            # æ‰¾åˆ°å±äºè¿™ä¸ªpromptçš„æ‰€æœ‰å“åº”çš„å¥–åŠ±
            group_rewards = [
                reward for i, reward in enumerate(all_rewards) 
                if prompt_indices[i] == prompt_idx
            ]
            # è®¡ç®—ç»„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºå‡†
            group_baselines[prompt_idx] = np.mean(group_rewards)
        
        # 3. è®¡ç®—ä¼˜åŠ¿ = ä¸ªä½“å¥–åŠ± - ç»„å¹³å‡å¥–åŠ±
        advantages = []
        for prompt_idx, reward in zip(prompt_indices, all_rewards):
            advantage = reward - group_baselines[prompt_idx]
            advantages.append(advantage)
        
        return advantages, all_rewards, group_baselines
    
    def compute_policy_loss(self, prompts, responses, prompt_indices, advantages):
        """è®¡ç®—ç­–ç•¥æŸå¤± - å¸¦æ•°å€¼å®‰å…¨æ£€æŸ¥"""
        
        total_loss = 0.0
        total_kl = 0.0
        valid_samples = 0
        
        for prompt, response, advantage in zip(
            [prompts[i] for i in prompt_indices], responses, advantages
        ):
            try:
                # ç¼–ç åºåˆ—
                full_text = prompt + response
                input_ids = self.tokenizer.encode(
                    full_text, return_tensors="pt", truncation=True, max_length=200
                ).to(self.model.device)
                
                # å®‰å…¨æ£€æŸ¥
                if input_ids.numel() == 0:
                    continue
                    
                prompt_length = len(self.tokenizer.encode(prompt, truncation=True, max_length=100))
                
                # ç¡®ä¿æœ‰å“åº”tokens
                if input_ids.shape[1] <= prompt_length:
                    continue
                
                # è·å–æ¨¡å‹è¾“å‡º - å®‰å…¨æ¨¡å¼
                with torch.no_grad():
                    try:
                        ref_logits = self.ref_model(input_ids).logits
                    except RuntimeError as e:
                        if "CUDA" in str(e):
                            print(f"       å‚è€ƒæ¨¡å‹CUDAé”™è¯¯ï¼Œè·³è¿‡æ ·æœ¬")
                            continue
                        raise e
                
                try:
                    current_logits = self.model(input_ids).logits
                except RuntimeError as e:
                    if "CUDA" in str(e):
                        print(f"       ä¸»æ¨¡å‹CUDAé”™è¯¯ï¼Œè·³è¿‡æ ·æœ¬") 
                        continue
                    raise e
                
                # æå–å“åº”éƒ¨åˆ†çš„logits
                response_start = max(0, prompt_length - 1)
                response_logits = current_logits[0, response_start:-1, :]
                response_targets = input_ids[0, prompt_length:]
                ref_response_logits = ref_logits[0, response_start:-1, :]
                
                if response_targets.numel() == 0 or response_logits.shape[0] != response_targets.shape[0]:
                    continue
                
                # æ•°å€¼ç¨³å®šçš„log_softmax
                current_log_probs = F.log_softmax(response_logits, dim=-1)
                ref_log_probs = F.log_softmax(ref_response_logits, dim=-1)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–inf
                if torch.isnan(current_log_probs).any() or torch.isinf(current_log_probs).any():
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨current_log_probsï¼Œè·³è¿‡æ ·æœ¬")
                    continue
                    
                if torch.isnan(ref_log_probs).any() or torch.isinf(ref_log_probs).any():
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨ref_log_probsï¼Œè·³è¿‡æ ·æœ¬")
                    continue
                
                # è·å–é€‰æ‹©tokençš„å¯¹æ•°æ¦‚ç‡
                current_selected_log_probs = current_log_probs.gather(1, response_targets.unsqueeze(1)).squeeze()
                ref_selected_log_probs = ref_log_probs.gather(1, response_targets.unsqueeze(1)).squeeze()
                
                # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡ - æ•°å€¼ç¨³å®šç‰ˆæœ¬
                log_ratio = current_selected_log_probs - ref_selected_log_probs
                
                # æ£€æŸ¥log_ratioçš„æ•°å€¼ç¨³å®šæ€§
                if torch.isnan(log_ratio).any() or torch.isinf(log_ratio).any():
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨log_ratioï¼Œè·³è¿‡æ ·æœ¬")
                    continue
                
                # é™åˆ¶log_ratioèŒƒå›´é˜²æ­¢expæº¢å‡º
                log_ratio = torch.clamp(log_ratio, min=-10, max=10)
                ratio = torch.exp(log_ratio)
                
                # å®‰å…¨æ£€æŸ¥ratio
                if torch.isnan(ratio).any() or torch.isinf(ratio).any():
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨ratioï¼Œè·³è¿‡æ ·æœ¬")
                    continue
                
                # PPO clip loss
                advantage_tensor = torch.tensor(advantage, dtype=ratio.dtype, device=ratio.device)
                clip_ratio = torch.clamp(ratio, 1 - self.config['clip_range'], 1 + self.config['clip_range'])
                
                policy_loss_1 = ratio * advantage_tensor
                policy_loss_2 = clip_ratio * advantage_tensor
                policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()
                
                # KLæ•£åº¦æƒ©ç½š
                kl_div = (current_selected_log_probs - ref_selected_log_probs).mean()
                
                # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
                if torch.isnan(policy_loss) or torch.isinf(policy_loss):
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨policy_lossï¼Œè·³è¿‡æ ·æœ¬")
                    continue
                    
                if torch.isnan(kl_div) or torch.isinf(kl_div):
                    print(f"       æ£€æµ‹åˆ°NaN/infåœ¨kl_divï¼Œè®¾ä¸º0")
                    kl_div = torch.tensor(0.0)
                
                # ç´¯åŠ æŸå¤±
                sample_loss = policy_loss + self.config['kl_coef'] * kl_div
                total_loss += sample_loss
                total_kl += kl_div.item()
                valid_samples += 1
                
            except Exception as e:
                print(f"       ç­–ç•¥æŸå¤±è®¡ç®—é”™è¯¯: {e}")
                continue
        
        if valid_samples == 0:
            print(f"       è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ç”¨äºæŸå¤±è®¡ç®—")
            return torch.tensor(0.0, requires_grad=True), 0.0
        
        avg_loss = total_loss / valid_samples
        avg_kl = total_kl / valid_samples
        
        # æœ€ç»ˆæ•°å€¼æ£€æŸ¥
        if torch.isnan(avg_loss) or torch.isinf(avg_loss):
            print(f"       æœ€ç»ˆæŸå¤±åŒ…å«NaN/infï¼Œè¿”å›é›¶æŸå¤±")
            return torch.tensor(0.0, requires_grad=True), avg_kl
        
        return avg_loss, avg_kl
    
    def train_step(self, prompts):
        """GRPOè®­ç»ƒæ­¥éª¤ - å¸¦å®Œæ•´CUDAé”™è¯¯æ¢å¤"""
        
        # åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤å¼€å§‹å‰é‡ç½®CUDAçŠ¶æ€
        self.reset_cuda_state()
        
        try:
            # 1. å®‰å…¨ç”Ÿæˆå¤šä¸ªå“åº”
            def _generate_responses():
                return self.generate_responses(
                    prompts, num_samples_per_prompt=self.config['num_samples_per_prompt']
                )
            
            responses, prompt_indices = self.safe_cuda_operation(_generate_responses)
            
            if len(responses) == 0:
                print("       è­¦å‘Š: æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å“åº”")
                return {
                    'policy_loss': 0.0,
                    'kl_div': 0.0,
                    'mean_reward': 0.0,
                    'mean_advantage': 0.0,
                    'num_responses': 0
                }
            
            # 2. å®‰å…¨è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿
            def _compute_advantages():
                return self.compute_group_advantages(prompts, responses, prompt_indices)
            
            advantages, rewards, baselines = self.safe_cuda_operation(_compute_advantages)
            
            # 3. å®‰å…¨è®¡ç®—ç­–ç•¥æŸå¤±
            def _compute_loss():
                return self.compute_policy_loss(prompts, responses, prompt_indices, advantages)
            
            policy_loss, kl_div = self.safe_cuda_operation(_compute_loss)
            
            # 4. å®‰å…¨æ‰§è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–
            if hasattr(policy_loss, 'backward') and policy_loss.requires_grad:
                def _optimize():
                    self.optimizer.zero_grad()
                    policy_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])
                    self.optimizer.step()
                    return True
                
                try:
                    self.safe_cuda_operation(_optimize)
                except Exception as e:
                    print(f"       ä¼˜åŒ–æ­¥éª¤å¤±è´¥: {e}")
            
            return {
                'policy_loss': policy_loss.item() if hasattr(policy_loss, 'item') else float(policy_loss),
                'kl_div': kl_div,
                'mean_reward': np.mean(rewards) if rewards else 0.0,
                'mean_advantage': np.mean(advantages) if advantages else 0.0,
                'num_responses': len(responses)
            }
            
        except Exception as e:
            print(f"       è®­ç»ƒæ­¥éª¤å®Œå…¨å¤±è´¥: {e}")
            # å¼ºåˆ¶é‡ç½®æ‰€æœ‰çŠ¶æ€
            self.reset_cuda_state()
            return {
                'policy_loss': 0.0,
                'kl_div': 0.0,
                'mean_reward': 0.0,
                'mean_advantage': 0.0,
                'num_responses': 0
            }

def create_preference_dataset():
    """åˆ›å»ºåå¥½æ•°æ®é›†"""
    data = {
        "query": [
            "å†™ä¸€ä¸ªå…³äºç§‘æŠ€çš„æ•…äº‹",
            "è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",
            "æè¿°æœªæ¥åŸå¸‚çš„æ ·å­", 
            "è°ˆè°ˆç¯ä¿çš„é‡è¦æ€§",
            "ä»‹ç»ä¸€ç§ç¼–ç¨‹è¯­è¨€",
            "è§£é‡ŠåŒºå—é“¾æŠ€æœ¯",
            "æè¿°å¤ªç©ºæ¢ç´¢çš„æ„ä¹‰",
            "è°ˆè°ˆäººå·¥æ™ºèƒ½çš„ä¼¦ç†"
        ]
    }
    return Dataset.from_dict(data)

def monitor_resources():
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨"""
    # CPUå’Œå†…å­˜
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    
    # GPU
    gpu_info = {}
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]
            gpu_info = {
                'gpu_memory_used': gpu.memoryUsed,
                'gpu_memory_total': gpu.memoryTotal,
                'gpu_utilization': gpu.load * 100
            }
    except:
        gpu_info = {'gpu_memory_used': 0, 'gpu_memory_total': 0, 'gpu_utilization': 0}
    
    return {
        'cpu_percent': cpu_percent,
        'memory_used_gb': memory.used / (1024**3),
        'memory_total_gb': memory.total / (1024**3),
        **gpu_info
    }

def main():
    """ä¸»å‡½æ•°ï¼šGRPO vs PPO å¯¹æ¯”å®éªŒ"""
    
    print("=== Lab03: GRPOç®—æ³•å®ç°ä¸æ€§èƒ½å¯¹æ¯” ===")
    
    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. é…ç½®
    print("\n1. åˆå§‹åŒ–GRPOé…ç½®...")
    config = {
        'learning_rate': 1e-5,
        'clip_range': 0.2,
        'kl_coef': 0.05,
        'max_grad_norm': 0.5,
        'weight_decay': 0.01,
        'num_samples_per_prompt': 4,  # æ¯ä¸ªprompté‡‡æ ·4ä¸ªå“åº”
        'max_new_tokens': 25,
        'temperature': 0.8,
        'top_p': 0.9,
    }
    
    model_name = "gpt2"
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   æ¯prompté‡‡æ ·æ•°: {config['num_samples_per_prompt']}")
    print(f"   å­¦ä¹ ç‡: {config['learning_rate']}")
    
    # 2. åˆå§‹åŒ–åˆ†è¯å™¨
    print("\n2. åˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 3. åˆ›å»ºæ¨¡å‹ç»„ä»¶ (GRPOåªéœ€è¦3ä¸ªï¼Œç§»é™¤äº†Critic)
    print("\n3. åˆ›å»ºGRPOæ‰€éœ€æ¨¡å‹ç»„ä»¶...")
    
    # 3.1 ä¸»ç­–ç•¥æ¨¡å‹ (Actor) - ä¸éœ€è¦Value Head!
    print("   3.1 åˆ›å»ºä¸»ç­–ç•¥æ¨¡å‹ (ä»…Actor, æ— Critic)...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device.type == "cuda" else torch.float32
    ).to(device)
    
    # ç¡®ä¿æœ‰generation_config
    if not hasattr(model, 'generation_config'):
        model.generation_config = GenerationConfig.from_pretrained(model_name)
    
    print(f"       å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3.2 å‚è€ƒæ¨¡å‹
    print("   3.2 åˆ›å»ºå‚è€ƒæ¨¡å‹...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device.type == "cuda" else torch.float32
    ).to(device)
    
    for param in ref_model.parameters():
        param.requires_grad = False
    print(f"       å‚æ•°é‡: {sum(p.numel() for p in ref_model.parameters()):,} (å·²å†»ç»“)")
    
    # 3.3 å¥–åŠ±æ¨¡å‹
    print("   3.3 åˆ›å»ºå¥–åŠ±æ¨¡å‹...")
    reward_model = SimpleRewardModel(
        vocab_size=len(tokenizer),
        hidden_size=256
    ).to(device)
    print(f"       å‚æ•°é‡: {sum(p.numel() for p in reward_model.parameters()):,}")
    
    # âœ¨ æ³¨æ„ï¼šæ²¡æœ‰3.4! GRPOç§»é™¤äº†Criticæ¨¡å‹
    print("   âœ¨ GRPOç‰¹è‰²ï¼šæ— éœ€Criticæ¨¡å‹ï¼ŒèŠ‚çœ50%æ˜¾å­˜!")
    
    # 4. åˆ›å»ºæ•°æ®é›†
    print("\n4. å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
    dataset = create_preference_dataset()
    prompts = dataset["query"]
    print(f"   æ•°æ®é›†å¤§å°: {len(prompts)}")
    
    # 5. åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
    print("\n5. åˆå§‹åŒ–GRPOè®­ç»ƒå™¨...")
    grpo_trainer = GRPOTrainer(
        model=model,
        ref_model=ref_model,
        reward_model=reward_model,
        tokenizer=tokenizer,
        config=config
    )
    print("   âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ!")
    
    # 6. èµ„æºç›‘æ§åŸºå‡†
    print("\n6. å¼€å§‹GRPOè®­ç»ƒä¸èµ„æºç›‘æ§...")
    initial_resources = monitor_resources()
    print("   åˆå§‹èµ„æºçŠ¶æ€:")
    print(f"     GPUæ˜¾å­˜: {initial_resources['gpu_memory_used']:.1f}MB / {initial_resources['gpu_memory_total']:.1f}MB")
    print(f"     CPUä½¿ç”¨ç‡: {initial_resources['cpu_percent']:.1f}%")
    print(f"     å†…å­˜ä½¿ç”¨: {initial_resources['memory_used_gb']:.1f}GB / {initial_resources['memory_total_gb']:.1f}GB")
    
    # 7. è®­ç»ƒå¾ªç¯
    num_epochs = 3
    
    for epoch in range(num_epochs):
        print(f"\n   --- è®­ç»ƒè½®æ¬¡ {epoch + 1}/{num_epochs} ---")
        
        # éšæœºé€‰æ‹©ä¸€æ‰¹prompts
        batch_prompts = np.random.choice(prompts, size=min(2, len(prompts)), replace=False).tolist()
        
        print(f"     è®­ç»ƒprompts: {len(batch_prompts)}")
        for i, prompt in enumerate(batch_prompts):
            print(f"       {i+1}. {prompt}")
        
        # è®­ç»ƒæ­¥éª¤
        start_time = time.time()
        stats = grpo_trainer.train_step(batch_prompts)
        step_time = time.time() - start_time
        
        # èµ„æºç›‘æ§
        current_resources = monitor_resources()
        
        print(f"     è®­ç»ƒç»Ÿè®¡:")
        print(f"       ç­–ç•¥æŸå¤±: {stats['policy_loss']:.4f}")
        print(f"       KLæ•£åº¦: {stats['kl_div']:.4f}")
        print(f"       å¹³å‡å¥–åŠ±: {stats['mean_reward']:.4f}")
        print(f"       å¹³å‡ä¼˜åŠ¿: {stats['mean_advantage']:.4f}")
        print(f"       ç”Ÿæˆå“åº”æ•°: {stats['num_responses']}")
        print(f"       è®­ç»ƒæ—¶é—´: {step_time:.2f}ç§’")
        
        print(f"     èµ„æºä½¿ç”¨:")
        print(f"       GPUæ˜¾å­˜: {current_resources['gpu_memory_used']:.1f}MB")
        print(f"       GPUåˆ©ç”¨ç‡: {current_resources['gpu_utilization']:.1f}%")
        print(f"       CPUä½¿ç”¨ç‡: {current_resources['cpu_percent']:.1f}%")
        
        # ååé‡è®¡ç®—
        throughput = stats['num_responses'] / step_time
        print(f"       ååé‡: {throughput:.2f} å“åº”/ç§’")
    
    # 8. æ•ˆæœæ¼”ç¤º
    print("\n8. GRPOè®­ç»ƒæ•ˆæœæ¼”ç¤º...")
    test_prompt = "äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿æ˜¯"
    print(f"   æµ‹è¯•prompt: {test_prompt}")
    
    # ç”Ÿæˆå¤šä¸ªå“åº”å±•ç¤ºç»„ç›¸å¯¹ä¼˜åŠ¿
    responses, _ = grpo_trainer.generate_responses([test_prompt], num_samples_per_prompt=4)
    advantages, rewards, baselines = grpo_trainer.compute_group_advantages(
        [test_prompt], responses, [0] * len(responses)
    )
    
    print(f"   ç»„å¹³å‡å¥–åŠ± (åŸºå‡†): {baselines[0]:.4f}")
    print("   ç”Ÿæˆçš„å“åº”åŠå…¶ç›¸å¯¹ä¼˜åŠ¿:")
    for i, (response, reward, advantage) in enumerate(zip(responses, rewards, advantages)):
        print(f"     {i+1}. å“åº”: {response}")
        print(f"        å¥–åŠ±: {reward:.4f} | ä¼˜åŠ¿: {advantage:+.4f}")
    
    # 9. ä¸PPOå¯¹æ¯”æ€»ç»“
    print("\n=== GRPO vs PPO æ€§èƒ½å¯¹æ¯” ===")
    final_resources = monitor_resources()
    
    # ä¼°ç®—æ˜¾å­˜èŠ‚çœ (å‡è®¾PPOéœ€è¦ç›¸åŒå¤§å°çš„Critic)
    critic_params = sum(p.numel() for p in model.parameters())  # Criticä¸ActoråŒæ ·å¤§å°
    grpo_params = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in ref_model.parameters())
    ppo_params = grpo_params + critic_params  # PPOè¿˜éœ€è¦Critic
    
    memory_saving = (critic_params / ppo_params) * 100
    
    print(f"ğŸš€ GRPOä¼˜åŠ¿:")
    print(f"   âœ… ç§»é™¤Criticæ¨¡å‹: èŠ‚çœ {memory_saving:.1f}% å‚æ•°é‡")
    print(f"   âœ… å½“å‰GPUæ˜¾å­˜ä½¿ç”¨: {final_resources['gpu_memory_used']:.1f}MB")
    print(f"   âœ… ä¼°ç®—PPOéœ€è¦æ˜¾å­˜: {final_resources['gpu_memory_used'] * (1 + memory_saving/100):.1f}MB")
    print(f"   âœ… æ˜¾å­˜èŠ‚çœ: ~{memory_saving:.1f}%")
    print(f"   âœ… è®­ç»ƒæ›´ç¨³å®š: ä½¿ç”¨åŠ¨æ€ç»„ç›¸å¯¹åŸºå‡†")
    print(f"   âœ… å®ç°æ›´ç®€å•: æ— éœ€å¤æ‚çš„ä»·å€¼å‡½æ•°è®­ç»ƒ")
    
    print("\n=== Lab03 GRPOå®ç°å®Œæˆ ===")
    print("æ ¸å¿ƒåˆ›æ–°:")
    print("1. ğŸ¯ ç§»é™¤Critic: å¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚")
    print("2. ğŸ“Š ç»„ç›¸å¯¹ä¼˜åŠ¿: ä½¿ç”¨åŒç»„æ ·æœ¬å¹³å‡å¥–åŠ±ä½œä¸ºåŠ¨æ€åŸºå‡†")
    print("3. âš¡ æå‡æ•ˆç‡: æ›´å°‘çš„æ¨¡å‹ï¼Œæ›´å¿«çš„è®­ç»ƒ")
    print("4. ğŸ”„ åŠ¨æ€åŸºå‡†: ç›¸å¯¹è¯„ä¼°é¿å…ç»å¯¹å¥–åŠ±çš„åå·®")
    print("5. ğŸ’¡ å·¥ç¨‹ä¼˜åŒ–: æ›´é€‚åˆå¤§è§„æ¨¡LLMè®­ç»ƒåœºæ™¯")

if __name__ == "__main__":
    main() 
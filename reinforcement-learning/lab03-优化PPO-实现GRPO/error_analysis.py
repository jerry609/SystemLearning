#!/usr/bin/env python3
"""
CUDAé”™è¯¯æ ¹å› åˆ†æžè„šæœ¬
åˆ†æžä¸ºä»€ä¹ˆtorch.multinomialåœ¨ç¬¬äºŒè½®è®­ç»ƒæ—¶å¤±è´¥
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM

def analyze_cuda_error():
    """åˆ†æžCUDAé”™è¯¯çš„æ ¹æœ¬åŽŸå› """
    
    print("=== CUDAé”™è¯¯æ ¹å› åˆ†æž ===")
    
    # å¤çŽ°é”™è¯¯åœºæ™¯
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained("gpt2").cuda()
    
    print("\n1. é”™è¯¯æœºåˆ¶åˆ†æž:")
    print("   torch.multinomial éœ€è¦æœ‰æ•ˆçš„æ¦‚çŽ‡åˆ†å¸ƒä½œä¸ºè¾“å…¥")
    print("   CUDAæ–­è¨€ 'input[0] != 0' æ£€æŸ¥æ¦‚çŽ‡åˆ†å¸ƒä¸èƒ½å…¨é›¶")
    
    # æ¨¡æ‹Ÿå¯¼è‡´é”™è¯¯çš„æƒ…å†µ
    print("\n2. é”™è¯¯è§¦å‘æ¡ä»¶:")
    
    # æƒ…å†µ1ï¼šå…¨é›¶logits
    print("   æƒ…å†µ1: å…¨é›¶logits -> å…¨é›¶æ¦‚çŽ‡åˆ†å¸ƒ")
    zero_logits = torch.zeros(1, 50257).cuda()  # GPT2è¯æ±‡è¡¨å¤§å°
    zero_probs = F.softmax(zero_logits, dim=-1)
    print(f"     å…¨é›¶logitsçš„softmax: {zero_probs[0][:5]} (å‰5ä¸ªå€¼)")
    print(f"     æ‰€æœ‰å€¼æ˜¯å¦ç›¸ç­‰: {torch.all(zero_probs[0] == zero_probs[0][0])}")
    
    # æƒ…å†µ2ï¼šæžå¤§è´Ÿå€¼logits
    print("   æƒ…å†µ2: æžå¤§è´Ÿå€¼logits -> æŽ¥è¿‘å…¨é›¶æ¦‚çŽ‡åˆ†å¸ƒ")
    neg_inf_logits = torch.full((1, 50257), -1e6).cuda()
    neg_inf_probs = F.softmax(neg_inf_logits, dim=-1)
    print(f"     æžå¤§è´Ÿå€¼logitsçš„softmax: {neg_inf_probs[0][:5]} (å‰5ä¸ªå€¼)")
    print(f"     æœ€å¤§æ¦‚çŽ‡å€¼: {neg_inf_probs.max().item()}")
    
    # æƒ…å†µ3ï¼šNaNæˆ–inf logits
    print("   æƒ…å†µ3: NaN/inf logits")
    nan_logits = torch.full((1, 50257), float('nan')).cuda()
    try:
        nan_probs = F.softmax(nan_logits, dim=-1)
        print(f"     NaN logitsçš„softmax: {nan_probs[0][:5]} (å‰5ä¸ªå€¼)")
    except Exception as e:
        print(f"     NaN logitså¤„ç†å¤±è´¥: {e}")
    
    print("\n3. è®­ç»ƒå¯¼è‡´æ¨¡åž‹é€€åŒ–çš„åŽŸå› :")
    print("   âœ— æ¢¯åº¦çˆ†ç‚¸: ç¬¬ä¸€è½®è®­ç»ƒçš„æ¢¯åº¦è¿‡å¤§ï¼Œç ´åæ¨¡åž‹å‚æ•°")
    print("   âœ— æ•°å€¼ä¸ç¨³å®š: æŸå¤±è®¡ç®—ä¸­çš„NaN/infä¼ æ’­åˆ°æ¨¡åž‹å‚æ•°")
    print("   âœ— å­¦ä¹ çŽ‡è¿‡é«˜: å‚æ•°æ›´æ–°æ­¥é•¿è¿‡å¤§ï¼Œè·³å‡ºæœ‰æ•ˆå‚æ•°ç©ºé—´")
    print("   âœ— æ¸©åº¦å‚æ•°å¼‚å¸¸: ç”Ÿæˆå‚æ•°å¯¼è‡´logitsåˆ†å¸ƒé€€åŒ–")
    
    # æ¨¡æ‹Ÿæ­£å¸¸çš„å‚æ•°æ›´æ–°
    print("\n4. æ¨¡åž‹å‚æ•°å¥åº·æ£€æŸ¥:")
    
    # æ£€æŸ¥æ¨¡åž‹å‚æ•°çŠ¶æ€
    def check_model_health(model, name):
        print(f"   {name}æ¨¡åž‹çŠ¶æ€:")
        total_params = 0
        nan_params = 0
        inf_params = 0
        
        for param in model.parameters():
            if param.requires_grad:
                total_params += param.numel()
                nan_params += torch.isnan(param).sum().item()
                inf_params += torch.isinf(param).sum().item()
        
        print(f"     æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"     NaNå‚æ•°æ•°: {nan_params}")
        print(f"     Infå‚æ•°æ•°: {inf_params}")
        print(f"     å‚æ•°å¥åº·åº¦: {'âœ… å¥åº·' if (nan_params + inf_params) == 0 else 'âŒ å¼‚å¸¸'}")
        
        return nan_params + inf_params == 0
    
    # åˆå§‹çŠ¶æ€æ£€æŸ¥
    print("   åˆå§‹çŠ¶æ€:")
    initial_health = check_model_health(model, "åˆå§‹")
    
    # æ¨¡æ‹Ÿä¸€æ¬¡æœ‰é—®é¢˜çš„è®­ç»ƒæ­¥éª¤
    print("\n5. æ¨¡æ‹Ÿæœ‰é—®é¢˜çš„è®­ç»ƒæ­¥éª¤:")
    
    # åˆ›å»ºä¸€ä¸ªä¼šå¯¼è‡´é—®é¢˜çš„æŸå¤±
    prompt = "äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿æ˜¯"
    input_ids = tokenizer.encode(prompt, return_tensors="pt").cuda()
    
    print(f"   è¾“å…¥: {prompt}")
    print(f"   è¾“å…¥tokens: {input_ids[0][:5].tolist()}...")
    
    # æ­£å¸¸å‰å‘ä¼ æ’­
    model.train()
    outputs = model(input_ids)
    logits = outputs.logits
    
    print(f"   æ­£å¸¸logitsèŒƒå›´: [{logits.min().item():.2f}, {logits.max().item():.2f}]")
    print(f"   æ­£å¸¸logitså‡å€¼: {logits.mean().item():.2f}")
    
    # åˆ›å»ºä¸€ä¸ªä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸çš„æŸå¤±
    # è¿™é‡Œæ•…æ„åˆ›å»ºæžå¤§çš„æŸå¤±æ¥æ¨¡æ‹Ÿé—®é¢˜
    fake_targets = torch.randint(0, 50257, (1, input_ids.shape[1])).cuda()
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = fake_targets[..., 1:].contiguous()
    
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)), 
        shift_labels.view(-1), 
        reduction='mean'
    )
    
    print(f"   æ­£å¸¸æŸå¤±å€¼: {loss.item():.4f}")
    
    # äººä¸ºæ”¾å¤§æŸå¤±æ¥æ¨¡æ‹Ÿé—®é¢˜ï¼ˆä¸è¦åœ¨çœŸå®žè®­ç»ƒä¸­è¿™æ ·åšï¼‰
    problematic_loss = loss * 1e6  # æ”¾å¤§æŸå¤±
    print(f"   é—®é¢˜æŸå¤±å€¼: {problematic_loss.item():.4f}")
    
    # æ£€æŸ¥æ¢¯åº¦
    print("\n6. æ¢¯åº¦åˆ†æž:")
    model.zero_grad()
    problematic_loss.backward()
    
    total_grad_norm = 0.0
    max_grad = 0.0
    nan_grads = 0
    
    for param in model.parameters():
        if param.grad is not None:
            param_grad_norm = param.grad.data.norm(2).item()
            total_grad_norm += param_grad_norm ** 2
            max_grad = max(max_grad, param_grad_norm)
            nan_grads += torch.isnan(param.grad).sum().item()
    
    total_grad_norm = total_grad_norm ** 0.5
    
    print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.2f}")
    print(f"   æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad:.2f}")
    print(f"   NaNæ¢¯åº¦æ•°: {nan_grads}")
    print(f"   æ¢¯åº¦çŠ¶æ€: {'âŒ æ¢¯åº¦çˆ†ç‚¸' if total_grad_norm > 100 else 'âœ… æ­£å¸¸'}")
    
    print("\n7. è§£å†³æ–¹æ¡ˆ:")
    print("   ðŸ”§ æ¢¯åº¦è£å‰ª: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)")
    print("   ðŸ”§ å­¦ä¹ çŽ‡è¡°å‡: ä½¿ç”¨æ›´å°çš„å­¦ä¹ çŽ‡ (1e-6 è€Œä¸æ˜¯ 1e-5)")
    print("   ðŸ”§ å‚æ•°ç›‘æŽ§: æ¯æ­¥æ£€æŸ¥å‚æ•°å¥åº·åº¦")
    print("   ðŸ”§ æ•°å€¼ç¨³å®š: åœ¨softmaxå‰é™åˆ¶logitsèŒƒå›´")
    print("   ðŸ”§ æ¸©åº¦é™åˆ¶: ç¡®ä¿temperature >= 0.1, top_p <= 0.95")
    print("   ðŸ”§ å®šæœŸé‡ç½®: æ£€æµ‹åˆ°CUDAé”™è¯¯æ—¶é‡æ–°åŠ è½½æ¨¡åž‹")
    
    print("\n=== ç»“è®º ===")
    print("CUDAåœ¨ç¬¬äºŒè½®è¢«ç ´åçš„åŽŸå› æ˜¯:")
    print("1. ç¬¬ä¸€è½®è®­ç»ƒäº§ç”Ÿäº†æ— æ•ˆçš„æ¢¯åº¦æ›´æ–°")
    print("2. æ¨¡åž‹å‚æ•°è¢«ç ´å(NaN/inf/æžå€¼)")
    print("3. ç ´åçš„æ¨¡åž‹ç”Ÿæˆæ— æ•ˆçš„logitsåˆ†å¸ƒ")
    print("4. torch.multinomialæ”¶åˆ°æ— æ•ˆæ¦‚çŽ‡åˆ†å¸ƒæ—¶è§¦å‘CUDAæ–­è¨€")
    print("5. CUDAé”™è¯¯ä¼ æ’­ï¼Œç ´åæ•´ä¸ªGPUçŠ¶æ€")

if __name__ == "__main__":
    try:
        analyze_cuda_error()
    except Exception as e:
        print(f"åˆ†æžè¿‡ç¨‹å‡ºé”™: {e}")
        print("è¿™è¿›ä¸€æ­¥è¯å®žäº†CUDAçŠ¶æ€çš„ä¸ç¨³å®šæ€§") 